
R version 4.0.0 (2020-04-24) -- "Arbor Day"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin17.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "MLInterfaces"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('MLInterfaces')
Loading required package: Rcpp
Loading required package: BiocGenerics
Loading required package: parallel

Attaching package: ‘BiocGenerics’

The following objects are masked from ‘package:parallel’:

    clusterApply, clusterApplyLB, clusterCall, clusterEvalQ,
    clusterExport, clusterMap, parApply, parCapply, parLapply,
    parLapplyLB, parRapply, parSapply, parSapplyLB

The following objects are masked from ‘package:stats’:

    IQR, mad, sd, var, xtabs

The following objects are masked from ‘package:base’:

    Filter, Find, Map, Position, Reduce, anyDuplicated, append,
    as.data.frame, basename, cbind, colnames, dirname, do.call,
    duplicated, eval, evalq, get, grep, grepl, intersect, is.unsorted,
    lapply, mapply, match, mget, order, paste, pmax, pmax.int, pmin,
    pmin.int, rank, rbind, rownames, sapply, setdiff, sort, table,
    tapply, union, unique, unsplit, which, which.max, which.min

Loading required package: Biobase
Welcome to Bioconductor

    Vignettes contain introductory material; view with
    'browseVignettes()'. To cite Bioconductor, see
    'citation("Biobase")', and for packages 'citation("pkgname")'.

Loading required package: annotate
Loading required package: AnnotationDbi
Loading required package: stats4
Loading required package: IRanges
Loading required package: S4Vectors

Attaching package: ‘S4Vectors’

The following object is masked from ‘package:base’:

    expand.grid

Loading required package: XML
Loading required package: cluster
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("MLearn-new")
> ### * MLearn-new
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: MLearn
> ### Title: revised MLearn interface for machine learning
> ### Aliases: MLearn_new MLearn baggingI dlda glmI.logistic knnI knn.cvI
> ###   ksvmI ldaI lvqI naiveBayesI nnetI qdaI RABI randomForestI rpartI svmI
> ###   svm2 ksvm2 plsda2 plsdaI dlda2 dldaI sldaI blackboostI knn2 knn.cv2
> ###   ldaI.predParms lvq rab adaI
> ###   MLearn,formula,ExpressionSet,character,numeric-method
> ###   MLearn,formula,ExpressionSet,learnerSchema,numeric-method
> ###   MLearn,formula,data.frame,learnerSchema,numeric-method
> ###   MLearn,formula,data.frame,learnerSchema,xvalSpec-method
> ###   MLearn,formula,ExpressionSet,learnerSchema,xvalSpec-method
> ###   MLearn,formula,data.frame,clusteringSchema,ANY-method plotXvalRDA
> ###   rdacvI rdaI BgbmI gbm2 rdaML rdacvML hclustI kmeansI pamI
> ###   makeLearnerSchema standardMLIConverter
> ### Keywords: models
> 
> ### ** Examples
> 
> library("MASS")

Attaching package: ‘MASS’

The following object is masked from ‘package:AnnotationDbi’:

    select

> data(crabs)
> set.seed(1234)
> kp = sample(1:200, size=120)
> rf1 = MLearn(sp~CW+RW, data=crabs, randomForestI, kp, ntree=600 )
> rf1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = randomForestI, 
    trainInd = kp, ntree = 600)
Predicted outcome distribution for test set:

 B  O 
40 40 
Summary of scores on test set (use testScores() method for details):
     B      O 
0.4605 0.5395 
> nn1 = MLearn(sp~CW+RW, data=crabs, nnetI, kp, size=3, decay=.01,
+     trace=FALSE )
> nn1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = nnetI, 
    trainInd = kp, size = 3, decay = 0.01, trace = FALSE)
Predicted outcome distribution for test set:

 B  O 
38 42 
Summary of scores on test set (use testScores() method for details):
[1] 0.5394541
> RObject(nn1)
a 2-3-1 network with 13 weights
inputs: CW RW 
output(s): sp 
options were - entropy fitting  decay=0.01
> knn1 = MLearn(sp~CW+RW, data=crabs, knnI(k=3,l=2), kp)
> knn1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = knnI(k = 3, 
    l = 2), trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
28 50 
Summary of scores on test set (use testScores() method for details):
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.5000  0.6667  0.6667  0.8177  1.0000  1.0000 
> names(RObject(knn1))
[1] "traindat" "ans"      "traincl" 
> dlda1 = MLearn(sp~CW+RW, data=crabs, dldaI, kp )
> dlda1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = dldaI, 
    trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
39 41 
> names(RObject(dlda1))
[1] "traindat" "ans"      "traincl" 
> lda1 = MLearn(sp~CW+RW, data=crabs, ldaI, kp )
> lda1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = ldaI, 
    trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
41 39 
> names(RObject(lda1))
 [1] "prior"   "counts"  "means"   "scaling" "lev"     "svd"     "N"      
 [8] "call"    "terms"   "xlevels"
> slda1 = MLearn(sp~CW+RW, data=crabs, sldaI, kp )
> slda1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = sldaI, 
    trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
32 48 
Summary of scores on test set (use testScores() method for details):
        B         O 
0.4635331 0.5364669 
> names(RObject(slda1))
[1] "scores"  "mylda"   "terms"   "call"    "xlevels"
> svm1 = MLearn(sp~CW+RW, data=crabs, svmI, kp )
> svm1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = svmI, 
    trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
41 39 
Summary of scores on test set (use testScores() method for details):
        B         O 
0.4623062 0.5376938 
> names(RObject(svm1))
 [1] "call"            "type"            "kernel"          "cost"           
 [5] "degree"          "gamma"           "coef0"           "nu"             
 [9] "epsilon"         "sparse"          "scaled"          "x.scale"        
[13] "y.scale"         "nclasses"        "levels"          "tot.nSV"        
[17] "nSV"             "labels"          "SV"              "index"          
[21] "rho"             "compprob"        "probA"           "probB"          
[25] "sigma"           "coefs"           "na.action"       "fitted"         
[29] "decision.values" "terms"          
> ldapp1 = MLearn(sp~CW+RW, data=crabs, ldaI.predParms(method="debiased"), kp )
> ldapp1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = ldaI.predParms(method = "debiased"), 
    trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
41 39 
> names(RObject(ldapp1))
 [1] "prior"   "counts"  "means"   "scaling" "lev"     "svd"     "N"      
 [8] "call"    "terms"   "xlevels"
> qda1 = MLearn(sp~CW+RW, data=crabs, qdaI, kp )
> qda1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = qdaI, 
    trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
45 35 
> names(RObject(qda1))
 [1] "prior"   "counts"  "means"   "scaling" "ldet"    "lev"     "N"      
 [8] "call"    "terms"   "xlevels"
> logi = MLearn(sp~CW+RW, data=crabs, glmI.logistic(threshold=0.5), kp, family=binomial ) # need family
> logi
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = glmI.logistic(threshold = 0.5), 
    trainInd = kp, family = binomial)
Predicted outcome distribution for test set:

 B  O 
40 40 
Summary of scores on test set (use testScores() method for details):
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1417  0.3956  0.4990  0.5514  0.7374  0.9353 
> names(RObject(logi))
 [1] "coefficients"      "residuals"         "fitted.values"    
 [4] "effects"           "R"                 "rank"             
 [7] "qr"                "family"            "linear.predictors"
[10] "deviance"          "aic"               "null.deviance"    
[13] "iter"              "weights"           "prior.weights"    
[16] "df.residual"       "df.null"           "y"                
[19] "converged"         "boundary"          "model"            
[22] "call"              "formula"           "terms"            
[25] "data"              "offset"            "control"          
[28] "method"            "contrasts"         "xlevels"          
> rp2 = MLearn(sp~CW+RW, data=crabs, rpartI, kp)
> rp2
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = rpartI, 
    trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
36 44 
Summary of scores on test set (use testScores() method for details):
        B         O 
0.4226958 0.5773042 
> ## recode data for RAB
> #nsp = ifelse(crabs$sp=="O", -1, 1)
> #nsp = factor(nsp)
> #ncrabs = cbind(nsp,crabs)
> #rab1 = MLearn(nsp~CW+RW, data=ncrabs, RABI, kp, maxiter=10)
> #rab1
> #
> # new approach to adaboost
> #
> ada1 = MLearn(sp ~ CW+RW, data = crabs, .method = adaI, 
+     trainInd = kp, type = "discrete", iter = 200)
> ada1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = adaI, 
    trainInd = kp, type = "discrete", iter = 200)
Predicted outcome distribution for test set:

 B  O 
44 36 
> confuMat(ada1)
     predicted
given  B  O
    B 29 16
    O 15 20
> #
> lvq.1 = MLearn(sp~CW+RW, data=crabs, lvqI, kp )
> lvq.1
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = lvqI, 
    trainInd = kp)
Predicted outcome distribution for test set:

 B  O 
43 37 
> nb.1 = MLearn(sp~CW+RW, data=crabs, naiveBayesI, kp )
> confuMat(nb.1)
     predicted
given  B  O
    B 24 21
    O 15 20
> bb.1 = MLearn(sp~CW+RW, data=crabs, baggingI, kp )
> confuMat(bb.1)
     predicted
given  B  O
    B 22 23
    O 13 22
> #
> # new mboost interface -- you MUST supply family for nonGaussian response
> #
> require(party)  # trafo ... killing cmd check
Loading required package: party
Loading required package: grid
Loading required package: mvtnorm
Loading required package: modeltools

Attaching package: ‘modeltools’

The following object is masked from ‘package:MLInterfaces’:

    Predict

Loading required package: strucchange
Loading required package: zoo

Attaching package: ‘zoo’

The following objects are masked from ‘package:base’:

    as.Date, as.Date.numeric

Loading required package: sandwich
> blb.1 = MLearn(sp~CW+RW+FL, data=crabs, blackboostI, kp, family=mboost::Binomial() )
> confuMat(blb.1)
     predicted
given  B  O
    B 44  1
    O  0 35
> #
> # ExpressionSet illustration
> # 
> data(sample.ExpressionSet)
> #  needed to increase training set size to avoid a new randomForest condition
> # on empty class
> set.seed(1234)
> X = MLearn(type~., sample.ExpressionSet[100:250,], randomForestI, 1:19, importance=TRUE )
> library(randomForest)
randomForest 4.6-14
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:Biobase’:

    combine

The following object is masked from ‘package:BiocGenerics’:

    combine

> library(hgu95av2.db)
Loading required package: org.Hs.eg.db


> opar = par(no.readonly=TRUE)
> par(las=2)
> plot(getVarImp(X), n=10, plat="hgu95av2", toktype="SYMBOL")
> par(opar)
> #
> # demonstrate cross validation
> #
> nn1cv = MLearn(sp~CW+RW, data=crabs[c(1:20,101:120),], 
+    nnetI, xvalSpec("LOO"), size=3, decay=.01, trace=FALSE )
> confuMat(nn1cv)
     predicted
given  B  O
    B  9 11
    O  8 12
> nn2cv = MLearn(sp~CW+RW, data=crabs[c(1:20,101:120),], nnetI, 
+    xvalSpec("LOG",5, balKfold.xvspec(5)), size=3, decay=.01,
+    trace=FALSE )
> confuMat(nn2cv)
     predicted
given  B  O
    B 11  9
    O  7 13
> nn3cv = MLearn(sp~CW+RW+CL+BD+FL, data=crabs[c(1:20,101:120),], nnetI, 
+    xvalSpec("LOG",5, balKfold.xvspec(5), fsFun=fs.absT(2)), size=3, decay=.01,
+    trace=FALSE )
> confuMat(nn3cv)
     predicted
given  B  O
    B 15  5
    O  5 15
> nn4cv = MLearn(sp~.-index-sex, data=crabs[c(1:20,101:120),], nnetI, 
+    xvalSpec("LOG",5, balKfold.xvspec(5), fsFun=fs.absT(2)), size=3, decay=.01,
+    trace=FALSE )
> confuMat(nn4cv)
     predicted
given  B  O
    B 15  5
    O  5 15
> #
> # try with expression data
> #
> library(golubEsets)
> data(Golub_Train)
> litg = Golub_Train[ 100:150, ]
> g1 = MLearn(ALL.AML~. , litg, nnetI, 
+    xvalSpec("LOG",5, balKfold.xvspec(5), 
+    fsFun=fs.probT(.75)), size=3, decay=.01, trace=FALSE )
> confuMat(g1)
     predicted
given ALL AML
  ALL  18   9
  AML   8   3
> #
> # illustrate rda.cv interface from package rda (requiring local bridge)
> #
> library(ALL)
> data(ALL)
> #
> # restrict to BCR/ABL or NEG
> #
> bio <- which( ALL$mol.biol %in% c("BCR/ABL", "NEG"))
> #
> # restrict to B-cell
> #
> isb <- grep("^B", as.character(ALL$BT))
> kp <- intersect(bio,isb)
> all2 <- ALL[,kp]
> mads = apply(exprs(all2),1,mad)
> kp = which(mads>1)  # get around 250 genes
> vall2 = all2[kp, ]
> vall2$mol.biol = factor(vall2$mol.biol) # drop unused levels
> 
> if (requireNamespace("rda", quietly=TRUE)) {
+  library("rda")
+  r1 = MLearn(mol.biol~., vall2, MLInterfaces:::rdacvI, 1:40)
+  confuMat(r1)
+  RObject(r1)
+  MLInterfaces:::plotXvalRDA(r1)  # special interface to plots of parameter space
+ }
> 
> # illustrate clustering support
> 
> cl1 = MLearn(~CW+RW+CL+FL+BD, data=crabs, hclustI(distFun=dist, cutParm=list(k=4)))
> plot(cl1)
> 
> cl1a = MLearn(~CW+RW+CL+FL+BD, data=crabs, hclustI(distFun=dist, cutParm=list(k=4)), 
+    method="complete")
> plot(cl1a)
> 
> cl2 = MLearn(~CW+RW+CL+FL+BD, data=crabs, kmeansI, centers=5, algorithm="Hartigan-Wong")
> plot(cl2, crabs[,-c(1:3)])
> 
> c3 = MLearn(~CL+CW+RW, crabs, pamI(dist), k=5)
> c3
clusteringOutput: partition table

 1  2  3  4  5 
30 48 43 52 27 
The call that created this object was:
MLearn(formula = ~CL + CW + RW, data = crabs, .method = pamI(dist), 
    k = 5)
> plot(c3, data=crabs[,c("CL", "CW", "RW")])
> 
> 
> #  new interfaces to PLS thanks to Laurent Gatto
> 
> set.seed(1234)
> kp = sample(1:200, size=120)
> 
> #plsda.1 = MLearn(sp~CW+RW, data=crabs, plsdaI, kp, probMethod="Bayes")
> #plsda.1
> #confuMat(plsda.1)
> #confuMat(plsda.1,t=.65) ## requires at least 0.65 post error prob to assign species
> #
> #plsda.2 = MLearn(type~., data=sample.ExpressionSet[100:250,], plsdaI, 1:16)
> #plsda.2
> #confuMat(plsda.2)
> #confuMat(plsda.2,t=.65) ## requires at least 0.65 post error prob to assign outcome
> 
> ## examples for predict
> #clout <- MLearn(type~., sample.ExpressionSet[100:250,], svmI , 1:16)
> #predict(clout, sample.ExpressionSet[100:250,17:26])
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:ALL’, ‘package:golubEsets’, ‘package:hgu95av2.db’,
  ‘package:org.Hs.eg.db’, ‘package:randomForest’, ‘package:party’,
  ‘package:strucchange’, ‘package:sandwich’, ‘package:zoo’,
  ‘package:modeltools’, ‘package:mvtnorm’, ‘package:grid’,
  ‘package:MASS’

> nameEx("RAB")
> ### * RAB
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: RAB
> ### Title: real adaboost (Friedman et al)
> ### Aliases: RAB RAB4es DAB Predict tonp mkfmla Predict,raboostCont-method
> ###   Predict,daboostCont-method
> ### Keywords: models
> 
> ### ** Examples
> 
> library(MASS)

Attaching package: ‘MASS’

The following object is masked from ‘package:AnnotationDbi’:

    select

> library(rpart)
> data(Pima.tr)
> data(Pima.te)
> Pima.all = rbind(Pima.tr, Pima.te)
> tonp = ifelse(Pima.all$type == "Yes", 1, -1)
> tonp = factor(tonp)
> Pima.all = data.frame(Pima.all[,1:7], mtype=tonp)
> fit1 = RAB(mtype~ped+glu+npreg+bmi+age, data=Pima.all[1:200,], maxiter=10, maxdepth=5)
[1] "real adaboost iterations:"
12345678910> pfit1 = Predict(fit1, newdata=Pima.tr)
> table(Pima.tr$type, pfit1)
     pfit1
       -1   1
  No  132   0
  Yes   0  68
> 
> 
> 
> cleanEx()

detaching ‘package:rpart’, ‘package:MASS’

> nameEx("balKfold.xvspec")
> ### * balKfold.xvspec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: balKfold.xvspec
> ### Title: generate a partition function for cross-validation, where the
> ###   partitions are approximately balanced with respect to the
> ###   distribution of a response variable
> ### Aliases: balKfold.xvspec
> ### Keywords: models manip
> 
> ### ** Examples
> 
> ## The function is currently defined as
> function (K) 
+ function(data, clab, iternum) {
+     clabs <- data[[clab]]
+     narr <- nrow(data)
+     cnames <- unique(clabs)
+     ilist <- list()
+     for (i in 1:length(cnames)) ilist[[cnames[i]]] <- which(clabs == 
+         cnames[i])
+     clens <- lapply(ilist, length)
+     nrep <- lapply(clens, function(x) ceiling(x/K))
+     grpinds <- list()
+     for (i in 1:length(nrep)) grpinds[[i]] <- rep(1:K, nrep[[i]])[1:clens[[i]]]
+     (1:narr)[-which(unlist(grpinds) == iternum)]
+   }
function (K) 
function(data, clab, iternum) {
    clabs <- data[[clab]]
    narr <- nrow(data)
    cnames <- unique(clabs)
    ilist <- list()
    for (i in 1:length(cnames)) ilist[[cnames[i]]] <- which(clabs == 
        cnames[i])
    clens <- lapply(ilist, length)
    nrep <- lapply(clens, function(x) ceiling(x/K))
    grpinds <- list()
    for (i in 1:length(nrep)) grpinds[[i]] <- rep(1:K, nrep[[i]])[1:clens[[i]]]
    (1:narr)[-which(unlist(grpinds) == iternum)]
}
> # try it out
> library("MASS")

Attaching package: ‘MASS’

The following object is masked from ‘package:AnnotationDbi’:

    select

> data(crabs)
> p1c = balKfold.xvspec(5)
> inds = p1c( crabs, "sp", 3 )
> table(crabs$sp[inds] )

 B  O 
80 80 
> inds2 = p1c( crabs, "sp", 4 )
> table(crabs$sp[inds2] )

 B  O 
80 80 
> allc = 1:200
> # are test sets disjoint?
> intersect(setdiff(allc,inds), setdiff(allc,inds2))
integer(0)
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("classifierOutput-class")
> ### * classifierOutput-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: classifierOutput-class
> ### Title: Class "classifierOutput"
> ### Aliases: classifierOutput-class RObject,classifierOutput-method RObject
> ###   trainInd,classifierOutput-method trainInd
> ###   show,classifierOutput-method testScores,classifierOutput-method
> ###   trainScores,classifierOutput-method
> ###   predictions,classifierOutput-method predictions
> ###   predScores,classifierOutput-method predScores
> ###   predScore,classifierOutput-method predScore
> ###   testPredictions,classifierOutput-method testPredictions
> ###   trainPredictions trainPredictions,classifierOutput-method
> ###   fsHistory,classifierOutput-method testScores trainScores
> ###   testPredictions
> ### Keywords: classes
> 
> ### ** Examples
> 
> showClass("classifierOutput")
Class "classifierOutput" [package "MLInterfaces"]

Slots:
                                                                          
Name:          trainInd     testOutcomes  testPredictions       testScores
Class:          numeric           factor           factor              ANY
                                                                          
Name:     trainOutcomes trainPredictions      trainScores        fsHistory
Class:           factor           factor              ANY             list
                                                                          
Name:           RObject             call       embeddedCV    learnerSchema
Class:              ANY             call          logical    learnerSchema
> library(golubEsets)
> data(Golub_Train) # now cross-validate a neural net
> set.seed(1234)
> xv5 = xvalSpec("LOG", 5, balKfold.xvspec(5))
> m2 = MLearn(ALL.AML~., Golub_Train[1000:1050,], nnetI, xv5, 
+    size=5, decay=.01, maxit=1900 )
# weights:  266
# weights:  266
initial  value 21.960198 
initial  value 18.831628 
iter  10 value 18.353792
iter  10 value 16.920016
iter  20 value 18.328605
iter  20 value 14.775596
iter  30 value 17.233337
iter  30 value 14.714340
iter  40 value 13.594664
iter  40 value 12.929781
iter  50 value 10.823594
iter  50 value 11.900252
iter  60 value 10.281741
iter  60 value 5.750855
iter  70 value 7.445617
iter  70 value 2.417988
iter  80 value 4.296259
iter  80 value 1.679816
iter  90 value 4.051972
iter  90 value 1.154495
iter 100 value 4.050533
iter 100 value 1.131177
iter 110 value 2.045689
iter 110 value 1.126150
iter 120 value 1.885376
iter 120 value 1.117510
iter 130 value 1.809953
iter 130 value 1.081609
iter 140 value 1.640468
iter 140 value 1.042434
iter 150 value 1.556926
iter 150 value 1.007381
iter 160 value 1.547231
iter 160 value 0.979817
iter 170 value 1.364080
iter 170 value 0.971152
iter 180 value 1.262026
iter 180 value 0.952136
iter 190 value 1.197167
iter 190 value 0.939351
iter 200 value 1.166432
iter 200 value 0.899418
iter 210 value 1.128731
iter 210 value 0.824496
iter 220 value 1.100383
iter 220 value 0.687010
iter 230 value 1.062163
iter 230 value 0.651583
iter 240 value 1.034442
iter 240 value 0.641404
iter 250 value 0.987909
iter 250 value 0.637961
iter 260 value 0.938734
iter 260 value 0.619191
iter 270 value 0.935550
iter 270 value 0.596199
iter 280 value 0.583652
iter 280 value 0.933653
iter 290 value 0.929045
iter 290 value 0.581696
iter 300 value 0.927508
iter 300 value 0.562176
iter 310 value 0.558324
iter 310 value 0.925396
iter 320 value 0.923486
iter 320 value 0.553732
iter 330 value 0.922718
iter 330 value 0.548521
iter 340 value 0.922327
iter 340 value 0.547529
iter 350 value 0.899879
iter 350 value 0.538246
iter 360 value 0.853107
iter 360 value 0.529724
iter 370 value 0.845641
iter 370 value 0.524189
iter 380 value 0.839305
iter 380 value 0.512697
iter 390 value 0.829332
iter 390 value 0.499640
iter 400 value 0.828624
iter 400 value 0.488397
iter 410 value 0.819684
iter 410 value 0.486166
iter 420 value 0.817057
iter 420 value 0.482587
iter 430 value 0.816114
iter 430 value 0.474250
iter 440 value 0.814017
iter 440 value 0.459824
iter 450 value 0.813179
iter 450 value 0.444700
iter 460 value 0.812876
iter 460 value 0.435592
iter 470 value 0.812702
iter 470 value 0.420857
iter 480 value 0.812045
iter 480 value 0.412361
iter 490 value 0.811439
iter 490 value 0.411741
iter 500 value 0.811313
iter 500 value 0.410418
iter 510 value 0.811034
iter 510 value 0.408278
iter 520 value 0.810714
iter 520 value 0.405413
iter 530 value 0.810685
iter 530 value 0.405218
iter 540 value 0.810660
iter 540 value 0.404962
iter 550 value 0.766455
iter 550 value 0.404688
iter 560 value 0.760129
iter 560 value 0.401330
iter 570 value 0.755068
iter 570 value 0.401159
iter 580 value 0.746295
iter 580 value 0.401007
iter 590 value 0.739190
iter 590 value 0.400901
iter 600 value 0.735754
iter 600 value 0.400875
iter 610 value 0.735513
iter 610 value 0.400872
iter 620 value 0.735223
iter 620 value 0.400871
final  value 0.400870 
converged
iter 630 value 0.734713
iter 640 value 0.734456
iter 650 value 0.734217
iter 660 value 0.674155
iter 670 value 0.669242
iter 680 value 0.667591
iter 690 value 0.649731
iter 700 value 0.629035
iter 710 value 0.602548
iter 720 value 0.597625
iter 730 value 0.597389
iter 740 value 0.596885
iter 750 value 0.596668
iter 760 value 0.596634
iter 770 value 0.593295
iter 780 value 0.589765
iter 790 value 0.585686
iter 800 value 0.581017
iter 810 value 0.578805
iter 820 value 0.578390
iter 830 value 0.578305
iter 840 value 0.578289
iter 850 value 0.578275
iter 860 value 0.578264
iter 870 value 0.578261
iter 880 value 0.578258
iter 890 value 0.578258
iter 900 value 0.578257
iter 910 value 0.578257
iter 920 value 0.578256
iter 930 value 0.578256
iter 940 value 0.578256
iter 950 value 0.578256
iter 960 value 0.578255
final  value 0.578255 
converged
# weights:  266
initial  value 21.128757 
iter  10 value 16.440133
iter  20 value 16.299869
iter  30 value 15.985844
iter  40 value 15.583602
iter  50 value 15.555073
iter  60 value 12.991529
iter  70 value 11.546327
iter  80 value 10.834934
iter  90 value 10.673930
iter 100 value 10.562628
iter 110 value 10.331863
iter 120 value 10.286781
iter 130 value 9.274719
iter 140 value 7.983436
iter 150 value 6.919412
iter 160 value 6.826983
iter 170 value 5.409674
iter 180 value 5.173860
iter 190 value 5.143753
iter 200 value 4.365591
iter 210 value 4.321833
iter 220 value 4.305871
iter 230 value 4.305547
iter 240 value 4.201353
iter 250 value 3.314667
iter 260 value 3.150504
iter 270 value 2.828632
iter 280 value 2.722690
iter 290 value 2.326934
iter 300 value 2.137532
iter 310 value 2.063069
iter 320 value 2.057412
# weights:  266
initial  value 23.339991 
iter 330 value 2.011833
iter  10 value 18.591461
iter 340 value 1.924470
iter  20 value 17.876329
iter 350 value 1.821418
iter  30 value 14.305209
iter 360 value 1.802732
iter  40 value 12.070529
iter  50 value 11.878920
iter 370 value 1.699523
iter 380 value 1.338630
iter  60 value 10.857629
iter 390 value 1.188638
iter  70 value 10.664084
iter 400 value 1.027272
iter  80 value 9.614909
iter 410 value 0.995695
iter  90 value 8.002634
iter 420 value 0.920178
iter 100 value 4.887175
iter 430 value 0.883453
iter 110 value 3.213952
iter 440 value 0.882710
iter 120 value 1.678703
iter 450 value 0.880138
iter 130 value 1.158869
iter 460 value 0.859612
iter 140 value 1.084929
iter 470 value 0.844250
iter 150 value 1.048543
iter 480 value 0.832640
iter 160 value 1.002613
iter 490 value 0.807090
iter 170 value 0.992399
iter 500 value 0.802054
iter 180 value 0.973470
iter 510 value 0.791714
iter 190 value 0.937721
iter 520 value 0.786712
iter 200 value 0.923102
iter 530 value 0.771260
iter 210 value 0.919869
iter 540 value 0.756153
iter 220 value 0.902485
iter 550 value 0.749775
iter 230 value 0.834085
iter 560 value 0.712109
iter 240 value 0.819585
iter 570 value 0.679095
iter 250 value 0.777305
iter 580 value 0.658527
iter 260 value 0.758799
iter 590 value 0.653702
iter 270 value 0.735350
iter 600 value 0.640438
iter 280 value 0.731523
iter 610 value 0.612861
iter 290 value 0.688333
iter 620 value 0.606108
iter 300 value 0.681271
iter 630 value 0.598485
iter 310 value 0.679588
iter 640 value 0.590163
iter 320 value 0.673032
iter 650 value 0.581189
iter 330 value 0.648183
iter 660 value 0.577150
iter 340 value 0.644664
iter 670 value 0.577048
iter 350 value 0.640347
iter 680 value 0.576114
iter 360 value 0.632877
iter 690 value 0.560056
iter 370 value 0.626724
iter 700 value 0.556720
iter 380 value 0.623852
iter 710 value 0.548516
iter 390 value 0.618893
iter 720 value 0.545816
iter 400 value 0.609909
iter 730 value 0.510567
iter 410 value 0.590787
iter 740 value 0.501129
iter 420 value 0.570172
iter 750 value 0.487527
iter 760 value 0.474646
iter 430 value 0.522120
iter 770 value 0.464680
iter 440 value 0.513899
iter 780 value 0.449411
iter 450 value 0.511543
iter 790 value 0.446758
iter 460 value 0.509310
iter 800 value 0.444470
iter 470 value 0.501004
iter 810 value 0.442667
iter 480 value 0.493384
iter 820 value 0.441162
iter 490 value 0.491379
iter 830 value 0.440972
iter 500 value 0.489208
iter 840 value 0.440950
iter 510 value 0.486562
iter 850 value 0.440943
iter 520 value 0.483908
iter 860 value 0.440941
iter 530 value 0.481262
iter 870 value 0.440938
iter 880 value 0.440936
iter 540 value 0.472204
iter 890 value 0.440935
iter 550 value 0.466949
iter 900 value 0.440935
iter 560 value 0.465132
iter 910 value 0.440935
iter 570 value 0.458316
iter 920 value 0.440934
iter 580 value 0.450893
iter 930 value 0.440934
iter 590 value 0.431943
iter 940 value 0.440934
iter 950 value 0.440934
iter 600 value 0.428896
iter 960 value 0.440934
iter 610 value 0.424030
final  value 0.440934 
converged
iter 620 value 0.418057
iter 630 value 0.414099
iter 640 value 0.409983
iter 650 value 0.408971
iter 660 value 0.408261
iter 670 value 0.406572
iter 680 value 0.401445
iter 690 value 0.395842
iter 700 value 0.390413
iter 710 value 0.387046
iter 720 value 0.372679
iter 730 value 0.371646
iter 740 value 0.370088
iter 750 value 0.367818
iter 760 value 0.365913
iter 770 value 0.364939
iter 780 value 0.364513
# weights:  266
initial  value 22.054863 
iter 790 value 0.364441
iter  10 value 18.738486
iter 800 value 0.364426
iter  20 value 15.526902
iter 810 value 0.364422
iter  30 value 14.951527
iter 820 value 0.364420
iter  40 value 14.176522
iter 830 value 0.364419
iter 840 value 0.364418
iter  50 value 9.674506
iter 850 value 0.364418
iter  60 value 7.316654
iter 860 value 0.364418
iter  70 value 7.278616
iter 870 value 0.364418
iter  80 value 6.360165
iter 880 value 0.364418
iter  90 value 5.909777
iter 890 value 0.364417
iter 100 value 4.766926
iter 900 value 0.364417
iter 910 value 0.364417
iter 110 value 3.414711
iter 920 value 0.364417
iter 120 value 3.284967
iter 930 value 0.364417
iter 130 value 3.244990
iter 940 value 0.364417
final  value 0.364417 
converged
iter 140 value 3.143577
iter 150 value 2.645206
iter 160 value 2.123293
iter 170 value 1.714775
iter 180 value 1.632649
iter 190 value 1.628581
iter 200 value 1.621728
iter 210 value 1.605479
iter 220 value 1.570514
iter 230 value 1.560202
iter 240 value 1.526584
iter 250 value 1.506863
iter 260 value 1.501490
iter 270 value 1.484709
iter 280 value 1.478293
iter 290 value 1.475080
iter 300 value 1.463372
iter 310 value 1.431783
iter 320 value 1.412404
iter 330 value 1.403828
iter 340 value 1.373574
iter 350 value 1.365836
iter 360 value 1.363598
iter 370 value 1.362804
iter 380 value 1.360227
iter 390 value 1.342296
iter 400 value 1.324966
iter 410 value 1.221079
iter 420 value 1.148617
iter 430 value 0.999567
iter 440 value 0.927927
iter 450 value 0.760132
iter 460 value 0.656640
iter 470 value 0.602394
iter 480 value 0.582007
iter 490 value 0.564628
iter 500 value 0.557893
iter 510 value 0.553984
iter 520 value 0.530144
iter 530 value 0.522472
iter 540 value 0.513250
iter 550 value 0.507532
iter 560 value 0.505868
iter 570 value 0.502576
iter 580 value 0.500700
iter 590 value 0.500038
iter 600 value 0.496627
iter 610 value 0.491924
iter 620 value 0.489539
iter 630 value 0.486031
iter 640 value 0.479589
iter 650 value 0.469647
iter 660 value 0.462757
iter 670 value 0.461936
iter 680 value 0.461663
iter 690 value 0.461390
iter 700 value 0.460933
iter 710 value 0.458270
iter 720 value 0.454402
iter 730 value 0.450212
iter 740 value 0.442183
iter 750 value 0.441930
iter 760 value 0.440404
iter 770 value 0.438903
iter 780 value 0.436642
iter 790 value 0.431337
iter 800 value 0.430862
iter 810 value 0.430416
iter 820 value 0.428927
iter 830 value 0.428668
iter 840 value 0.428504
iter 850 value 0.428426
iter 860 value 0.426799
iter 870 value 0.424643
iter 880 value 0.423867
iter 890 value 0.422913
iter 900 value 0.418040
iter 910 value 0.415803
iter 920 value 0.412184
iter 930 value 0.410234
iter 940 value 0.406699
iter 950 value 0.405848
iter 960 value 0.398857
iter 970 value 0.394299
iter 980 value 0.385105
iter 990 value 0.379741
iter1000 value 0.374017
iter1010 value 0.370905
iter1020 value 0.370576
iter1030 value 0.370517
iter1040 value 0.370492
iter1050 value 0.370469
iter1060 value 0.370456
iter1070 value 0.370443
iter1080 value 0.370436
iter1090 value 0.370432
iter1100 value 0.370430
iter1110 value 0.370428
iter1120 value 0.370427
iter1130 value 0.370425
iter1140 value 0.370425
iter1150 value 0.370424
iter1160 value 0.370423
iter1170 value 0.370422
iter1180 value 0.370422
iter1190 value 0.370421
iter1200 value 0.370421
iter1210 value 0.370421
iter1220 value 0.370421
iter1230 value 0.370421
iter1240 value 0.370420
iter1250 value 0.370420
iter1260 value 0.370420
iter1270 value 0.370420
final  value 0.370420 
converged
> testScores(RObject(m2)[[1]]$mlans)
          [,1]
1  0.893872378
6  0.001642949
11 0.001375433
16 0.001375442
21 0.990560739
26 0.001375433
34 0.992514848
28 0.017984482
33 0.992515150
> alls = lapply(RObject(m2), function(x) testScores(x$mlans))
> 
> 
> 
> cleanEx()

detaching ‘package:golubEsets’

> nameEx("clusteringOutput-class")
> ### * clusteringOutput-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: clusteringOutput-class
> ### Title: container for clustering outputs in uniform structure
> ### Aliases: clusteringOutput-class RObject,clusteringOutput-method
> ###   plot,clusteringOutput,ANY-method show,clusteringOutput-method
> ###   show,clusteringSchema-method getConverter,clusteringSchema-method
> ###   getDist,clusteringSchema-method getConverter getDist
> ###   clusteringSchema-class prcompObj-class silhouette-class prcomp-class
> ### Keywords: classes
> 
> ### ** Examples
> 
> showClass("clusteringOutput")
Class "clusteringOutput" [package "MLInterfaces"]

Slots:
                                                                          
Name:         partition       silhouette           prcomp          distFun
Class:          numeric       silhouette        prcompObj         function
                                                                          
Name:         converter             call    learnerSchema          RObject
Class:         function             call clusteringSchema              ANY
> 
> 
> 
> cleanEx()
> nameEx("confuMat-methods")
> ### * confuMat-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confuMat-methods
> ### Title: Compute the confusion matrix for a classifier.
> ### Aliases: confuMat confuMat-methods confuMat,classifierOutput-method
> ###   confuMat,classifierOutput,character-method
> ###   confuMat,classifierOutput,missing-method
> ###   confuMat,classifierOutput,numeric-method
> ### Keywords: methods classif
> 
> ### ** Examples
> 
> library(golubEsets)
> data(Golub_Merge)
> smallG <- Golub_Merge[101:150,]
> k1 <- MLearn(ALL.AML~., smallG, knnI(k=1), 1:30)
> confuMat(k1)
     predicted
given ALL AML
  ALL  17  10
  AML   6   9
> confuMat(k1, "train")
     predicted
given ALL AML
  ALL  20   0
  AML   0  10
> 
> 
> 
> cleanEx()

detaching ‘package:golubEsets’

> nameEx("confuTab")
> ### * confuTab
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confuTab
> ### Title: Compute confusion tables for a confusion matrix.
> ### Aliases: confuTab
> 
> ### ** Examples
> 
> ## the confusion matrix
> cm <- table(iris$Species, sample(iris$Species))
> ## the 3 confusion tables
> (ct <- confuTab(cm))
$setosa
         known
predicted TRUE FALSE
    TRUE    22    28
    FALSE   28    72

$versicolor
         known
predicted TRUE FALSE
    TRUE    19    31
    FALSE   31    69

$virginica
         known
predicted TRUE FALSE
    TRUE    15    35
    FALSE   35    65

> 
> 
> 
> cleanEx()
> nameEx("fs.absT")
> ### * fs.absT
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fs.absT
> ### Title: support for feature selection in cross-validation
> ### Aliases: fs.absT fs.probT fs.topVariance
> ### Keywords: models
> 
> ### ** Examples
> 
> library("MASS")

Attaching package: ‘MASS’

The following object is masked from ‘package:AnnotationDbi’:

    select

> data(crabs)
> # we will demonstrate this procedure with the crabs data.
> # first, create the closure to pick 3 features
> demFS = fs.absT(3)
> # run it on the entire dataset with features excluding sex
> demFS(sp~.-sex, crabs)
sp ~ FL + BD + RW
<environment: 0x7f914cff6cf0>
> # emulate cross-validation by excluding last 50 records
> demFS(sp~.-sex, crabs[1:150,])
sp ~ BD + FL + CL
<environment: 0x7f9154ff0ee8>
> # emulate cross-validation by excluding first 50 records -- different features retained
> demFS(sp~.-sex, crabs[51:200,])
sp ~ FL + BD + CL
<environment: 0x7f915625ab10>
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("fsHistory")
> ### * fsHistory
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fsHistory
> ### Title: extract history of feature selection for a cross-validated
> ###   machine learner
> ### Aliases: fsHistory
> ### Keywords: models
> 
> ### ** Examples
> 
> data(iris)
> iris2 = iris[ iris$Species %in% levels(iris$Species)[1:2], ]
> iris2$Species = factor(iris2$Species) # drop unused levels
> x1 = MLearn(Species~., iris2, ldaI, xvalSpec("LOG", 3, 
+    balKfold.xvspec(3), fs.absT(3)))
Ignoring testScores of class NULL, expecting vector or matrix.
> fsHistory(x1)
[[1]]
[1] "Petal.Length" "Petal.Width"  "Sepal.Length"

[[2]]
[1] "Petal.Length" "Petal.Width"  "Sepal.Length"

[[3]]
[1] "Petal.Length" "Petal.Width"  "Sepal.Width" 

> 
> 
> 
> cleanEx()
> nameEx("hclustWidget")
> ### * hclustWidget
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hclustWidget
> ### Title: shiny-oriented GUI for cluster or classifier exploration
> ### Aliases: hclustWidget mlearnWidget
> ### Keywords: models
> 
> ### ** Examples
> 
> # should run with example(hclustWidget, ask=FALSE)
> if (interactive()) {
+  library(shiny)
+  library(MASS)
+  data(crabs)
+  cr = data.matrix(crabs[,-c(1:3)])
+  au = crabs[,1:3]
+  show(hclustWidget(cr, auxdf=au))
+ ## must use stop widget button to proceed
+   library(ALL)
+   library(hgu95av2.db)
+   data(ALL)
+   show(mlearnWidget(ALL[1:500,], mol.biol~.))
+  }
> 
> 
> 
> cleanEx()
> nameEx("learnerSchema-class")
> ### * learnerSchema-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: learnerSchema-class
> ### Title: Class "learnerSchema" - convey information on a machine learning
> ###   function to the MLearn wrapper
> ### Aliases: learnerSchema-class nonstandardLearnerSchema-class
> ###   show,learnerSchema-method
> ### Keywords: classes
> 
> ### ** Examples
> 
> showClass("learnerSchema")
Class "learnerSchema" [package "MLInterfaces"]

Slots:
                                                      
Name:  packageName   mlFunName   converter   predicter
Class:   character   character    function    function
> 
> 
> 
> cleanEx()
> nameEx("performance-analytics")
> ### * performance-analytics
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: performance-analytics
> ### Title: Assessing classifier performance
> ### Aliases: precision-methods precision,classifierOutput,character-method
> ###   precision,classifierOutput,missing-method
> ###   precision,classifierOutput,numeric-method
> ###   precision,table,missing-method precision recall-methods
> ###   recall,classifierOutput,character-method
> ###   recall,classifierOutput,missing-method
> ###   recall,classifierOutput,numeric-method recall,table,missing-method
> ###   recall sensitivity-methods
> ###   sensitivity,classifierOutput,character-method
> ###   sensitivity,classifierOutput,missing-method
> ###   sensitivity,classifierOutput,numeric-method
> ###   sensitivity,table,missing-method sensitivity macroF1-methods
> ###   macroF1,classifierOutput,character-method
> ###   macroF1,classifierOutput,missing-method
> ###   macroF1,classifierOutput,numeric-method macroF1,table,missing-method
> ###   macroF1,numeric,numeric-method macroF1 acc,table-method acc
> ###   specificity,table-method specificity tp,table-method tp
> ###   tn,table-method tn fp,table-method fp fn,table-method fn
> ###   F1,table-method F1
> ### Keywords: methods
> 
> ### ** Examples
> 
> ## the confusion matrix
> cm <- table(iris$Species, sample(iris$Species))
> tp(cm)
    setosa versicolor  virginica 
        22         19         15 
> tn(cm)
    setosa versicolor  virginica 
        72         69         65 
> fp(cm)
    setosa versicolor  virginica 
        28         31         35 
> fn(cm)
    setosa versicolor  virginica 
        28         31         35 
> acc(cm)
[1] 0.3733333
> precision(cm)
    setosa versicolor  virginica 
      0.44       0.38       0.30 
> recall(cm)
    setosa versicolor  virginica 
      0.44       0.38       0.30 
> F1(cm)
    setosa versicolor  virginica 
      0.44       0.38       0.30 
> macroF1(cm)
[1] 0
> 
> 
> 
> cleanEx()
> nameEx("planarPlot-methods")
> ### * planarPlot-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: planarPlot-methods
> ### Title: Methods for Function planarPlot in Package 'MLInterfaces'
> ### Aliases: planarPlot planarPlot-methods
> ###   planarPlot,classifierOutput,ExpressionSet,character-method
> ###   planarPlot,classifierOutput,data.frame,character-method
> ### Keywords: methods
> 
> ### ** Examples
> 
> library(ALL)
> library(hgu95av2.db)
Loading required package: org.Hs.eg.db
> data(ALL)
> #
> # restrict to BCR/ABL or NEG
> #
> bio <- which( ALL$mol.biol %in% c("BCR/ABL", "NEG"))
> #
> # restrict to B-cell
> #
> isb <- grep("^B", as.character(ALL$BT))
> kp <- intersect(bio,isb)
> all2 <- ALL[,kp]
> #
> # sample 2 genes at random
> #
> set.seed(1234)
> ng <- nrow(exprs(all2)) # pick 5 in case any NAs come back
> pick <- sample(1:ng, size=5, replace=FALSE)
> gg <- all2[pick,]
> sym <- unlist(mget(featureNames(gg), hgu95av2SYMBOL))
> bad = which(is.na(sym))
> if (length(bad)>0) {
+   gg = gg[-bad,]
+   sym = sym[-bad]
+   }
> gg = gg[1:2,]
> sym = sym[1:2]
> featureNames(gg) <- sym
> gg$class = factor(ifelse(all2$mol.biol=="NEG", "NEG", "POS"))
> 
> cl1 <- which( gg$class == "NEG" )
> cl2 <- which( gg$class != "NEG" )
> #
> # create balanced training sample
> #
> trainInds <- c( sample(cl1, size=floor(length(cl1)/2) ),
+       sample(cl2, size=floor(length(cl2)/2)) )
> #
> # run rpart
> #
> tgg <- MLearn(class~., gg, rpartI, trainInds, minsplit=4 )
> opar <- par(no.readonly=TRUE)
> par(mfrow=c(2,2))
> planarPlot( tgg, gg, "class" )
Loading required namespace: RColorBrewer
> title("rpart")
> points(exprs(gg)[1,trainInds], exprs(gg)[2,trainInds], col=ifelse(gg$class[trainInds]=="NEG", "yellow", "black"), pch=16)
> #
> # run nnet
> #
> ngg <- MLearn( class~., gg, nnetI, trainInds, size=8 )
# weights:  33
initial  value 26.990974 
iter  10 value 26.790129
iter  20 value 26.739289
iter  30 value 26.463308
iter  40 value 25.884326
iter  50 value 25.106988
iter  60 value 24.918273
iter  70 value 24.895713
iter  70 value 24.895713
final  value 24.895710 
converged
> planarPlot( ngg, gg, "class" )
> points(exprs(gg)[1,trainInds], exprs(gg)[2,trainInds], col=ifelse(gg$class[trainInds]=="NEG", "yellow", "black"), pch=16)
> title("nnet")
> #
> # run knn
> #
> kgg <- MLearn( class~.,  gg, knnI(k=3,l=1), trainInds)
> planarPlot( kgg, gg, "class" )
> points(exprs(gg)[1,trainInds], exprs(gg)[2,trainInds], col=ifelse(gg$class[trainInds]=="NEG", "yellow", "black"), pch=16)
> title("3-nn")
> #
> # run svm
> #
> sgg <- MLearn( class~., gg, svmI, trainInds )
> planarPlot( sgg, gg, "class" )
> points(exprs(gg)[1,trainInds], exprs(gg)[2,trainInds], col=ifelse(gg$class[trainInds]=="NEG", "yellow", "black"), pch=16)
> title("svm")
> par(opar)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:hgu95av2.db’, ‘package:org.Hs.eg.db’, ‘package:ALL’

> nameEx("plspinHcube")
> ### * plspinHcube
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plspinHcube
> ### Title: shiny app for interactive 3D visualization of mlbench hypercube
> ### Aliases: plspinHcube
> ### Keywords: models
> 
> ### ** Examples
> 
> if (interactive()) plspinHcube()
> 
> 
> 
> cleanEx()
> nameEx("predict.classifierOutput")
> ### * predict.classifierOutput
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.classifierOutput
> ### Title: Predict method for 'classifierOutput' objects
> ### Aliases: predict.classifierOutput
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(1234)
> ##D data(sample.ExpressionSet)
> ##D trainInd <- 1:16
> ##D 
> ##D clout.svm <- MLearn(type~., sample.ExpressionSet[100:250,], svmI, trainInd)
> ##D predict(clout.svm, sample.ExpressionSet[100:250,-trainInd])
> ##D 
> ##D clout.ksvm <- MLearn(type~., sample.ExpressionSet[100:250,], ksvmI, trainInd)
> ##D predict(clout.ksvm, sample.ExpressionSet[100:250,-trainInd])
> ##D 
> ##D clout.nnet <- MLearn(type~., sample.ExpressionSet[100:250,], nnetI, trainInd, size=3, decay=.01 )
> ##D predict(clout.nnet, sample.ExpressionSet[100:250,-trainInd])
> ##D 
> ##D clout.knn <- MLearn(type~., sample.ExpressionSet[100:250,], knnI(k=3), trainInd)
> ##D predict(clout.knn, sample.ExpressionSet[100:250,-trainInd],k=1)
> ##D predict(clout.knn, sample.ExpressionSet[100:250,-trainInd],k=3)
> ##D 
> ##D #clout.plsda <- MLearn(type~., sample.ExpressionSet[100:250,], plsdaI, trainInd)
> ##D #predict(clout.plsda, sample.ExpressionSet[100:250,-trainInd])
> ##D 
> ##D clout.nb <- MLearn(type~., sample.ExpressionSet[100:250,], naiveBayesI, trainInd)
> ##D predict(clout.nb, sample.ExpressionSet[100:250,-trainInd])
> ##D 
> ##D # this can fail if training set does not yield sufficient diversity in response vector;
> ##D # setting seed seems to help with this example, but other applications may have problems
> ##D #
> ##D clout.rf <- MLearn(type~., sample.ExpressionSet[100:250,], randomForestI, trainInd)
> ##D predict(clout.rf, sample.ExpressionSet[100:250,-trainInd])
> ## End(Not run) # end of dontrun
> 
> 
> 
> cleanEx()
> nameEx("projectLearnerToGrid")
> ### * projectLearnerToGrid
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: projectLearnerToGrid
> ### Title: create learned tesselation of feature space after PC
> ###   transformation
> ### Aliases: projectLearnerToGrid
> ### Keywords: models
> 
> ### ** Examples
> 
> library(mlbench)
> # demostrate with 3 dimensional hypercube problem
> kk = mlbench.hypercube()
> colnames(kk$x) = c("f1", "f2", "f3")
> hcu = data.frame(cl=kk$classes, kk$x)
> set.seed(1234)
> sam = sample(1:nrow(kk$x), size=nrow(kk$x)/2)
> ldap = projectLearnerToGrid(cl~., data=hcu, ldaI, 
+    sam, predWrapper=function(x)x$class)
> plot(ldap)
> confuMat(ldap@fittedLearner)
     predicted
given  1  2  3  4  5  6  7  8
    1 51  0  0  0  0  0  0  0
    2  0 50  0  0  0  0  0  0
    3  0  0 51  0  0  0  0  0
    4  0  0  0 53  0  0  0  0
    5  0  0  0  0 47  0  0  0
    6  0  0  0  0  0 52  0  0
    7  0  0  0  0  0  0 51  0
    8  0  0  0  0  0  0  0 45
> nnetp = projectLearnerToGrid(cl~., data=hcu, nnetI, sam, size=2,
+    decay=.01, predExtras=list(type="class"))
# weights:  32
initial  value 875.669514 
iter  10 value 432.690570
iter  20 value 306.611981
iter  30 value 212.588189
iter  40 value 145.524292
iter  50 value 105.577146
iter  60 value 95.314313
iter  70 value 93.454490
iter  80 value 87.571102
iter  90 value 83.605070
iter 100 value 83.138302
final  value 83.138302 
stopped after 100 iterations
> plot(nnetp)
> confuMat(nnetp@fittedLearner)
     predicted
given  1  2  3  4  5  6  7  8
    1 50  0  1  0  0  0  0  0
    2  0 50  0  0  0  0  0  0
    3  2  1 46  0  0  2  0  0
    4  0  0  0 53  0  0  0  0
    5  0  0  0  0 47  0  0  0
    6  0  0  2  0  0 48  0  2
    7  0  0  0  0  0  0 51  0
    8  0  0  0  0  0  0  0 45
> #if (requireNamespace("rgl") && interactive()) {
> #    learnerIn3D(nnetp)
> #    ## customising the rgl plot
> #    learnerIn3D(nnetp, size = 10, alpha = 0.1)
> #}
> 
> 
> 
> cleanEx()

detaching ‘package:mlbench’

> nameEx("projectedLearner-class")
> ### * projectedLearner-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: projectedLearner-class
> ### Title: Class '"projectedLearner"'
> ### Aliases: projectedLearner-class learnerIn3D,projectedLearner-method
> ###   plot,projectedLearner,ANY-method plotOne,projectedLearner-method
> ###   show,projectedLearner-method plotOne learnerIn3D
> ### Keywords: classes
> 
> ### ** Examples
> 
> showClass("projectedLearner")
Class "projectedLearner" [package "MLInterfaces"]

Slots:
                                                                        
Name:                     fittedLearner                   trainingSetPCA
Class:                 classifierOutput                           prcomp
                                                                        
Name:                    trainingLabels                       testLabels
Class:                              ANY                              ANY
                                                                        
Name:   gridFeatsProjectedToTrainingPCs                  gridPredictions
Class:                           matrix                              ANY
                                                                        
Name:  trainFeatsProjectedToTrainingPCs  testFeatsProjectedToTrainingPCs
Class:                           matrix                           matrix
                                                                        
Name:                  trainPredictions                  testPredictions
Class:                              ANY                              ANY
                                       
Name:                           theCall
Class:                             call
> 
> 
> 
> cleanEx()
> nameEx("raboostCont-class")
> ### * raboostCont-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: raboostCont-class
> ### Title: Class "raboostCont" ~~~
> ### Aliases: raboostCont-class daboostCont-class show,raboostCont-method
> ### Keywords: classes
> 
> ### ** Examples
> 
> showClass("raboostCont")
Class "raboostCont" [package "MLInterfaces"]

Slots:
                              
Name:    .Data formula    call
Class:    list formula    call

Extends: 
Class "list", from data part
Class "vector", by class "list", distance 2
Class "AssayData", by class "list", distance 2
Class "list_OR_List", by class "list", distance 2
Class "vector_OR_Vector", by class "list", distance 3
Class "vector_OR_factor", by class "list", distance 3

Known Subclasses: "daboostCont"
> 
> 
> 
> cleanEx()
> nameEx("varImpStruct-class")
> ### * varImpStruct-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: varImpStruct-class
> ### Title: Class "varImpStruct" - collect data on variable importance from
> ###   various machine learning methods
> ### Aliases: varImpStruct-class plot plot,varImpStruct-method
> ###   plot,varImpStruct,ANY-method show,varImpStruct-method
> ###   report,varImpStruct-method report getVarImp
> ###   getVarImp,classifOutput,logical-method
> ###   getVarImp,classifierOutput,logical-method
> ###   getVarImp,classifierOutput,missing-method
> ### Keywords: classes
> 
> ### ** Examples
> 
> library(golubEsets)
> data(Golub_Merge)
> library(hu6800.db)
Loading required package: org.Hs.eg.db

> smallG <- Golub_Merge[1001:1060,]
> set.seed(1234)
> opar=par(no.readonly=TRUE)
> par(las=2, mar=c(10,11,5,5))
> rf2 <- MLearn(ALL.AML~., smallG, randomForestI, 1:40, importance=TRUE,
+  sampsize=table(smallG$ALL.AML[1:40]), mtry=sqrt(ncol(exprs(smallG))))
> plot( getVarImp( rf2, FALSE ), n=10, plat="hu6800", toktype="SYMBOL")
> par(opar)
> report( getVarImp( rf2, FALSE ), n=10, plat="hu6800", toktype="SYMBOL")
                   MnDecrAcc            names
HG4740.HT5187_at 0.024413313 HG4740.HT5187_at
HG64.HT64_at     0.007810415     HG64.HT64_at
J02874_at        0.007665816            FABP4
J00301_at        0.006983934              PTH
HG960.HT960_at   0.006577261   HG960.HT960_at
J00073_at        0.005323128            ACTC1
HG987.HT987_at   0.005129924   HG987.HT987_at
J02645_at        0.004961747           EIF2S1
HG511.HT511_at   0.004156396   HG511.HT511_at
HG4606.HT5011_at 0.003570340 HG4606.HT5011_at
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()

detaching ‘package:hu6800.db’, ‘package:org.Hs.eg.db’,
  ‘package:golubEsets’

> nameEx("xvalLoop")
> ### * xvalLoop
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: xvalLoop
> ### Title: Cross-validation in clustered computing environments
> ### Aliases: xvalLoop xvalLoop,ANY-method
> ### Keywords: methods
> 
> ### ** Examples
> 
> ## Not run: 
> ##D library(golubEsets)
> ##D data(Golub_Merge)
> ##D smallG <- Golub_Merge[200:250,]
> ##D 
> ##D # Evaluation on one node
> ##D 
> ##D lk1 <- xval(smallG, "ALL.AML", knnB, xvalMethod="LOO", group=as.integer(0))
> ##D table(lk1,smallG$ALL.AML)
> ##D 
> ##D # Evaluation on several nodes -- a cluster programmer might write the following...
> ##D 
> ##D library(snow)
> ##D setOldClass("spawnedMPIcluster")
> ##D 
> ##D setMethod("xvalLoop", signature( cluster = "spawnedMPIcluster"),
> ##D ## use the function returned below to evalutae
> ##D ## the central cross-validation loop in xval
> ##D function( cluster, ... ) {
> ##D     clusterExportEnv <- function (cl, env = .GlobalEnv)
> ##D     {
> ##D         unpackEnv <- function(env) {
> ##D             for ( name in ls(env) ) assign(name, get(name, env), .GlobalEnv )
> ##D             NULL
> ##D         }
> ##D         clusterCall(cl, unpackEnv, env)
> ##D     }
> ##D     function(X, FUN, ...) { # this gets returned to xval
> ##D         ## send all visible variables from the parent (i.e., xval) frame
> ##D         clusterExportEnv( cluster, parent.frame(1) )
> ##D         parLapply( cluster, X, FUN, ... )
> ##D     }
> ##D })
> ##D 
> ##D # ... and use the cluster like this...
> ##D 
> ##D cl <- makeCluster(2, "MPI")
> ##D clusterEvalQ(cl, library(MLInterfaces))
> ##D 
> ##D lk1 <- xval(smallG, "ALL.AML", knnB, xvalMethod="LOO", group=as.integer(0), cluster = cl)
> ##D table(lk1,smallG$ALL.AML)
> ## End(Not run)
> 
> 
> cleanEx()
> nameEx("xvalSpec")
> ### * xvalSpec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: xvalSpec
> ### Title: container for information specifying a cross-validated machine
> ###   learning exercise
> ### Aliases: xvalSpec xvalSpec-class
> ### Keywords: models
> 
> ### ** Examples
> 
> library("MASS")

Attaching package: ‘MASS’

The following object is masked from ‘package:AnnotationDbi’:

    select

> data(crabs)
> set.seed(1234)
> #
> # demonstrate cross validation
> #
> nn1cv = MLearn(sp~CW+RW, data=crabs, nnetI, xvalSpec("LOG",
+    5, balKfold.xvspec(5)), size=3, decay=.01 )
# weights: # weights:   1313

initial  value 119.586522 
initial  value 123.183221 
iter  10 value 110.446749
iter  10 value 105.874807
iter  20 value 101.334888
iter  20 value 92.579957
iter  30 value 100.714963
iter  30 value 92.023828
iter  40 value 100.694182
iter  40 value 91.933449
iter  50 value 100.681437
iter  50 value 91.914734
iter  60 value 100.679938
final  value 100.679934 
converged
iter  60 value 91.909393
iter  70 value 91.894075
iter  80 value 90.070650
iter  90 value 86.074854
iter 100 value 85.749491
final  value 85.749491 
stopped after 100 iterations
# weights:  13
initial  value 122.478154 
iter  10 value 105.892411
iter  20 value 101.469895
iter  30 value 101.347855
iter  40 value 101.324397
iter  50 value 100.629852
iter  60 value 97.229539
iter  70 value 96.767091
# weights:  13
initial  value 112.817489 
iter  80 value 96.755593
iter  10 value 110.675313
iter  90 value 96.740960
iter  20 value 102.365904
iter  30 value 101.322653
iter 100 value 96.620459
final  value 96.620459 
stopped after 100 iterations
iter  40 value 101.077698
iter  50 value 96.863551
iter  60 value 95.082208
iter  70 value 94.920162
iter  80 value 94.915093
iter  90 value 94.910805
iter 100 value 93.577974
final  value 93.577974 
stopped after 100 iterations
# weights:  13
initial  value 114.799231 
iter  10 value 100.524612
iter  20 value 98.628399
iter  30 value 94.849023
iter  40 value 94.158513
iter  50 value 93.881131
iter  60 value 93.846005
iter  70 value 93.399405
iter  80 value 89.288656
iter  90 value 87.925316
iter 100 value 87.711581
final  value 87.711581 
stopped after 100 iterations
> nn1cv
MLInterfaces classification output container
The call was:
MLearn(formula = sp ~ CW + RW, data = crabs, .method = nnetI, 
    trainInd = xvalSpec("LOG", 5, balKfold.xvspec(5)), size = 3, 
    decay = 0.01)
Predicted outcome distribution for test set:

  B   O 
 92 108 
Summary of scores on test set (use testScores() method for details):
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
0.001748 0.374739 0.504995 0.518410 0.684008 0.942069 
history of feature selection in cross-validation available; use fsHistory()
> confuMat(nn1cv)
     predicted
given  B  O
    B 63 37
    O 29 71
> names(RObject(nn1cv)[[1]])
[1] "test.idx"  "mlans"     "featInUse"
> RObject(RObject(nn1cv)[[1]]$mlans)
a 2-3-1 network with 13 weights
inputs: CW RW 
output(s): sp 
options were - entropy fitting  decay=0.01
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()

detaching ‘package:MASS’

> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  18.449 1.94 23.111 2.945 0.843 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
