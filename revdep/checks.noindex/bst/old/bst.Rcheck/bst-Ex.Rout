
R version 4.0.0 (2020-04-24) -- "Arbor Day"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin17.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "bst"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('bst')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("bst")
> ### * bst
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bst
> ### Title: Boosting for Classification and Regression
> ### Aliases: bst print.bst predict.bst plot.bst coef.bst fpartial.bst
> ### Keywords: classification
> 
> ### ** Examples
> 
> x <- matrix(rnorm(100*5),ncol=5)
> c <- 2*x[,1]
> p <- exp(c)/(exp(c)+exp(-c))
> y <- rbinom(100,1,p)
> y[y != 1] <- -1
> x <- as.data.frame(x)
> dat.m <- bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
> predict(dat.m)
           1            2            3            4            5            6 
-0.812720741  0.238246996 -1.084090628  2.069614349  0.427482116 -1.064422726 
           7            8            9           10           11           12 
 0.632358993  0.957854819  0.746981563 -0.396191183  1.961287314  0.505757453 
          13           14           15           16           17           18 
-0.805957433 -2.873208691  1.459412767 -0.058293964 -0.021004202  1.224472182 
          19           20           21           22           23           24 
 1.065399375  0.770489242  1.192221929  1.014693155  0.096735797 -2.580856492 
          25           26           27           28           29           30 
 0.804121920 -0.072817804 -0.202119035 -1.908059217 -0.620321020  0.542210405 
          31           32           33           34           35           36 
 1.762663157 -0.133350163  0.502940128 -0.069803187 -1.786508190 -0.538387162 
          37           38           39           40           41           42 
-0.511526337 -0.076949373  1.427101919  0.990094959 -0.213442295 -0.328695091 
          43           44           45           46           47           48 
 0.904195299  0.722178905 -0.893547184 -0.917858553  0.472985107  0.997045013 
          49           50           51           52           53           54 
-0.145750725  1.143092295  0.516476874 -0.794003541  0.442546671 -1.465162789 
          55           56           57           58           59           60 
 1.859112460  2.569243008 -0.476409442 -1.354592873  0.739117473 -0.175211126 
          61           62           63           64           65           66 
 3.115703876 -0.050907447  0.894823331  0.036328193 -0.964274688  0.244926944 
          67           68           69           70           71           72 
-2.341636828  1.901316287  0.198820990  2.818606155  0.616895372 -0.921038678 
          73           74           75           76           77           78 
 0.792316953 -1.211837978 -1.626383061  0.378103535 -0.575098265  0.001434012 
          79           80           81           82           83           84 
 0.096445636 -0.764806426 -0.737754111 -0.175372010  1.528374033 -1.976577232 
          85           86           87           88           89           90 
 0.770547449  0.431948322  1.379197114 -0.394628590  0.480038522  0.346516732 
          91           92           93           94           95           96 
-0.703830473  1.567009733  1.505431458  0.908411995  2.058655306  0.724544242 
          97           98           99          100 
-1.656168337 -0.743717548 -1.588733367 -0.614159431 
> dat.m1 <- bst(x, y, ctrl = bst_control(twinboost=TRUE, 
+ coefir=coef(dat.m), xselect.init = dat.m$xselect, mstop=50))
> dat.m2 <- rbst(x, y, ctrl = bst_control(mstop=50, s=0, trace=TRUE), 
+ rfamily = "thinge", learner = "ls")

generate initial values

robust boosting ...

initial loss 0.4854019 

m= 10   risk =  0.3500714
m= 20   risk =  0.2554629
m= 30   risk =  0.2149859
m= 40   risk =  0.1884801
m= 50   risk =  0.1691262
iteration 1 : los[k] <= ellu2 0
iteration 1 : ellu2 <= ellu1 -0.3162757
iteration 1 : relative change of fk 951.2904 , robust loss value 0.1691262 
d1= 951.2904 , k= 1 , d1 > del && k <= iter:  TRUE 

m= 10   risk =  0.3500714
m= 20   risk =  0.2554629
m= 30   risk =  0.2149859
m= 40   risk =  0.1884801
m= 50   risk =  0.1691262
iteration 2 : los[k] <= ellu2 0
iteration 2 : ellu2 <= ellu1 0
iteration 2 : relative change of fk 0 , robust loss value 0.1691262 
d1= 0 , k= 2 , d1 > del && k <= iter:  FALSE 
> predict(dat.m2)
           1            2            3            4            5            6 
-0.834882945  0.244743789 -1.113652859  2.126050975  0.439139190 -1.093448630 
           7            8            9           10           11           12 
 0.649602885  0.983974706  0.767351116 -0.406994980  2.014769953  0.519549030 
          13           14           15           16           17           18 
-0.827935207 -2.951558652  1.499209714 -0.059883591 -0.021576969  1.257862499 
          19           20           21           22           23           24 
 1.094451911  0.791499829  1.224732808  1.042362976  0.099373700 -2.651234257 
          25           26           27           28           29           30 
 0.826049642 -0.074803484 -0.207630649 -1.960090371 -0.637236648  0.556996022 
          31           32           33           34           35           36 
 1.810729485 -0.136986509  0.516654878 -0.071706661 -1.835224751 -0.553068522 
          37           38           39           40           41           42 
-0.525475226 -0.079047717  1.466017776  1.017094007 -0.219262685 -0.337658327 
          43           44           45           46           47           48 
 0.928851937  0.741872111 -0.917913457 -0.942887776  0.485883009  1.024233584 
          49           50           51           52           53           54 
-0.149725223  1.174263451  0.530560760 -0.815655343  0.454614543 -1.505116534 
          55           56           57           58           59           60 
 1.909808877  2.639304083 -0.489400723 -1.391531470  0.759272579 -0.179988985 
          61           62           63           64           65           66 
 3.200666474 -0.052295650  0.919224404  0.037318832 -0.990569640  0.251605894 
          67           68           69           70           71           72 
-2.405491276  1.953163567  0.204242669  2.895467151  0.633717585 -0.946154620 
          73           74           75           76           77           78 
 0.813922764 -1.244883771 -1.670733146  0.388414098 -0.590780706  0.001473116 
          79           80           81           82           83           84 
 0.099075626 -0.785662048 -0.757872039 -0.180154256  1.570051494 -2.030476814 
          85           86           87           88           89           90 
 0.791559624  0.443727186  1.416806648 -0.405389776  0.493128765  0.355965949 
          91           92           93           94           95           96 
-0.723023332  1.609740757  1.546483296  0.933183619  2.114793087  0.744301949 
          97           98           99          100 
-1.701330641 -0.763998094 -1.632056777 -0.630907037 
> 
> 
> 
> cleanEx()
> nameEx("bst.sel")
> ### * bst.sel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bst.sel
> ### Title: Function to select number of predictors
> ### Aliases: bst.sel
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- matrix(rnorm(100*100), nrow = 100, ncol = 100)
> ##D y <- x[,1] * 2 + x[,2] * 2.5 + rnorm(100)
> ##D sel <- bst.sel(x, y, q=10)
> ##D library("hdi")
> ##D fit.multi <- hdi(x, y, method = "multi.split",
> ##D model.selector =bst.sel,
> ##D args.model.selector=list(type="firstq", q=10))
> ##D fit.multi
> ##D fit.multi$pval[1:10] ## the first 10 p-values
> ##D fit.multi <- hdi(x, y, method = "multi.split",
> ##D model.selector =bst.sel,
> ##D args.model.selector=list(type="cv"))
> ##D fit.multi
> ##D fit.multi$pval[1:10] ## the first 10 p-values
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cv.bst")
> ### * cv.bst
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cv.bst
> ### Title: Cross-Validation for Boosting
> ### Aliases: cv.bst
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- matrix(rnorm(100*5),ncol=5)
> ##D c <- 2*x[,1]
> ##D p <- exp(c)/(exp(c)+exp(-c))
> ##D y <- rbinom(100,1,p)
> ##D y[y != 1] <- -1
> ##D x <- as.data.frame(x)
> ##D cv.bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls", type="loss")
> ##D cv.bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls", type="error")
> ##D dat.m <- bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
> ##D dat.m1 <- cv.bst(x, y, ctrl = bst_control(twinboost=TRUE, coefir=coef(dat.m), 
> ##D xselect.init = dat.m$xselect, mstop=50), family = "hinge", learner="ls")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("cv.rbst")
> ### * cv.rbst
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cv.rbst
> ### Title: Cross-Validation for Nonconvex Loss Boosting
> ### Aliases: cv.rbst
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- matrix(rnorm(100*5),ncol=5)
> ##D c <- 2*x[,1]
> ##D p <- exp(c)/(exp(c)+exp(-c))
> ##D y <- rbinom(100,1,p)
> ##D y[y != 1] <- -1
> ##D x <- as.data.frame(x)
> ##D cv.rbst(x, y, ctrl = bst_control(mstop=50), rfamily = "thinge", learner = "ls", type="lose")
> ##D cv.rbst(x, y, ctrl = bst_control(mstop=50), rfamily = "thinge", learner = "ls", type="error")
> ##D dat.m <- rbst(x, y, ctrl = bst_control(mstop=50), rfamily = "thinge", learner = "ls")
> ##D dat.m1 <- cv.rbst(x, y, ctrl = bst_control(twinboost=TRUE, coefir=coef(dat.m), 
> ##D xselect.init = dat.m$xselect, mstop=50), family = "thinge", learner="ls")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("ex1data")
> ### * ex1data
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ex1data
> ### Title: Generating Three-class Data with 50 Predictors
> ### Aliases: ex1data
> ### Keywords: classification
> 
> ### ** Examples
> 
> ## Not run: 
> ##D dat <- ex1data(100, p=5)
> ##D mhingebst(x=dat$x, y=dat$y)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("mada")
> ### * mada
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mada
> ### Title: Multi-class AdaBoost
> ### Aliases: mada
> ### Keywords: classification
> 
> ### ** Examples
> 
> data(iris)
> mada(xtr=iris[,-5], ytr=iris[,5])
> 
> 
> 
> cleanEx()
> nameEx("mbst")
> ### * mbst
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mbst
> ### Title: Boosting for Multi-Classification
> ### Aliases: mbst print.mbst predict.mbst fpartial.mbst
> ### Keywords: classification
> 
> ### ** Examples
> 
> x <- matrix(rnorm(100*5),ncol=5)
> c <- quantile(x[,1], prob=c(0.33, 0.67))
> y <- rep(1, 100)
> y[x[,1] > c[1] & x[,1] < c[2] ] <- 2
> y[x[,1] > c[2]] <- 3
> x <- as.data.frame(x)
> dat.m <- mbst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
> predict(dat.m)
               [,1]         [,2]        [,3]
  [1,]  0.533923303 -0.176913579 -0.35700972
  [2,] -0.183114746  0.036722306  0.14639244
  [3,]  0.761315876 -0.266408065 -0.49490781
  [4,] -1.170776228  0.237407081  0.93336915
  [5,] -0.114177829 -0.055433159  0.16961099
  [6,]  0.444845898  0.306903585 -0.75174948
  [7,] -0.422252922  0.219014408  0.20323851
  [8,] -0.610003424  0.310974800  0.29902862
  [9,] -0.497309324  0.158073629  0.33923569
 [10,]  0.052365316  0.319786285 -0.37215160
 [11,] -0.983383733  0.095197094  0.88818664
 [12,] -0.232703638 -0.032010246  0.26471388
 [13,]  0.268151030  0.230587867 -0.49873890
 [14,]  1.648933588 -0.480122548 -1.16881104
 [15,] -0.782228493  0.101107356  0.68112114
 [16,]  0.101372502 -0.051388497 -0.04998401
 [17,]  0.023546522 -0.068188913  0.04464239
 [18,] -0.649046690  0.062411250  0.58663544
 [19,] -0.668709625  0.158821672  0.50988795
 [20,] -0.407697527  0.087488828  0.32020870
 [21,] -0.579624764  0.080168389  0.49945638
 [22,] -0.657868029  0.430739819  0.22712821
 [23,] -0.006111179 -0.013678090  0.01978927
 [24,]  1.409257909 -0.398919358 -1.01033855
 [25,] -0.425260931  0.087895464  0.33736547
 [26,] -0.037539186  0.187626034 -0.15008685
 [27,]  0.123678133 -0.075875043 -0.04780309
 [28,]  1.072138457 -0.220322486 -0.85181597
 [29,]  0.397781546 -0.250078994 -0.14770255
 [30,] -0.239508008 -0.004394365  0.24390237
 [31,] -0.974487941  0.156871204  0.81761674
 [32,]  0.087858510 -0.178822107  0.09096360
 [33,] -0.333588534  0.131984679  0.20160385
 [34,]  0.243027695 -0.297490854  0.05446316
 [35,]  0.941113276 -0.123811956 -0.81730132
 [36,]  0.487587998 -0.372731359 -0.11485664
 [37,]  0.335641023 -0.077106846 -0.25853418
 [38,]  0.100217067 -0.123059251  0.02284218
 [39,] -0.724112185 -0.010327119  0.73443930
 [40,] -0.562922364  0.089109456  0.47381291
 [41,]  0.350862320 -0.423810388  0.07294807
 [42,]  0.008393788  0.219556391 -0.22795018
 [43,] -0.312843123 -0.257588265  0.57043139
 [44,] -0.365312915 -0.017144468  0.38245738
 [45,]  0.548238351 -0.399505980 -0.14873237
 [46,]  0.574519987 -0.353258502 -0.22126149
 [47,] -0.449831333  0.506119565 -0.05628823
 [48,] -0.555823626  0.097404449  0.45841918
 [49,]  0.241396698 -0.267066183  0.02566948
 [50,] -0.380206337 -0.219081579  0.59928792
 [51,] -0.300257370  0.211996513  0.08826086
 [52,]  0.466453230 -0.079530941 -0.38692229
 [53,] -0.193064393 -0.023602453  0.21666685
 [54,]  0.919705645 -0.390209958 -0.52949569
 [55,] -0.809847509 -0.103154862  0.91300237
 [56,] -1.289140734  0.059490092  1.22965064
 [57,]  0.116960836  0.140637000 -0.25759784
 [58,]  0.830820219 -0.319837461 -0.51098276
 [59,] -0.306192689 -0.294111569  0.60030426
 [60,] -0.115879990  0.390078581 -0.27419859
 [61,] -1.842437220  0.341874533  1.50056269
 [62,]  0.073587412  0.018095201 -0.09168261
 [63,] -0.639291862  0.327963006  0.31132886
 [64,] -0.133780880  0.193799949 -0.06001907
 [65,]  0.596057186 -0.242036487 -0.35402070
 [66,] -0.359385451  0.526978302 -0.16759285
 [67,]  1.349857935 -0.297753606 -1.05210433
 [68,] -0.887194626 -0.078565048  0.96575967
 [69,] -0.123733684 -0.001282349  0.12501603
 [70,] -1.631896918  0.324998003  1.30689891
 [71,] -0.596031428  0.541001003  0.05503042
 [72,]  0.520428449 -0.095124355 -0.42530409
 [73,] -0.511147915  0.157920352  0.35322756
 [74,]  0.673535983 -0.140550427 -0.53298556
 [75,]  0.891218040 -0.256749966 -0.63446807
 [76,] -0.194505880  0.009975081  0.18453080
 [77,]  0.233643831  0.119342253 -0.35298608
 [78,] -0.231981552  0.449835328 -0.21785378
 [79,] -0.179764343  0.248767183 -0.06900284
 [80,]  0.343900853  0.230880440 -0.57478129
 [81,]  0.567125748 -0.329732960 -0.23739279
 [82,]  0.032520234  0.207137135 -0.23965737
 [83,] -0.892383943  0.175214038  0.71716990
 [84,]  1.261282581 -0.530589554 -0.73069303
 [85,] -0.469775989  0.269060179  0.20071581
 [86,] -0.244576237  0.052046010  0.19253023
 [87,] -0.982701814  0.400159470  0.58254234
 [88,]  0.320490552 -0.216127366 -0.10436319
 [89,] -0.257470139 -0.131119515  0.38858965
 [90,] -0.119408468 -0.178474135  0.29788260
 [91,]  0.456642570 -0.093858872 -0.36278370
 [92,] -0.912921916  0.311531068  0.60139085
 [93,] -0.714922914  0.018566190  0.69635672
 [94,] -0.585027455  0.287816378  0.29721108
 [95,] -1.044737862  0.020091012  1.02464685
 [96,] -0.304849431 -0.195674630  0.50052406
 [97,]  0.804305271  0.187081335 -0.99138661
 [98,]  0.532689074 -0.317330752 -0.21535832
 [99,]  0.811244074 -0.095096659 -0.71614741
[100,]  0.413934086 -0.138082370 -0.27585172
> dat.m1 <- mbst(x, y, ctrl = bst_control(twinboost=TRUE, 
+ f.init=predict(dat.m), xselect.init = dat.m$xselect, mstop=50))
> dat.m2 <- rmbst(x, y, ctrl = bst_control(mstop=50, s=1, trace=TRUE), 
+ rfamily = "thinge", learner = "ls")

generate initial values

robust boosting ...

initial loss 1.982429 
m= 10   risk =  1.824293 
m= 20   risk =  1.660903 
m= 30   risk =  1.55308 
m= 40   risk =  1.495267 
m= 50   risk =  1.456325 
check if the difference of loss value between thingeDC and thinge is non-negative as expected:  0 

iteration 1 : relative change of fk 1352.947 , robust loss value 1.456325 
m= 10   risk =  1.824293 
m= 20   risk =  1.660903 
m= 30   risk =  1.55308 
m= 40   risk =  1.495267 
m= 50   risk =  1.456325 
check if the difference of loss value between thingeDC and thinge is non-negative as expected:  0 

iteration 2 : relative change of fk 0 , robust loss value 1.456325 
> predict(dat.m2)
              [,1]          [,2]         [,3]
  [1,]  0.74153977 -0.2443000246 -0.497239743
  [2,] -0.20054946  0.0379538858  0.162595572
  [3,]  0.99918856 -0.3459654215 -0.653223137
  [4,] -1.71711071  0.2796374241  1.437473290
  [5,] -0.27190065 -0.1077830173  0.379683665
  [6,]  0.66048087  0.3014713914 -0.961952265
  [7,] -0.60516009  0.2464548265  0.358705263
  [8,] -0.89552457  0.3310508166  0.564473756
  [9,] -0.65915578  0.1797341765  0.479421603
 [10,]  0.12246123  0.3589670329 -0.481428265
 [11,] -1.53262736  0.0757868538  1.456840505
 [12,] -0.35936220 -0.0521701354  0.411532331
 [13,]  0.48876415  0.2509423305 -0.739706476
 [14,]  2.43578204 -0.4921113588 -1.943670677
 [15,] -1.17244159  0.1203913768  1.052050217
 [16,]  0.09513986 -0.1014258699  0.006286009
 [17,]  0.05577432 -0.0795332378  0.023758921
 [18,] -0.97103787  0.0756891997  0.895348669
 [19,] -0.93366029  0.2433988062  0.690261489
 [20,] -0.61081291  0.0472171475  0.563595762
 [21,] -0.91725547  0.0172814480  0.899974020
 [22,] -0.99429465  0.4419558435  0.552338807
 [23,] -0.05352528 -0.0403977759  0.093923057
 [24,]  2.13916756 -0.3444966193 -1.794670940
 [25,] -0.64769877  0.0697246674  0.577974098
 [26,] -0.02608986  0.1631713503 -0.137081492
 [27,]  0.17469493 -0.0413124273 -0.133382503
 [28,]  1.57005730 -0.2317790126 -1.338278292
 [29,]  0.59106202 -0.2366074179 -0.354454602
 [30,] -0.40581686 -0.0148237689  0.420640627
 [31,] -1.45347586  0.2202342639  1.233241596
 [32,]  0.18033916 -0.1574209859 -0.022918173
 [33,] -0.47666715  0.1867340561  0.289933098
 [34,]  0.24015152 -0.3739063996  0.133754879
 [35,]  1.42887171 -0.1346804729 -1.294191239
 [36,]  0.62679001 -0.4329500681 -0.193839943
 [37,]  0.45594752 -0.1322073679 -0.323740154
 [38,]  0.12676285 -0.1362365820  0.009473728
 [39,] -1.09236865  0.0094949041  1.082873748
 [40,] -0.80550038  0.1018607893  0.703639586
 [41,]  0.40569566 -0.4860545452  0.080358887
 [42,]  0.12797696  0.2450554878 -0.373032452
 [43,] -0.54134053 -0.2955274036  0.836867937
 [44,] -0.53670457 -0.0273631070  0.564067673
 [45,]  0.86754202 -0.3731058213 -0.494436196
 [46,]  0.84351527 -0.2879960213 -0.555519247
 [47,] -0.63945871  0.5579757309  0.081482976
 [48,] -0.82015073  0.1205679628  0.699582766
 [49,]  0.27451100 -0.3268632663  0.052352266
 [50,] -0.74028557 -0.2617731143  1.002058684
 [51,] -0.47798070  0.1687279356  0.309252767
 [52,]  0.65369947 -0.0971488908 -0.556550579
 [53,] -0.32479172 -0.0249628259  0.349754550
 [54,]  1.31407169 -0.3948871722 -0.919184519
 [55,] -1.34621105 -0.1413069996  1.487518054
 [56,] -1.97851366  0.0408905320  1.937623129
 [57,]  0.27043834  0.1852845119 -0.455722848
 [58,]  1.18624345 -0.3077655104 -0.878477938
 [59,] -0.43968712 -0.2472163998  0.686903521
 [60,] -0.08138507  0.4298335239 -0.348448455
 [61,] -2.60757312  0.4660732580  2.141499866
 [62,]  0.07051183 -0.0634286214 -0.007083212
 [63,] -0.86167120  0.3594190346  0.502252165
 [64,] -0.13656965  0.2177664940 -0.081196848
 [65,]  0.86575143 -0.2617191187 -0.604032308
 [66,] -0.46666649  0.5600061189 -0.093339631
 [67,]  1.95198246 -0.3347540286 -1.617228433
 [68,] -1.38842220 -0.1212133611  1.509635565
 [69,] -0.14573679 -0.0115768512  0.157313639
 [70,] -2.33760662  0.3789877670  1.958618854
 [71,] -0.78412937  0.6279636277  0.156165746
 [72,]  0.74295075 -0.0820197286 -0.660931026
 [73,] -0.70512241  0.2025655261  0.502556882
 [74,]  1.00358140 -0.1600337988 -0.843547605
 [75,]  1.37464282 -0.2702914305 -1.104351389
 [76,] -0.30604392  0.0357682110  0.270275707
 [77,]  0.37699126  0.1226038699 -0.499595134
 [78,] -0.25112457  0.5000633725 -0.248938799
 [79,] -0.20287358  0.2587411016 -0.055867519
 [80,]  0.48202474  0.2016972229 -0.683721966
 [81,]  0.75361639 -0.3827202211 -0.370896166
 [82,]  0.02538625  0.2165361031 -0.241922358
 [83,] -1.28048875  0.2313723297  1.049116417
 [84,]  1.79846168 -0.5841500357 -1.214311648
 [85,] -0.69497218  0.2154469591  0.479525217
 [86,] -0.33528416  0.0121764513  0.323107711
 [87,] -1.30800250  0.5137809090  0.794221593
 [88,]  0.41605383 -0.2305999293 -0.185453902
 [89,] -0.34204620 -0.0476003725  0.389646568
 [90,] -0.17276699 -0.1826398440  0.355406837
 [91,]  0.59880974 -0.1248146104 -0.473995129
 [92,] -1.33411960  0.2797440598  1.054375545
 [93,] -1.14704281 -0.0005496044  1.147592417
 [94,] -0.84534627  0.3060568236  0.539289447
 [95,] -1.54358095 -0.0507179190  1.594298868
 [96,] -0.46825209 -0.1678733915  0.636125483
 [97,]  1.18527626  0.1538415346 -1.339117791
 [98,]  0.73255678 -0.3315113009 -0.401045474
 [99,]  1.25390473 -0.0862023270 -1.167702402
[100,]  0.54980338 -0.1634818553 -0.386321520
> 
> 
> 
> cleanEx()
> nameEx("mhingebst")
> ### * mhingebst
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mhingebst
> ### Title: Boosting for Multi-class Classification
> ### Aliases: mhingebst print.mhingebst predict.mhingebst fpartial.mhingebst
> ### Keywords: classification
> 
> ### ** Examples
> 
> ## Not run: 
> ##D dat <- ex1data(100, p=5)
> ##D res <- mhingebst(x=dat$x, y=dat$y)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("mhingeova")
> ### * mhingeova
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mhingeova
> ### Title: Multi-class HingeBoost
> ### Aliases: mhingeova print.mhingeova
> ### Keywords: classification
> 
> ### ** Examples
> 
> ## Not run: 
> ##D dat1 <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/
> ##D thyroid-disease/ann-train.data")
> ##D dat2 <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/
> ##D thyroid-disease/ann-test.data")
> ##D res <- mhingeova(xtr=dat1[,-22], ytr=dat1[,22], xte=dat2[,-22], yte=dat2[,22], 
> ##D cost=c(2/3, 0.5, 0.5), nu=0.5, learner="ls", m1=100, K=5, cv1=FALSE, 
> ##D twinboost=TRUE, m2= 200, cv2=FALSE)
> ##D res <- mhingeova(xtr=dat1[,-22], ytr=dat1[,22], xte=dat2[,-22], yte=dat2[,22], 
> ##D cost=c(2/3, 0.5, 0.5), nu=0.5, learner="ls", m1=100, K=5, cv1=FALSE, 
> ##D twinboost=TRUE, m2= 200, cv2=TRUE)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("rbst")
> ### * rbst
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rbst
> ### Title: Robust Boosting for Robust Loss Functions
> ### Aliases: rbst
> ### Keywords: classification
> 
> ### ** Examples
> 
> x <- matrix(rnorm(100*5),ncol=5)
> c <- 2*x[,1]
> p <- exp(c)/(exp(c)+exp(-c))
> y <- rbinom(100,1,p)
> y[y != 1] <- -1
> y[1:10] <- -y[1:10]
> x <- as.data.frame(x)
> dat.m <- bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
> predict(dat.m)
           1            2            3            4            5            6 
-0.684150122  0.200556850 -0.912589895  1.742205949  0.359855393 -0.896033412 
           7            8            9           10           11           12 
 0.532321203  0.806324311  0.628810737 -0.333514617  1.651016011  0.425747746 
          13           14           15           16           17           18 
-0.678456754 -2.418673449  1.228536904 -0.049071988 -0.017681384  1.030763399 
          19           20           21           22           23           24 
 0.896855557  0.648599553  1.003615064  0.854170949  0.081432408 -2.172570719 
          25           26           27           28           29           30 
 0.676911616 -0.061298189 -0.170144252 -1.606208481 -0.522187611  0.456433922 
          31           32           33           34           35           36 
 1.483813755 -0.112254463  0.423376115 -0.058760478 -1.503886557 -0.453215507 
          37           38           39           40           41           42 
-0.430604005 -0.064776153  1.201337561  0.833464132 -0.179676197 -0.276696257 
          43           44           45           46           47           48 
 0.761153608  0.607931804 -0.752190001 -0.772655366  0.398159912  0.839314703 
          49           50           51           52           53           54 
-0.122693283  0.962257629  0.434771378 -0.668393942  0.372536769 -1.233377285 
          55           56           57           58           59           60 
 1.565004992  2.162794463 -0.401042525 -1.140299285  0.622190728 -0.147493115 
          61           62           63           64           65           66 
 2.622806433 -0.042854002  0.753264265  0.030581154 -0.811728571  0.206180045 
          67           68           69           70           71           72 
-1.971195076  1.600532267  0.167367950  2.372708913  0.519303893 -0.775332402 
          73           74           75           76           77           78 
 0.666974168 -1.020127898 -1.369092868  0.318288395 -0.484118995  0.001207154 
          79           80           81           82           83           84 
 0.081188150 -0.643815746 -0.621043047 -0.147628547  1.286588650 -1.663887098 
          85           86           87           88           89           90 
 0.648648552  0.363615056  1.161011189 -0.332199222  0.404097493  0.291698553 
          91           92           93           94           95           96 
-0.592486053  1.319112268  1.267275540  0.764703232  1.732980602  0.609922950 
          97           98           99          100 
-1.394166179 -0.626063082 -1.337399272 -0.517000771 
> dat.m1 <- bst(x, y, ctrl = bst_control(twinboost=TRUE, 
+ coefir=coef(dat.m), xselect.init = dat.m$xselect, mstop=50))
> dat.m2 <- rbst(x, y, ctrl = bst_control(mstop=50, s=0, trace=TRUE), 
+ rfamily = "thinge", learner = "ls")

generate initial values

robust boosting ...

initial loss 0.4890911 

m= 10   risk =  0.3751262
m= 20   risk =  0.288524
m= 30   risk =  0.2505538
m= 40   risk =  0.229095
m= 50   risk =  0.2127517
iteration 1 : los[k] <= ellu2 0
iteration 1 : ellu2 <= ellu1 -0.2763394
iteration 1 : relative change of fk 1243.437 , robust loss value 0.2127517 
d1= 1243.437 , k= 1 , d1 > del && k <= iter:  TRUE 

m= 10   risk =  0.3751262
m= 20   risk =  0.288524
m= 30   risk =  0.2505538
m= 40   risk =  0.229095
m= 50   risk =  0.2127517
iteration 2 : los[k] <= ellu2 0
iteration 2 : ellu2 <= ellu1 0
iteration 2 : relative change of fk 0 , robust loss value 0.2127517 
d1= 0 , k= 2 , d1 > del && k <= iter:  FALSE 
> predict(dat.m2)
           1            2            3            4            5            6 
-0.778935628  0.228342977 -1.039024565  1.983579688  0.409711521 -1.020174266 
           7            8            9           10           11           12 
 0.606071587  0.918036427  0.715929253 -0.379721364  1.879755849  0.484732922 
          13           14           15           16           17           18 
-0.772453473 -2.753768305  1.398744419 -0.055870662 -0.020131050  1.173570404 
          19           20           21           22           23           24 
 1.021110315  0.738459709  1.142660806  0.972511972  0.092714453 -2.473569299 
          25           26           27           28           29           30 
 0.770694264 -0.069790740 -0.193716869 -1.828740464 -0.594534038  0.519670511 
          31           32           33           34           35           36 
 1.689388575 -0.127806746  0.482032714 -0.066901442 -1.712242360 -0.516006201 
          37           38           39           40           41           42 
-0.490261992 -0.073750558  1.367776746  0.948936332 -0.204569417 -0.315031110 
          43           44           45           46           47           48 
 0.866607554  0.692157652 -0.856402085 -0.879702821  0.453322935  0.955597470 
          49           50           51           52           53           54 
-0.139691811  1.095573511  0.495006733 -0.760996509  0.424149836 -1.404255410 
          55           56           57           58           59           60 
 1.781828442  2.462438592 -0.456604919 -1.298281928  0.708392077 -0.167927533 
          61           62           63           64           65           66 
 2.986182872 -0.048791205  0.857625183  0.034818017 -0.924189420  0.234745237 
          67           68           69           70           71           72 
-2.244294088  1.822277840  0.190555925  2.701435617  0.591250795 -0.882750747 
          73           74           75           76           77           78 
 0.759380034 -1.161461409 -1.558773694  0.362385626 -0.551191210  0.001374399 
          79           80           81           82           83           84 
 0.092436354 -0.733013130 -0.707085390 -0.168081729  1.464838938 -1.894410159 
          85           86           87           88           89           90 
 0.738515496  0.413992065  1.321863361 -0.378223728  0.460083137  0.332111899 
          91           92           93           94           95           96 
-0.674571970  1.501868538  1.442850096  0.870648961  1.973076216  0.694424661 
          97           98           99          100 
-1.587320784 -0.712800925 -1.522689112 -0.588628588 
> 
> 
> 
> cleanEx()
> nameEx("rbstpath")
> ### * rbstpath
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rbstpath
> ### Title: Robust Boosting Path for Nonconvex Loss Functions
> ### Aliases: rbstpath
> ### Keywords: classification
> 
> ### ** Examples
> 
> x <- matrix(rnorm(100*5),ncol=5)
> c <- 2*x[,1]
> p <- exp(c)/(exp(c)+exp(-c))
> y <- rbinom(100,1,p)
> y[y != 1] <- -1
> y[1:10] <- -y[1:10]
> x <- as.data.frame(x)
> dat.m <- bst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
> predict(dat.m)
           1            2            3            4            5            6 
-0.684150122  0.200556850 -0.912589895  1.742205949  0.359855393 -0.896033412 
           7            8            9           10           11           12 
 0.532321203  0.806324311  0.628810737 -0.333514617  1.651016011  0.425747746 
          13           14           15           16           17           18 
-0.678456754 -2.418673449  1.228536904 -0.049071988 -0.017681384  1.030763399 
          19           20           21           22           23           24 
 0.896855557  0.648599553  1.003615064  0.854170949  0.081432408 -2.172570719 
          25           26           27           28           29           30 
 0.676911616 -0.061298189 -0.170144252 -1.606208481 -0.522187611  0.456433922 
          31           32           33           34           35           36 
 1.483813755 -0.112254463  0.423376115 -0.058760478 -1.503886557 -0.453215507 
          37           38           39           40           41           42 
-0.430604005 -0.064776153  1.201337561  0.833464132 -0.179676197 -0.276696257 
          43           44           45           46           47           48 
 0.761153608  0.607931804 -0.752190001 -0.772655366  0.398159912  0.839314703 
          49           50           51           52           53           54 
-0.122693283  0.962257629  0.434771378 -0.668393942  0.372536769 -1.233377285 
          55           56           57           58           59           60 
 1.565004992  2.162794463 -0.401042525 -1.140299285  0.622190728 -0.147493115 
          61           62           63           64           65           66 
 2.622806433 -0.042854002  0.753264265  0.030581154 -0.811728571  0.206180045 
          67           68           69           70           71           72 
-1.971195076  1.600532267  0.167367950  2.372708913  0.519303893 -0.775332402 
          73           74           75           76           77           78 
 0.666974168 -1.020127898 -1.369092868  0.318288395 -0.484118995  0.001207154 
          79           80           81           82           83           84 
 0.081188150 -0.643815746 -0.621043047 -0.147628547  1.286588650 -1.663887098 
          85           86           87           88           89           90 
 0.648648552  0.363615056  1.161011189 -0.332199222  0.404097493  0.291698553 
          91           92           93           94           95           96 
-0.592486053  1.319112268  1.267275540  0.764703232  1.732980602  0.609922950 
          97           98           99          100 
-1.394166179 -0.626063082 -1.337399272 -0.517000771 
> dat.m1 <- bst(x, y, ctrl = bst_control(twinboost=TRUE, 
+ coefir=coef(dat.m), xselect.init = dat.m$xselect, mstop=50))
> dat.m2 <- rbst(x, y, ctrl = bst_control(mstop=50, s=0, trace=TRUE), 
+ rfamily = "thinge", learner = "ls")

generate initial values

robust boosting ...

initial loss 0.4890911 

m= 10   risk =  0.3751262
m= 20   risk =  0.288524
m= 30   risk =  0.2505538
m= 40   risk =  0.229095
m= 50   risk =  0.2127517
iteration 1 : los[k] <= ellu2 0
iteration 1 : ellu2 <= ellu1 -0.2763394
iteration 1 : relative change of fk 1243.437 , robust loss value 0.2127517 
d1= 1243.437 , k= 1 , d1 > del && k <= iter:  TRUE 

m= 10   risk =  0.3751262
m= 20   risk =  0.288524
m= 30   risk =  0.2505538
m= 40   risk =  0.229095
m= 50   risk =  0.2127517
iteration 2 : los[k] <= ellu2 0
iteration 2 : ellu2 <= ellu1 0
iteration 2 : relative change of fk 0 , robust loss value 0.2127517 
d1= 0 , k= 2 , d1 > del && k <= iter:  FALSE 
> predict(dat.m2)
           1            2            3            4            5            6 
-0.778935628  0.228342977 -1.039024565  1.983579688  0.409711521 -1.020174266 
           7            8            9           10           11           12 
 0.606071587  0.918036427  0.715929253 -0.379721364  1.879755849  0.484732922 
          13           14           15           16           17           18 
-0.772453473 -2.753768305  1.398744419 -0.055870662 -0.020131050  1.173570404 
          19           20           21           22           23           24 
 1.021110315  0.738459709  1.142660806  0.972511972  0.092714453 -2.473569299 
          25           26           27           28           29           30 
 0.770694264 -0.069790740 -0.193716869 -1.828740464 -0.594534038  0.519670511 
          31           32           33           34           35           36 
 1.689388575 -0.127806746  0.482032714 -0.066901442 -1.712242360 -0.516006201 
          37           38           39           40           41           42 
-0.490261992 -0.073750558  1.367776746  0.948936332 -0.204569417 -0.315031110 
          43           44           45           46           47           48 
 0.866607554  0.692157652 -0.856402085 -0.879702821  0.453322935  0.955597470 
          49           50           51           52           53           54 
-0.139691811  1.095573511  0.495006733 -0.760996509  0.424149836 -1.404255410 
          55           56           57           58           59           60 
 1.781828442  2.462438592 -0.456604919 -1.298281928  0.708392077 -0.167927533 
          61           62           63           64           65           66 
 2.986182872 -0.048791205  0.857625183  0.034818017 -0.924189420  0.234745237 
          67           68           69           70           71           72 
-2.244294088  1.822277840  0.190555925  2.701435617  0.591250795 -0.882750747 
          73           74           75           76           77           78 
 0.759380034 -1.161461409 -1.558773694  0.362385626 -0.551191210  0.001374399 
          79           80           81           82           83           84 
 0.092436354 -0.733013130 -0.707085390 -0.168081729  1.464838938 -1.894410159 
          85           86           87           88           89           90 
 0.738515496  0.413992065  1.321863361 -0.378223728  0.460083137  0.332111899 
          91           92           93           94           95           96 
-0.674571970  1.501868538  1.442850096  0.870648961  1.973076216  0.694424661 
          97           98           99          100 
-1.587320784 -0.712800925 -1.522689112 -0.588628588 
> rmstop <- seq(10, 40, by=10)
> dat.m3 <- rbstpath(x, y, rmstop, ctrl=bst_control(s=0), rfamily = "thinge", learner = "ls")
> 
> 
> 
> cleanEx()
> nameEx("rmbst")
> ### * rmbst
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rmbst
> ### Title: Robust Boosting for Multi-class Robust Loss Functions
> ### Aliases: rmbst
> ### Keywords: classification
> 
> ### ** Examples
> 
> x <- matrix(rnorm(100*5),ncol=5)
> c <- quantile(x[,1], prob=c(0.33, 0.67))
> y <- rep(1, 100)
> y[x[,1] > c[1] & x[,1] < c[2] ] <- 2
> y[x[,1] > c[2]] <- 3
> x <- as.data.frame(x)
> x <- as.data.frame(x)
> dat.m <- mbst(x, y, ctrl = bst_control(mstop=50), family = "hinge", learner = "ls")
> predict(dat.m)
               [,1]         [,2]        [,3]
  [1,]  0.533923303 -0.176913579 -0.35700972
  [2,] -0.183114746  0.036722306  0.14639244
  [3,]  0.761315876 -0.266408065 -0.49490781
  [4,] -1.170776228  0.237407081  0.93336915
  [5,] -0.114177829 -0.055433159  0.16961099
  [6,]  0.444845898  0.306903585 -0.75174948
  [7,] -0.422252922  0.219014408  0.20323851
  [8,] -0.610003424  0.310974800  0.29902862
  [9,] -0.497309324  0.158073629  0.33923569
 [10,]  0.052365316  0.319786285 -0.37215160
 [11,] -0.983383733  0.095197094  0.88818664
 [12,] -0.232703638 -0.032010246  0.26471388
 [13,]  0.268151030  0.230587867 -0.49873890
 [14,]  1.648933588 -0.480122548 -1.16881104
 [15,] -0.782228493  0.101107356  0.68112114
 [16,]  0.101372502 -0.051388497 -0.04998401
 [17,]  0.023546522 -0.068188913  0.04464239
 [18,] -0.649046690  0.062411250  0.58663544
 [19,] -0.668709625  0.158821672  0.50988795
 [20,] -0.407697527  0.087488828  0.32020870
 [21,] -0.579624764  0.080168389  0.49945638
 [22,] -0.657868029  0.430739819  0.22712821
 [23,] -0.006111179 -0.013678090  0.01978927
 [24,]  1.409257909 -0.398919358 -1.01033855
 [25,] -0.425260931  0.087895464  0.33736547
 [26,] -0.037539186  0.187626034 -0.15008685
 [27,]  0.123678133 -0.075875043 -0.04780309
 [28,]  1.072138457 -0.220322486 -0.85181597
 [29,]  0.397781546 -0.250078994 -0.14770255
 [30,] -0.239508008 -0.004394365  0.24390237
 [31,] -0.974487941  0.156871204  0.81761674
 [32,]  0.087858510 -0.178822107  0.09096360
 [33,] -0.333588534  0.131984679  0.20160385
 [34,]  0.243027695 -0.297490854  0.05446316
 [35,]  0.941113276 -0.123811956 -0.81730132
 [36,]  0.487587998 -0.372731359 -0.11485664
 [37,]  0.335641023 -0.077106846 -0.25853418
 [38,]  0.100217067 -0.123059251  0.02284218
 [39,] -0.724112185 -0.010327119  0.73443930
 [40,] -0.562922364  0.089109456  0.47381291
 [41,]  0.350862320 -0.423810388  0.07294807
 [42,]  0.008393788  0.219556391 -0.22795018
 [43,] -0.312843123 -0.257588265  0.57043139
 [44,] -0.365312915 -0.017144468  0.38245738
 [45,]  0.548238351 -0.399505980 -0.14873237
 [46,]  0.574519987 -0.353258502 -0.22126149
 [47,] -0.449831333  0.506119565 -0.05628823
 [48,] -0.555823626  0.097404449  0.45841918
 [49,]  0.241396698 -0.267066183  0.02566948
 [50,] -0.380206337 -0.219081579  0.59928792
 [51,] -0.300257370  0.211996513  0.08826086
 [52,]  0.466453230 -0.079530941 -0.38692229
 [53,] -0.193064393 -0.023602453  0.21666685
 [54,]  0.919705645 -0.390209958 -0.52949569
 [55,] -0.809847509 -0.103154862  0.91300237
 [56,] -1.289140734  0.059490092  1.22965064
 [57,]  0.116960836  0.140637000 -0.25759784
 [58,]  0.830820219 -0.319837461 -0.51098276
 [59,] -0.306192689 -0.294111569  0.60030426
 [60,] -0.115879990  0.390078581 -0.27419859
 [61,] -1.842437220  0.341874533  1.50056269
 [62,]  0.073587412  0.018095201 -0.09168261
 [63,] -0.639291862  0.327963006  0.31132886
 [64,] -0.133780880  0.193799949 -0.06001907
 [65,]  0.596057186 -0.242036487 -0.35402070
 [66,] -0.359385451  0.526978302 -0.16759285
 [67,]  1.349857935 -0.297753606 -1.05210433
 [68,] -0.887194626 -0.078565048  0.96575967
 [69,] -0.123733684 -0.001282349  0.12501603
 [70,] -1.631896918  0.324998003  1.30689891
 [71,] -0.596031428  0.541001003  0.05503042
 [72,]  0.520428449 -0.095124355 -0.42530409
 [73,] -0.511147915  0.157920352  0.35322756
 [74,]  0.673535983 -0.140550427 -0.53298556
 [75,]  0.891218040 -0.256749966 -0.63446807
 [76,] -0.194505880  0.009975081  0.18453080
 [77,]  0.233643831  0.119342253 -0.35298608
 [78,] -0.231981552  0.449835328 -0.21785378
 [79,] -0.179764343  0.248767183 -0.06900284
 [80,]  0.343900853  0.230880440 -0.57478129
 [81,]  0.567125748 -0.329732960 -0.23739279
 [82,]  0.032520234  0.207137135 -0.23965737
 [83,] -0.892383943  0.175214038  0.71716990
 [84,]  1.261282581 -0.530589554 -0.73069303
 [85,] -0.469775989  0.269060179  0.20071581
 [86,] -0.244576237  0.052046010  0.19253023
 [87,] -0.982701814  0.400159470  0.58254234
 [88,]  0.320490552 -0.216127366 -0.10436319
 [89,] -0.257470139 -0.131119515  0.38858965
 [90,] -0.119408468 -0.178474135  0.29788260
 [91,]  0.456642570 -0.093858872 -0.36278370
 [92,] -0.912921916  0.311531068  0.60139085
 [93,] -0.714922914  0.018566190  0.69635672
 [94,] -0.585027455  0.287816378  0.29721108
 [95,] -1.044737862  0.020091012  1.02464685
 [96,] -0.304849431 -0.195674630  0.50052406
 [97,]  0.804305271  0.187081335 -0.99138661
 [98,]  0.532689074 -0.317330752 -0.21535832
 [99,]  0.811244074 -0.095096659 -0.71614741
[100,]  0.413934086 -0.138082370 -0.27585172
> dat.m1 <- mbst(x, y, ctrl = bst_control(twinboost=TRUE, 
+ f.init=predict(dat.m), xselect.init = dat.m$xselect, mstop=50))
> dat.m2 <- rmbst(x, y, ctrl = bst_control(mstop=50, s=1, trace=TRUE), 
+ rfamily = "thinge", learner = "ls")

generate initial values

robust boosting ...

initial loss 1.982429 
m= 10   risk =  1.824293 
m= 20   risk =  1.660903 
m= 30   risk =  1.55308 
m= 40   risk =  1.495267 
m= 50   risk =  1.456325 
check if the difference of loss value between thingeDC and thinge is non-negative as expected:  0 

iteration 1 : relative change of fk 1352.947 , robust loss value 1.456325 
m= 10   risk =  1.824293 
m= 20   risk =  1.660903 
m= 30   risk =  1.55308 
m= 40   risk =  1.495267 
m= 50   risk =  1.456325 
check if the difference of loss value between thingeDC and thinge is non-negative as expected:  0 

iteration 2 : relative change of fk 0 , robust loss value 1.456325 
> predict(dat.m2)
              [,1]          [,2]         [,3]
  [1,]  0.74153977 -0.2443000246 -0.497239743
  [2,] -0.20054946  0.0379538858  0.162595572
  [3,]  0.99918856 -0.3459654215 -0.653223137
  [4,] -1.71711071  0.2796374241  1.437473290
  [5,] -0.27190065 -0.1077830173  0.379683665
  [6,]  0.66048087  0.3014713914 -0.961952265
  [7,] -0.60516009  0.2464548265  0.358705263
  [8,] -0.89552457  0.3310508166  0.564473756
  [9,] -0.65915578  0.1797341765  0.479421603
 [10,]  0.12246123  0.3589670329 -0.481428265
 [11,] -1.53262736  0.0757868538  1.456840505
 [12,] -0.35936220 -0.0521701354  0.411532331
 [13,]  0.48876415  0.2509423305 -0.739706476
 [14,]  2.43578204 -0.4921113588 -1.943670677
 [15,] -1.17244159  0.1203913768  1.052050217
 [16,]  0.09513986 -0.1014258699  0.006286009
 [17,]  0.05577432 -0.0795332378  0.023758921
 [18,] -0.97103787  0.0756891997  0.895348669
 [19,] -0.93366029  0.2433988062  0.690261489
 [20,] -0.61081291  0.0472171475  0.563595762
 [21,] -0.91725547  0.0172814480  0.899974020
 [22,] -0.99429465  0.4419558435  0.552338807
 [23,] -0.05352528 -0.0403977759  0.093923057
 [24,]  2.13916756 -0.3444966193 -1.794670940
 [25,] -0.64769877  0.0697246674  0.577974098
 [26,] -0.02608986  0.1631713503 -0.137081492
 [27,]  0.17469493 -0.0413124273 -0.133382503
 [28,]  1.57005730 -0.2317790126 -1.338278292
 [29,]  0.59106202 -0.2366074179 -0.354454602
 [30,] -0.40581686 -0.0148237689  0.420640627
 [31,] -1.45347586  0.2202342639  1.233241596
 [32,]  0.18033916 -0.1574209859 -0.022918173
 [33,] -0.47666715  0.1867340561  0.289933098
 [34,]  0.24015152 -0.3739063996  0.133754879
 [35,]  1.42887171 -0.1346804729 -1.294191239
 [36,]  0.62679001 -0.4329500681 -0.193839943
 [37,]  0.45594752 -0.1322073679 -0.323740154
 [38,]  0.12676285 -0.1362365820  0.009473728
 [39,] -1.09236865  0.0094949041  1.082873748
 [40,] -0.80550038  0.1018607893  0.703639586
 [41,]  0.40569566 -0.4860545452  0.080358887
 [42,]  0.12797696  0.2450554878 -0.373032452
 [43,] -0.54134053 -0.2955274036  0.836867937
 [44,] -0.53670457 -0.0273631070  0.564067673
 [45,]  0.86754202 -0.3731058213 -0.494436196
 [46,]  0.84351527 -0.2879960213 -0.555519247
 [47,] -0.63945871  0.5579757309  0.081482976
 [48,] -0.82015073  0.1205679628  0.699582766
 [49,]  0.27451100 -0.3268632663  0.052352266
 [50,] -0.74028557 -0.2617731143  1.002058684
 [51,] -0.47798070  0.1687279356  0.309252767
 [52,]  0.65369947 -0.0971488908 -0.556550579
 [53,] -0.32479172 -0.0249628259  0.349754550
 [54,]  1.31407169 -0.3948871722 -0.919184519
 [55,] -1.34621105 -0.1413069996  1.487518054
 [56,] -1.97851366  0.0408905320  1.937623129
 [57,]  0.27043834  0.1852845119 -0.455722848
 [58,]  1.18624345 -0.3077655104 -0.878477938
 [59,] -0.43968712 -0.2472163998  0.686903521
 [60,] -0.08138507  0.4298335239 -0.348448455
 [61,] -2.60757312  0.4660732580  2.141499866
 [62,]  0.07051183 -0.0634286214 -0.007083212
 [63,] -0.86167120  0.3594190346  0.502252165
 [64,] -0.13656965  0.2177664940 -0.081196848
 [65,]  0.86575143 -0.2617191187 -0.604032308
 [66,] -0.46666649  0.5600061189 -0.093339631
 [67,]  1.95198246 -0.3347540286 -1.617228433
 [68,] -1.38842220 -0.1212133611  1.509635565
 [69,] -0.14573679 -0.0115768512  0.157313639
 [70,] -2.33760662  0.3789877670  1.958618854
 [71,] -0.78412937  0.6279636277  0.156165746
 [72,]  0.74295075 -0.0820197286 -0.660931026
 [73,] -0.70512241  0.2025655261  0.502556882
 [74,]  1.00358140 -0.1600337988 -0.843547605
 [75,]  1.37464282 -0.2702914305 -1.104351389
 [76,] -0.30604392  0.0357682110  0.270275707
 [77,]  0.37699126  0.1226038699 -0.499595134
 [78,] -0.25112457  0.5000633725 -0.248938799
 [79,] -0.20287358  0.2587411016 -0.055867519
 [80,]  0.48202474  0.2016972229 -0.683721966
 [81,]  0.75361639 -0.3827202211 -0.370896166
 [82,]  0.02538625  0.2165361031 -0.241922358
 [83,] -1.28048875  0.2313723297  1.049116417
 [84,]  1.79846168 -0.5841500357 -1.214311648
 [85,] -0.69497218  0.2154469591  0.479525217
 [86,] -0.33528416  0.0121764513  0.323107711
 [87,] -1.30800250  0.5137809090  0.794221593
 [88,]  0.41605383 -0.2305999293 -0.185453902
 [89,] -0.34204620 -0.0476003725  0.389646568
 [90,] -0.17276699 -0.1826398440  0.355406837
 [91,]  0.59880974 -0.1248146104 -0.473995129
 [92,] -1.33411960  0.2797440598  1.054375545
 [93,] -1.14704281 -0.0005496044  1.147592417
 [94,] -0.84534627  0.3060568236  0.539289447
 [95,] -1.54358095 -0.0507179190  1.594298868
 [96,] -0.46825209 -0.1678733915  0.636125483
 [97,]  1.18527626  0.1538415346 -1.339117791
 [98,]  0.73255678 -0.3315113009 -0.401045474
 [99,]  1.25390473 -0.0862023270 -1.167702402
[100,]  0.54980338 -0.1634818553 -0.386321520
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  4.38 0.202 4.645 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
