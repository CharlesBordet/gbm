
R version 4.0.0 (2020-04-24) -- "Arbor Day"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin17.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "MachineShop"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('MachineShop')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> base::assign(".old_wd", base::getwd(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("AdaBagModel")
> ### * AdaBagModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: AdaBagModel
> ### Title: Bagging with Classification Trees
> ### Aliases: AdaBagModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = AdaBagModel(mfinal = 5))
$formula
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
<environment: 0x7fa8570a67c0>

$trees
$trees[[1]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 97 virginica (0.30666667 0.34000000 0.35333333)  
  2) Petal.Length< 2.45 46  0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 104 51 virginica (0.00000000 0.49038462 0.50961538)  
    6) Petal.Width< 1.75 53  3 versicolor (0.00000000 0.94339623 0.05660377) *
    7) Petal.Width>=1.75 51  1 virginica (0.00000000 0.01960784 0.98039216) *

$trees[[2]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 95 versicolor (0.33333333 0.36666667 0.30000000)  
  2) Petal.Length< 2.7 50  0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.7 100 45 versicolor (0.00000000 0.55000000 0.45000000)  
    6) Petal.Width< 1.65 57  3 versicolor (0.00000000 0.94736842 0.05263158) *
    7) Petal.Width>=1.65 43  1 virginica (0.00000000 0.02325581 0.97674419) *

$trees[[3]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 150 93 versicolor (0.29333333 0.38000000 0.32666667)  
   2) Petal.Length< 2.45 44  0 setosa (1.00000000 0.00000000 0.00000000) *
   3) Petal.Length>=2.45 106 49 versicolor (0.00000000 0.53773585 0.46226415)  
     6) Petal.Length< 4.85 51  1 versicolor (0.00000000 0.98039216 0.01960784) *
     7) Petal.Length>=4.85 55  7 virginica (0.00000000 0.12727273 0.87272727)  
      14) Petal.Width< 1.75 13  6 versicolor (0.00000000 0.53846154 0.46153846) *
      15) Petal.Width>=1.75 42  0 virginica (0.00000000 0.00000000 1.00000000) *

$trees[[4]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 94 virginica (0.34000000 0.28666667 0.37333333)  
  2) Petal.Length< 2.45 51  0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 99 43 virginica (0.00000000 0.43434343 0.56565657)  
    6) Petal.Length< 4.75 41  0 versicolor (0.00000000 1.00000000 0.00000000) *
    7) Petal.Length>=4.75 58  2 virginica (0.00000000 0.03448276 0.96551724) *

$trees[[5]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 150 95 virginica (0.2933333 0.3400000 0.3666667)  
   2) Petal.Length< 2.45 44  0 setosa (1.0000000 0.0000000 0.0000000) *
   3) Petal.Length>=2.45 106 51 virginica (0.0000000 0.4811321 0.5188679)  
     6) Petal.Length< 4.95 57  6 versicolor (0.0000000 0.8947368 0.1052632)  
      12) Petal.Width< 1.65 50  0 versicolor (0.0000000 1.0000000 0.0000000) *
      13) Petal.Width>=1.65 7  1 virginica (0.0000000 0.1428571 0.8571429) *
     7) Petal.Length>=4.95 49  0 virginica (0.0000000 0.0000000 1.0000000) *


$votes
       [,1] [,2] [,3]
  [1,]    5    0    0
  [2,]    5    0    0
  [3,]    5    0    0
  [4,]    5    0    0
  [5,]    5    0    0
  [6,]    5    0    0
  [7,]    5    0    0
  [8,]    5    0    0
  [9,]    5    0    0
 [10,]    5    0    0
 [11,]    5    0    0
 [12,]    5    0    0
 [13,]    5    0    0
 [14,]    5    0    0
 [15,]    5    0    0
 [16,]    5    0    0
 [17,]    5    0    0
 [18,]    5    0    0
 [19,]    5    0    0
 [20,]    5    0    0
 [21,]    5    0    0
 [22,]    5    0    0
 [23,]    5    0    0
 [24,]    5    0    0
 [25,]    5    0    0
 [26,]    5    0    0
 [27,]    5    0    0
 [28,]    5    0    0
 [29,]    5    0    0
 [30,]    5    0    0
 [31,]    5    0    0
 [32,]    5    0    0
 [33,]    5    0    0
 [34,]    5    0    0
 [35,]    5    0    0
 [36,]    5    0    0
 [37,]    5    0    0
 [38,]    5    0    0
 [39,]    5    0    0
 [40,]    5    0    0
 [41,]    5    0    0
 [42,]    5    0    0
 [43,]    5    0    0
 [44,]    5    0    0
 [45,]    5    0    0
 [46,]    5    0    0
 [47,]    5    0    0
 [48,]    5    0    0
 [49,]    5    0    0
 [50,]    5    0    0
 [51,]    0    5    0
 [52,]    0    5    0
 [53,]    0    4    1
 [54,]    0    5    0
 [55,]    0    5    0
 [56,]    0    5    0
 [57,]    0    5    0
 [58,]    0    5    0
 [59,]    0    5    0
 [60,]    0    5    0
 [61,]    0    5    0
 [62,]    0    5    0
 [63,]    0    5    0
 [64,]    0    5    0
 [65,]    0    5    0
 [66,]    0    5    0
 [67,]    0    5    0
 [68,]    0    5    0
 [69,]    0    5    0
 [70,]    0    5    0
 [71,]    0    1    4
 [72,]    0    5    0
 [73,]    0    4    1
 [74,]    0    5    0
 [75,]    0    5    0
 [76,]    0    5    0
 [77,]    0    4    1
 [78,]    0    2    3
 [79,]    0    5    0
 [80,]    0    5    0
 [81,]    0    5    0
 [82,]    0    5    0
 [83,]    0    5    0
 [84,]    0    3    2
 [85,]    0    5    0
 [86,]    0    5    0
 [87,]    0    5    0
 [88,]    0    5    0
 [89,]    0    5    0
 [90,]    0    5    0
 [91,]    0    5    0
 [92,]    0    5    0
 [93,]    0    5    0
 [94,]    0    5    0
 [95,]    0    5    0
 [96,]    0    5    0
 [97,]    0    5    0
 [98,]    0    5    0
 [99,]    0    5    0
[100,]    0    5    0
[101,]    0    0    5
[102,]    0    0    5
[103,]    0    0    5
[104,]    0    0    5
[105,]    0    0    5
[106,]    0    0    5
[107,]    0    3    2
[108,]    0    0    5
[109,]    0    0    5
[110,]    0    0    5
[111,]    0    0    5
[112,]    0    0    5
[113,]    0    0    5
[114,]    0    0    5
[115,]    0    0    5
[116,]    0    0    5
[117,]    0    0    5
[118,]    0    0    5
[119,]    0    0    5
[120,]    0    3    2
[121,]    0    0    5
[122,]    0    0    5
[123,]    0    0    5
[124,]    0    0    5
[125,]    0    0    5
[126,]    0    0    5
[127,]    0    1    4
[128,]    0    0    5
[129,]    0    0    5
[130,]    0    3    2
[131,]    0    0    5
[132,]    0    0    5
[133,]    0    0    5
[134,]    0    3    2
[135,]    0    3    2
[136,]    0    0    5
[137,]    0    0    5
[138,]    0    0    5
[139,]    0    1    4
[140,]    0    0    5
[141,]    0    0    5
[142,]    0    0    5
[143,]    0    0    5
[144,]    0    0    5
[145,]    0    0    5
[146,]    0    0    5
[147,]    0    0    5
[148,]    0    0    5
[149,]    0    0    5
[150,]    0    0    5

$prob
       [,1] [,2] [,3]
  [1,]    1  0.0  0.0
  [2,]    1  0.0  0.0
  [3,]    1  0.0  0.0
  [4,]    1  0.0  0.0
  [5,]    1  0.0  0.0
  [6,]    1  0.0  0.0
  [7,]    1  0.0  0.0
  [8,]    1  0.0  0.0
  [9,]    1  0.0  0.0
 [10,]    1  0.0  0.0
 [11,]    1  0.0  0.0
 [12,]    1  0.0  0.0
 [13,]    1  0.0  0.0
 [14,]    1  0.0  0.0
 [15,]    1  0.0  0.0
 [16,]    1  0.0  0.0
 [17,]    1  0.0  0.0
 [18,]    1  0.0  0.0
 [19,]    1  0.0  0.0
 [20,]    1  0.0  0.0
 [21,]    1  0.0  0.0
 [22,]    1  0.0  0.0
 [23,]    1  0.0  0.0
 [24,]    1  0.0  0.0
 [25,]    1  0.0  0.0
 [26,]    1  0.0  0.0
 [27,]    1  0.0  0.0
 [28,]    1  0.0  0.0
 [29,]    1  0.0  0.0
 [30,]    1  0.0  0.0
 [31,]    1  0.0  0.0
 [32,]    1  0.0  0.0
 [33,]    1  0.0  0.0
 [34,]    1  0.0  0.0
 [35,]    1  0.0  0.0
 [36,]    1  0.0  0.0
 [37,]    1  0.0  0.0
 [38,]    1  0.0  0.0
 [39,]    1  0.0  0.0
 [40,]    1  0.0  0.0
 [41,]    1  0.0  0.0
 [42,]    1  0.0  0.0
 [43,]    1  0.0  0.0
 [44,]    1  0.0  0.0
 [45,]    1  0.0  0.0
 [46,]    1  0.0  0.0
 [47,]    1  0.0  0.0
 [48,]    1  0.0  0.0
 [49,]    1  0.0  0.0
 [50,]    1  0.0  0.0
 [51,]    0  1.0  0.0
 [52,]    0  1.0  0.0
 [53,]    0  0.8  0.2
 [54,]    0  1.0  0.0
 [55,]    0  1.0  0.0
 [56,]    0  1.0  0.0
 [57,]    0  1.0  0.0
 [58,]    0  1.0  0.0
 [59,]    0  1.0  0.0
 [60,]    0  1.0  0.0
 [61,]    0  1.0  0.0
 [62,]    0  1.0  0.0
 [63,]    0  1.0  0.0
 [64,]    0  1.0  0.0
 [65,]    0  1.0  0.0
 [66,]    0  1.0  0.0
 [67,]    0  1.0  0.0
 [68,]    0  1.0  0.0
 [69,]    0  1.0  0.0
 [70,]    0  1.0  0.0
 [71,]    0  0.2  0.8
 [72,]    0  1.0  0.0
 [73,]    0  0.8  0.2
 [74,]    0  1.0  0.0
 [75,]    0  1.0  0.0
 [76,]    0  1.0  0.0
 [77,]    0  0.8  0.2
 [78,]    0  0.4  0.6
 [79,]    0  1.0  0.0
 [80,]    0  1.0  0.0
 [81,]    0  1.0  0.0
 [82,]    0  1.0  0.0
 [83,]    0  1.0  0.0
 [84,]    0  0.6  0.4
 [85,]    0  1.0  0.0
 [86,]    0  1.0  0.0
 [87,]    0  1.0  0.0
 [88,]    0  1.0  0.0
 [89,]    0  1.0  0.0
 [90,]    0  1.0  0.0
 [91,]    0  1.0  0.0
 [92,]    0  1.0  0.0
 [93,]    0  1.0  0.0
 [94,]    0  1.0  0.0
 [95,]    0  1.0  0.0
 [96,]    0  1.0  0.0
 [97,]    0  1.0  0.0
 [98,]    0  1.0  0.0
 [99,]    0  1.0  0.0
[100,]    0  1.0  0.0
[101,]    0  0.0  1.0
[102,]    0  0.0  1.0
[103,]    0  0.0  1.0
[104,]    0  0.0  1.0
[105,]    0  0.0  1.0
[106,]    0  0.0  1.0
[107,]    0  0.6  0.4
[108,]    0  0.0  1.0
[109,]    0  0.0  1.0
[110,]    0  0.0  1.0
[111,]    0  0.0  1.0
[112,]    0  0.0  1.0
[113,]    0  0.0  1.0
[114,]    0  0.0  1.0
[115,]    0  0.0  1.0
[116,]    0  0.0  1.0
[117,]    0  0.0  1.0
[118,]    0  0.0  1.0
[119,]    0  0.0  1.0
[120,]    0  0.6  0.4
[121,]    0  0.0  1.0
[122,]    0  0.0  1.0
[123,]    0  0.0  1.0
[124,]    0  0.0  1.0
[125,]    0  0.0  1.0
[126,]    0  0.0  1.0
[127,]    0  0.2  0.8
[128,]    0  0.0  1.0
[129,]    0  0.0  1.0
[130,]    0  0.6  0.4
[131,]    0  0.0  1.0
[132,]    0  0.0  1.0
[133,]    0  0.0  1.0
[134,]    0  0.6  0.4
[135,]    0  0.6  0.4
[136,]    0  0.0  1.0
[137,]    0  0.0  1.0
[138,]    0  0.0  1.0
[139,]    0  0.2  0.8
[140,]    0  0.0  1.0
[141,]    0  0.0  1.0
[142,]    0  0.0  1.0
[143,]    0  0.0  1.0
[144,]    0  0.0  1.0
[145,]    0  0.0  1.0
[146,]    0  0.0  1.0
[147,]    0  0.0  1.0
[148,]    0  0.0  1.0
[149,]    0  0.0  1.0
[150,]    0  0.0  1.0

$class
  [1] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
  [6] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [11] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [16] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [21] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [26] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [31] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [36] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [41] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [46] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [51] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [56] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [61] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [66] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [71] "virginica"  "versicolor" "versicolor" "versicolor" "versicolor"
 [76] "versicolor" "versicolor" "virginica"  "versicolor" "versicolor"
 [81] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [86] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [91] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [96] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
[101] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[106] "virginica"  "versicolor" "virginica"  "virginica"  "virginica" 
[111] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[116] "virginica"  "virginica"  "virginica"  "virginica"  "versicolor"
[121] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[126] "virginica"  "virginica"  "virginica"  "virginica"  "versicolor"
[131] "virginica"  "virginica"  "virginica"  "versicolor" "versicolor"
[136] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[141] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[146] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 

$samples
       [,1] [,2] [,3] [,4] [,5]
  [1,]  140   44   59   80   98
  [2,]  126  116   66   55   96
  [3,]   14   11   51   81  147
  [4,]  116  134   12   98  117
  [5,]   16   48  138  150    9
  [6,]   15  100   56   82   41
  [7,]  130   57   44   68  107
  [8,]   65  134  124  144   72
  [9,]  102   54  121  118  120
 [10,]   17  104   23   37  134
 [11,]  127   11   70   15  100
 [12,]  133   54    8   56  137
 [13,]   41   85   23   62   44
 [14,]  108   81  141   69   71
 [15,]  117    9   94   10  103
 [16,]   72   54  107  116   61
 [17,]   36  146   98   23  105
 [18,]   49  115  136   47  106
 [19,]   41   31   12   87   11
 [20,]   64   72   52  118   61
 [21,]  129    6  131    4  144
 [22,]   53  121    4  105   81
 [23,]  106  139   88  106   53
 [24,]   88   25  141   10   53
 [25,]  117    2   22   13   42
 [26,]   53   39   56   51   35
 [27,]  102   32  118   66   54
 [28,]   50   40   55  139  104
 [29,]   80   59   96   69   68
 [30,]   30   86  133   60  128
 [31,]   93   11   52   37  131
 [32,]  130  130  118    7   95
 [33,]   72  143   38   79   23
 [34,]  126   55   91   31   35
 [35,]   78   41  138  119  109
 [36,]   72   61   77  126   25
 [37,]  116   92   12  118   41
 [38,]  100   60  113   95   17
 [39,]  113   65  114  127   26
 [40,]  121    2   91   10   61
 [41,]   73  137   42  109   57
 [42,]   27   69   48   70  136
 [43,]   41  106  129   64  139
 [44,]   15   66  120  109   56
 [45,]   38  101   73   25   22
 [46,]   62   31  125   42  105
 [47,]  134   89   99   39  120
 [48,]  132  124   11  140   83
 [49,]   35   17   58  103  144
 [50,]  125   33   55   25   38
 [51,]   99  122   74  122  147
 [52,]   77   40   21    1   46
 [53,]  105  148   25  138   64
 [54,]   71   87   24   49  141
 [55,]   31   20   76   33   72
 [56,]   37  112  122   95   12
 [57,]   28   31    5   72   52
 [58,]   62  133    6    2   92
 [59,]  148   27   83   71  137
 [60,]   29   97   90  103  137
 [61,]  127   61   65  112   42
 [62,]   42   36   65  120  150
 [63,]   60  148  137  134   15
 [64,]   28   85   70   60  145
 [65,]   78   52   25  102  149
 [66,]   31   47   68   66   54
 [67,]   12   84   73  133  148
 [68,]   93   67   57  131    9
 [69,]   80   84   39    6   26
 [70,]   44   34   62   99   80
 [71,]   98  122   36   39  112
 [72,]   26   79   58   96  125
 [73,]   33  141   19   16  121
 [74,]  132  124  132  104   35
 [75,]  117  107   55   67   62
 [76,]   86  112   93  129   57
 [77,]   24   30   56  146   44
 [78,]   37  111   16  121  150
 [79,]  108   93  131  130    7
 [80,]   15   92  100  139  115
 [81,]   14   13  148   75   66
 [82,]   82   48  142   46    4
 [83,]   97    7   14   97   92
 [84,]   72   82  108  108   10
 [85,]   53   61  129   32  114
 [86,]   56    1   64   66   68
 [87,]  148   69  108  135   12
 [88,]   98  110   83   20   51
 [89,]  101  118  114   17  136
 [90,]   93   36   50   27   55
 [91,]   75   98  126   71   22
 [92,]    2   26   20   31    2
 [93,]  116   19  120    2   53
 [94,]  131  115  149   94   31
 [95,]  111  141  119   69   77
 [96,]   15   50   32  127   82
 [97,]   43   71   28  117  140
 [98,]    1   86   28   18   25
 [99,]   97   73   71   25   96
[100,]   83   96    4   16  105
[101,]   68   52   89   60  119
[102,]  103   97  138   70   14
[103,]  127   12   45  140   76
[104,]  102   61    9  128   30
[105,]   74   56  120  122   75
[106,]   49   35   33   95   88
[107,]   35  131  109   20  117
[108,]  139   87   55   41  139
[109,]  141    1   75   22   79
[110,]   60   92   84  137   95
[111,]   90  137   84   38  130
[112,]   90  142   98   67  127
[113,]  112  116   73   22  144
[114,]   68   95   13   44  110
[115,]   56   80    5   50   82
[116,]   25   44   63  131   51
[117,]   81   86  131   90   54
[118,]   73   12   26   96    5
[119,]   73   12  110  104    1
[120,]  150   50  145   20   44
[121,]  104   89   48   40  131
[122,]    3    8  136  124  143
[123,]  147   54  128  120  147
[124,]  128   69  120  150   76
[125,]  127  140  130   47   12
[126,]    4   30   36  128  137
[127,]   60    9   55   40   90
[128,]   65  137   78  148  135
[129,]  133  105   61  114  143
[130,]  106  128   75    8  149
[131,]   35   43   45    9  125
[132,]  121   22  144  137   59
[133,]   23  121  123  111   99
[134,]  117   42   91   22   18
[135,]   35  102   27  119   79
[136,]  119   60  130  121   14
[137,]   99  118  105   39   56
[138,]  148   87   96   46   76
[139,]  150   67   46   85  144
[140,]  121  129   87  117    3
[141,]   86   66  146  145  127
[142,]  108  118  113   75   24
[143,]   18   98   85   89  105
[144,]   44  140   84   34   39
[145,]   29  105   90  144   32
[146,]   91   47   39  116    9
[147,]   32   62  104  109   12
[148,]   51   13   11   70   54
[149,]  119   39  123   50   73
[150,]   51  132   13   91  114

$importance
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
    78.44969     21.55031      0.00000      0.00000 

$terms
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
attr(,"variables")
list(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)
attr(,"factors")
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Species                 0           0            0           0
Sepal.Length            1           0            0           0
Sepal.Width             0           1            0           0
Petal.Length            0           0            1           0
Petal.Width             0           0            0           1
attr(,"term.labels")
[1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width" 
attr(,"order")
[1] 1 1 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,".Environment")
<environment: 0x7fa8570a67c0>
attr(,"predvars")
list(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)
attr(,"dataClasses")
     Species Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
    "factor"    "numeric"    "numeric"    "numeric"    "numeric" 

$call
adabag::bagging(formula = formula, data = as.data.frame(data), 
    mfinal = 5, control = ..2)

attr(,"vardep.summary")
    setosa versicolor  virginica 
        50         50         50 
attr(,"class")
[1] "bagging"
> 
> 
> 
> 
> cleanEx()
> nameEx("AdaBoostModel")
> ### * AdaBoostModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: AdaBoostModel
> ### Title: Boosting with Classification Trees
> ### Aliases: AdaBoostModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = AdaBoostModel(mfinal = 5))
$formula
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
<environment: 0x7fa840db7278>

$trees
$trees[[1]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 150 94 setosa (0.37333333 0.34000000 0.28666667)  
   2) Petal.Length< 2.6 56  0 setosa (1.00000000 0.00000000 0.00000000) *
   3) Petal.Length>=2.6 94 43 versicolor (0.00000000 0.54255319 0.45744681)  
     6) Petal.Length< 4.85 53  4 versicolor (0.00000000 0.92452830 0.07547170)  
      12) Petal.Width< 1.65 46  0 versicolor (0.00000000 1.00000000 0.00000000) *
      13) Petal.Width>=1.65 7  3 virginica (0.00000000 0.42857143 0.57142857) *
     7) Petal.Length>=4.85 41  2 virginica (0.00000000 0.04878049 0.95121951) *

$trees[[2]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 82 versicolor (0.26000000 0.45333333 0.28666667)  
  2) Petal.Length< 2.6 39  0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.6 111 43 versicolor (0.00000000 0.61261261 0.38738739)  
    6) Petal.Width< 1.75 68  4 versicolor (0.00000000 0.94117647 0.05882353) *
    7) Petal.Width>=1.75 43  4 virginica (0.00000000 0.09302326 0.90697674) *

$trees[[3]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 150 83 versicolor (0.25333333 0.44666667 0.30000000)  
   2) Petal.Length< 2.6 38  0 setosa (1.00000000 0.00000000 0.00000000) *
   3) Petal.Length>=2.6 112 45 versicolor (0.00000000 0.59821429 0.40178571)  
     6) Petal.Length< 5.15 85 18 versicolor (0.00000000 0.78823529 0.21176471)  
      12) Petal.Width< 1.65 56  4 versicolor (0.00000000 0.92857143 0.07142857) *
      13) Petal.Width>=1.65 29 14 versicolor (0.00000000 0.51724138 0.48275862)  
        26) Sepal.Width>=2.9 19  4 versicolor (0.00000000 0.78947368 0.21052632) *
        27) Sepal.Width< 2.9 10  0 virginica (0.00000000 0.00000000 1.00000000) *
     7) Petal.Length>=5.15 27  0 virginica (0.00000000 0.00000000 1.00000000) *

$trees[[4]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 150 80 virginica (0.22000000 0.31333333 0.46666667)  
   2) Petal.Length< 2.75 33  0 setosa (1.00000000 0.00000000 0.00000000) *
   3) Petal.Length>=2.75 117 47 virginica (0.00000000 0.40170940 0.59829060)  
     6) Petal.Length< 4.95 48  9 versicolor (0.00000000 0.81250000 0.18750000)  
      12) Petal.Width< 1.6 27  0 versicolor (0.00000000 1.00000000 0.00000000) *
      13) Petal.Width>=1.6 21  9 versicolor (0.00000000 0.57142857 0.42857143)  
        26) Sepal.Width>=3.1 12  0 versicolor (0.00000000 1.00000000 0.00000000) *
        27) Sepal.Width< 3.1 9  0 virginica (0.00000000 0.00000000 1.00000000) *
     7) Petal.Length>=4.95 69  8 virginica (0.00000000 0.11594203 0.88405797)  
      14) Sepal.Length< 6.05 25  7 virginica (0.00000000 0.28000000 0.72000000)  
        28) Sepal.Width>=2.45 10  3 versicolor (0.00000000 0.70000000 0.30000000) *
        29) Sepal.Width< 2.45 15  0 virginica (0.00000000 0.00000000 1.00000000) *
      15) Sepal.Length>=6.05 44  1 virginica (0.00000000 0.02272727 0.97727273) *

$trees[[5]]
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 150 84 virginica (0.23333333 0.32666667 0.44000000)  
   2) Petal.Length< 2.7 35  0 setosa (1.00000000 0.00000000 0.00000000) *
   3) Petal.Length>=2.7 115 49 virginica (0.00000000 0.42608696 0.57391304)  
     6) Petal.Length< 5.05 67 20 versicolor (0.00000000 0.70149254 0.29850746)  
      12) Sepal.Width>=2.55 46  4 versicolor (0.00000000 0.91304348 0.08695652) *
      13) Sepal.Width< 2.55 21  5 virginica (0.00000000 0.23809524 0.76190476)  
        26) Petal.Length< 4.95 7  2 versicolor (0.00000000 0.71428571 0.28571429) *
        27) Petal.Length>=4.95 14  0 virginica (0.00000000 0.00000000 1.00000000) *
     7) Petal.Length>=5.05 48  2 virginica (0.00000000 0.04166667 0.95833333) *


$weights
[1] 1.683648 1.372189 1.344342 1.418982 1.283356

$votes
           [,1]     [,2]     [,3]
  [1,] 7.102518 0.000000 0.000000
  [2,] 7.102518 0.000000 0.000000
  [3,] 7.102518 0.000000 0.000000
  [4,] 7.102518 0.000000 0.000000
  [5,] 7.102518 0.000000 0.000000
  [6,] 7.102518 0.000000 0.000000
  [7,] 7.102518 0.000000 0.000000
  [8,] 7.102518 0.000000 0.000000
  [9,] 7.102518 0.000000 0.000000
 [10,] 7.102518 0.000000 0.000000
 [11,] 7.102518 0.000000 0.000000
 [12,] 7.102518 0.000000 0.000000
 [13,] 7.102518 0.000000 0.000000
 [14,] 7.102518 0.000000 0.000000
 [15,] 7.102518 0.000000 0.000000
 [16,] 7.102518 0.000000 0.000000
 [17,] 7.102518 0.000000 0.000000
 [18,] 7.102518 0.000000 0.000000
 [19,] 7.102518 0.000000 0.000000
 [20,] 7.102518 0.000000 0.000000
 [21,] 7.102518 0.000000 0.000000
 [22,] 7.102518 0.000000 0.000000
 [23,] 7.102518 0.000000 0.000000
 [24,] 7.102518 0.000000 0.000000
 [25,] 7.102518 0.000000 0.000000
 [26,] 7.102518 0.000000 0.000000
 [27,] 7.102518 0.000000 0.000000
 [28,] 7.102518 0.000000 0.000000
 [29,] 7.102518 0.000000 0.000000
 [30,] 7.102518 0.000000 0.000000
 [31,] 7.102518 0.000000 0.000000
 [32,] 7.102518 0.000000 0.000000
 [33,] 7.102518 0.000000 0.000000
 [34,] 7.102518 0.000000 0.000000
 [35,] 7.102518 0.000000 0.000000
 [36,] 7.102518 0.000000 0.000000
 [37,] 7.102518 0.000000 0.000000
 [38,] 7.102518 0.000000 0.000000
 [39,] 7.102518 0.000000 0.000000
 [40,] 7.102518 0.000000 0.000000
 [41,] 7.102518 0.000000 0.000000
 [42,] 7.102518 0.000000 0.000000
 [43,] 7.102518 0.000000 0.000000
 [44,] 7.102518 0.000000 0.000000
 [45,] 7.102518 0.000000 0.000000
 [46,] 7.102518 0.000000 0.000000
 [47,] 7.102518 0.000000 0.000000
 [48,] 7.102518 0.000000 0.000000
 [49,] 7.102518 0.000000 0.000000
 [50,] 7.102518 0.000000 0.000000
 [51,] 0.000000 7.102518 0.000000
 [52,] 0.000000 7.102518 0.000000
 [53,] 0.000000 5.418870 1.683648
 [54,] 0.000000 7.102518 0.000000
 [55,] 0.000000 7.102518 0.000000
 [56,] 0.000000 7.102518 0.000000
 [57,] 0.000000 7.102518 0.000000
 [58,] 0.000000 7.102518 0.000000
 [59,] 0.000000 7.102518 0.000000
 [60,] 0.000000 7.102518 0.000000
 [61,] 0.000000 7.102518 0.000000
 [62,] 0.000000 7.102518 0.000000
 [63,] 0.000000 7.102518 0.000000
 [64,] 0.000000 7.102518 0.000000
 [65,] 0.000000 7.102518 0.000000
 [66,] 0.000000 7.102518 0.000000
 [67,] 0.000000 7.102518 0.000000
 [68,] 0.000000 7.102518 0.000000
 [69,] 0.000000 7.102518 0.000000
 [70,] 0.000000 7.102518 0.000000
 [71,] 0.000000 4.046680 3.055837
 [72,] 0.000000 7.102518 0.000000
 [73,] 0.000000 5.418870 1.683648
 [74,] 0.000000 7.102518 0.000000
 [75,] 0.000000 7.102518 0.000000
 [76,] 0.000000 7.102518 0.000000
 [77,] 0.000000 7.102518 0.000000
 [78,] 0.000000 3.999887 3.102630
 [79,] 0.000000 7.102518 0.000000
 [80,] 0.000000 7.102518 0.000000
 [81,] 0.000000 7.102518 0.000000
 [82,] 0.000000 7.102518 0.000000
 [83,] 0.000000 7.102518 0.000000
 [84,] 0.000000 4.135514 2.967004
 [85,] 0.000000 7.102518 0.000000
 [86,] 0.000000 7.102518 0.000000
 [87,] 0.000000 7.102518 0.000000
 [88,] 0.000000 7.102518 0.000000
 [89,] 0.000000 7.102518 0.000000
 [90,] 0.000000 7.102518 0.000000
 [91,] 0.000000 7.102518 0.000000
 [92,] 0.000000 7.102518 0.000000
 [93,] 0.000000 7.102518 0.000000
 [94,] 0.000000 7.102518 0.000000
 [95,] 0.000000 7.102518 0.000000
 [96,] 0.000000 7.102518 0.000000
 [97,] 0.000000 7.102518 0.000000
 [98,] 0.000000 7.102518 0.000000
 [99,] 0.000000 7.102518 0.000000
[100,] 0.000000 7.102518 0.000000
[101,] 0.000000 0.000000 7.102518
[102,] 0.000000 1.418982 5.683535
[103,] 0.000000 0.000000 7.102518
[104,] 0.000000 0.000000 7.102518
[105,] 0.000000 0.000000 7.102518
[106,] 0.000000 0.000000 7.102518
[107,] 0.000000 2.655545 4.446973
[108,] 0.000000 0.000000 7.102518
[109,] 0.000000 0.000000 7.102518
[110,] 0.000000 0.000000 7.102518
[111,] 0.000000 1.344342 5.758175
[112,] 0.000000 0.000000 7.102518
[113,] 0.000000 0.000000 7.102518
[114,] 0.000000 1.418982 5.683535
[115,] 0.000000 1.418982 5.683535
[116,] 0.000000 0.000000 7.102518
[117,] 0.000000 0.000000 7.102518
[118,] 0.000000 0.000000 7.102518
[119,] 0.000000 0.000000 7.102518
[120,] 0.000000 2.716532 4.385986
[121,] 0.000000 0.000000 7.102518
[122,] 0.000000 1.283356 5.819162
[123,] 0.000000 0.000000 7.102518
[124,] 0.000000 1.283356 5.819162
[125,] 0.000000 0.000000 7.102518
[126,] 0.000000 0.000000 7.102518
[127,] 0.000000 1.283356 5.819162
[128,] 0.000000 2.627698 4.474820
[129,] 0.000000 0.000000 7.102518
[130,] 0.000000 1.372189 5.730328
[131,] 0.000000 0.000000 7.102518
[132,] 0.000000 0.000000 7.102518
[133,] 0.000000 0.000000 7.102518
[134,] 0.000000 2.716532 4.385986
[135,] 0.000000 1.372189 5.730328
[136,] 0.000000 0.000000 7.102518
[137,] 0.000000 0.000000 7.102518
[138,] 0.000000 0.000000 7.102518
[139,] 0.000000 2.627698 4.474820
[140,] 0.000000 0.000000 7.102518
[141,] 0.000000 0.000000 7.102518
[142,] 0.000000 1.344342 5.758175
[143,] 0.000000 1.418982 5.683535
[144,] 0.000000 0.000000 7.102518
[145,] 0.000000 0.000000 7.102518
[146,] 0.000000 0.000000 7.102518
[147,] 0.000000 0.000000 7.102518
[148,] 0.000000 0.000000 7.102518
[149,] 0.000000 0.000000 7.102518
[150,] 0.000000 2.763325 4.339193

$prob
       [,1]      [,2]      [,3]
  [1,]    1 0.0000000 0.0000000
  [2,]    1 0.0000000 0.0000000
  [3,]    1 0.0000000 0.0000000
  [4,]    1 0.0000000 0.0000000
  [5,]    1 0.0000000 0.0000000
  [6,]    1 0.0000000 0.0000000
  [7,]    1 0.0000000 0.0000000
  [8,]    1 0.0000000 0.0000000
  [9,]    1 0.0000000 0.0000000
 [10,]    1 0.0000000 0.0000000
 [11,]    1 0.0000000 0.0000000
 [12,]    1 0.0000000 0.0000000
 [13,]    1 0.0000000 0.0000000
 [14,]    1 0.0000000 0.0000000
 [15,]    1 0.0000000 0.0000000
 [16,]    1 0.0000000 0.0000000
 [17,]    1 0.0000000 0.0000000
 [18,]    1 0.0000000 0.0000000
 [19,]    1 0.0000000 0.0000000
 [20,]    1 0.0000000 0.0000000
 [21,]    1 0.0000000 0.0000000
 [22,]    1 0.0000000 0.0000000
 [23,]    1 0.0000000 0.0000000
 [24,]    1 0.0000000 0.0000000
 [25,]    1 0.0000000 0.0000000
 [26,]    1 0.0000000 0.0000000
 [27,]    1 0.0000000 0.0000000
 [28,]    1 0.0000000 0.0000000
 [29,]    1 0.0000000 0.0000000
 [30,]    1 0.0000000 0.0000000
 [31,]    1 0.0000000 0.0000000
 [32,]    1 0.0000000 0.0000000
 [33,]    1 0.0000000 0.0000000
 [34,]    1 0.0000000 0.0000000
 [35,]    1 0.0000000 0.0000000
 [36,]    1 0.0000000 0.0000000
 [37,]    1 0.0000000 0.0000000
 [38,]    1 0.0000000 0.0000000
 [39,]    1 0.0000000 0.0000000
 [40,]    1 0.0000000 0.0000000
 [41,]    1 0.0000000 0.0000000
 [42,]    1 0.0000000 0.0000000
 [43,]    1 0.0000000 0.0000000
 [44,]    1 0.0000000 0.0000000
 [45,]    1 0.0000000 0.0000000
 [46,]    1 0.0000000 0.0000000
 [47,]    1 0.0000000 0.0000000
 [48,]    1 0.0000000 0.0000000
 [49,]    1 0.0000000 0.0000000
 [50,]    1 0.0000000 0.0000000
 [51,]    0 1.0000000 0.0000000
 [52,]    0 1.0000000 0.0000000
 [53,]    0 0.7629505 0.2370495
 [54,]    0 1.0000000 0.0000000
 [55,]    0 1.0000000 0.0000000
 [56,]    0 1.0000000 0.0000000
 [57,]    0 1.0000000 0.0000000
 [58,]    0 1.0000000 0.0000000
 [59,]    0 1.0000000 0.0000000
 [60,]    0 1.0000000 0.0000000
 [61,]    0 1.0000000 0.0000000
 [62,]    0 1.0000000 0.0000000
 [63,]    0 1.0000000 0.0000000
 [64,]    0 1.0000000 0.0000000
 [65,]    0 1.0000000 0.0000000
 [66,]    0 1.0000000 0.0000000
 [67,]    0 1.0000000 0.0000000
 [68,]    0 1.0000000 0.0000000
 [69,]    0 1.0000000 0.0000000
 [70,]    0 1.0000000 0.0000000
 [71,]    0 0.5697530 0.4302470
 [72,]    0 1.0000000 0.0000000
 [73,]    0 0.7629505 0.2370495
 [74,]    0 1.0000000 0.0000000
 [75,]    0 1.0000000 0.0000000
 [76,]    0 1.0000000 0.0000000
 [77,]    0 1.0000000 0.0000000
 [78,]    0 0.5631647 0.4368353
 [79,]    0 1.0000000 0.0000000
 [80,]    0 1.0000000 0.0000000
 [81,]    0 1.0000000 0.0000000
 [82,]    0 1.0000000 0.0000000
 [83,]    0 1.0000000 0.0000000
 [84,]    0 0.5822603 0.4177397
 [85,]    0 1.0000000 0.0000000
 [86,]    0 1.0000000 0.0000000
 [87,]    0 1.0000000 0.0000000
 [88,]    0 1.0000000 0.0000000
 [89,]    0 1.0000000 0.0000000
 [90,]    0 1.0000000 0.0000000
 [91,]    0 1.0000000 0.0000000
 [92,]    0 1.0000000 0.0000000
 [93,]    0 1.0000000 0.0000000
 [94,]    0 1.0000000 0.0000000
 [95,]    0 1.0000000 0.0000000
 [96,]    0 1.0000000 0.0000000
 [97,]    0 1.0000000 0.0000000
 [98,]    0 1.0000000 0.0000000
 [99,]    0 1.0000000 0.0000000
[100,]    0 1.0000000 0.0000000
[101,]    0 0.0000000 1.0000000
[102,]    0 0.1997858 0.8002142
[103,]    0 0.0000000 1.0000000
[104,]    0 0.0000000 1.0000000
[105,]    0 0.0000000 1.0000000
[106,]    0 0.0000000 1.0000000
[107,]    0 0.3738878 0.6261122
[108,]    0 0.0000000 1.0000000
[109,]    0 0.0000000 1.0000000
[110,]    0 0.0000000 1.0000000
[111,]    0 0.1892769 0.8107231
[112,]    0 0.0000000 1.0000000
[113,]    0 0.0000000 1.0000000
[114,]    0 0.1997858 0.8002142
[115,]    0 0.1997858 0.8002142
[116,]    0 0.0000000 1.0000000
[117,]    0 0.0000000 1.0000000
[118,]    0 0.0000000 1.0000000
[119,]    0 0.0000000 1.0000000
[120,]    0 0.3824745 0.6175255
[121,]    0 0.0000000 1.0000000
[122,]    0 0.1806903 0.8193097
[123,]    0 0.0000000 1.0000000
[124,]    0 0.1806903 0.8193097
[125,]    0 0.0000000 1.0000000
[126,]    0 0.0000000 1.0000000
[127,]    0 0.1806903 0.8193097
[128,]    0 0.3699671 0.6300329
[129,]    0 0.0000000 1.0000000
[130,]    0 0.1931976 0.8068024
[131,]    0 0.0000000 1.0000000
[132,]    0 0.0000000 1.0000000
[133,]    0 0.0000000 1.0000000
[134,]    0 0.3824745 0.6175255
[135,]    0 0.1931976 0.8068024
[136,]    0 0.0000000 1.0000000
[137,]    0 0.0000000 1.0000000
[138,]    0 0.0000000 1.0000000
[139,]    0 0.3699671 0.6300329
[140,]    0 0.0000000 1.0000000
[141,]    0 0.0000000 1.0000000
[142,]    0 0.1892769 0.8107231
[143,]    0 0.1997858 0.8002142
[144,]    0 0.0000000 1.0000000
[145,]    0 0.0000000 1.0000000
[146,]    0 0.0000000 1.0000000
[147,]    0 0.0000000 1.0000000
[148,]    0 0.0000000 1.0000000
[149,]    0 0.0000000 1.0000000
[150,]    0 0.3890627 0.6109373

$class
  [1] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
  [6] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [11] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [16] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [21] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [26] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [31] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [36] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [41] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [46] "setosa"     "setosa"     "setosa"     "setosa"     "setosa"    
 [51] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [56] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [61] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [66] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [71] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [76] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [81] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [86] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [91] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
 [96] "versicolor" "versicolor" "versicolor" "versicolor" "versicolor"
[101] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[106] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[111] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[116] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[121] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[126] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[131] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[136] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[141] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 
[146] "virginica"  "virginica"  "virginica"  "virginica"  "virginica" 

$importance
Petal.Length  Petal.Width Sepal.Length  Sepal.Width 
  79.2086178   12.0095186    0.4890744    8.2927892 

$terms
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
attr(,"variables")
list(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)
attr(,"factors")
             Sepal.Length Sepal.Width Petal.Length Petal.Width
Species                 0           0            0           0
Sepal.Length            1           0            0           0
Sepal.Width             0           1            0           0
Petal.Length            0           0            1           0
Petal.Width             0           0            0           1
attr(,"term.labels")
[1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width" 
attr(,"order")
[1] 1 1 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,".Environment")
<environment: 0x7fa840db7278>
attr(,"predvars")
list(Species, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)
attr(,"dataClasses")
     Species Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
    "factor"    "numeric"    "numeric"    "numeric"    "numeric" 

$call
adabag::boosting(formula = formula, data = as.data.frame(data), 
    boos = TRUE, mfinal = 5, coeflearn = "Breiman", control = ..4)

attr(,"vardep.summary")
    setosa versicolor  virginica 
        50         50         50 
attr(,"class")
[1] "boosting"
> 
> 
> 
> 
> cleanEx()
> nameEx("BARTMachineModel")
> ### * BARTMachineModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: BARTMachineModel
> ### Title: Bayesian Additive Regression Trees Model
> ### Aliases: BARTMachineModel
> 
> ### ** Examples
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("BARTModel")
> ### * BARTModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: BARTModel
> ### Title: Bayesian Additive Regression Trees Model
> ### Aliases: BARTModel
> 
> ### ** Examples
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("BlackBoostModel")
> ### * BlackBoostModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: BlackBoostModel
> ### Title: Gradient Boosting with Regression Trees
> ### Aliases: BlackBoostModel
> 
> ### ** Examples
> 
> library(MASS)
> 
> fit(type ~ ., data = Pima.tr, model = BlackBoostModel)

	 Model-based Boosting

Call:
mboost::blackboost(formula = formula, data = as.data.frame(data),     weights = weights, na.action = na.pass, family = family,     control = ..1, tree_controls = ..2)


	 Negative Binomial Likelihood (logit link) 

Loss function: { 
     f <- pmin(abs(f), 36) * sign(f) 
     p <- exp(f)/(exp(f) + exp(-f)) 
     y <- (y + 1)/2 
     -y * log(p) - (1 - y) * log(1 - p) 
 } 
 

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  -0.3316471 
Number of baselearners:  1 

> 
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("C50Model")
> ### * C50Model
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: C50Model
> ### Title: C5.0 Decision Trees and Rule-Based Model
> ### Aliases: C50Model
> 
> ### ** Examples
> 
> model_fit <- fit(Species ~ ., data = iris, model = C50Model)
> varimp(model_fit, metric = "splits", scale = FALSE)
Object of class "VarImp"
              Overall
Petal.Length 66.66667
Petal.Width  33.33333
Sepal.Length  0.00000
Sepal.Width   0.00000
case weight   0.00000
> 
> 
> 
> 
> cleanEx()
> nameEx("CForestModel")
> ### * CForestModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: CForestModel
> ### Title: Conditional Random Forest Model
> ### Aliases: CForestModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = CForestModel)

	 Random Forest using Conditional Inference Trees

Number of trees:  500 

Response:  sale_amount 
Inputs:  sale_year, sale_month, built, style, construction, base_size, add_size, garage1_size, garage2_size, lot_size, bedrooms, basement, ac, attic, lon, lat 
Number of observations:  753 

> 
> 
> 
> 
> cleanEx()
> nameEx("CoxModel")
> ### * CoxModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: CoxModel
> ### Title: Proportional Hazards Regression Model
> ### Aliases: CoxModel CoxStepAICModel
> 
> ### ** Examples
> 
> library(survival)
> 
> fit(Surv(time, status) ~ ., data = veteran, model = CoxModel)
Call:
survival::coxph(formula = formula, data = as.data.frame(data), 
    weights = weights, ties = "efron")

                        coef  exp(coef)   se(coef)      z        p
trt                2.946e-01  1.343e+00  2.075e-01  1.419  0.15577
celltypesmallcell  8.616e-01  2.367e+00  2.753e-01  3.130  0.00175
celltypeadeno      1.196e+00  3.307e+00  3.009e-01  3.975 7.05e-05
celltypelarge      4.013e-01  1.494e+00  2.827e-01  1.420  0.15574
karno             -3.282e-02  9.677e-01  5.508e-03 -5.958 2.55e-09
diagtime           8.132e-05  1.000e+00  9.136e-03  0.009  0.99290
age               -8.706e-03  9.913e-01  9.300e-03 -0.936  0.34920
prior              7.159e-03  1.007e+00  2.323e-02  0.308  0.75794

Likelihood ratio test=62.1  on 8 df, p=1.799e-10
n= 137, number of events= 128 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("DiscreteVariate")
> ### * DiscreteVariate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: DiscreteVariate
> ### Title: Discrete Variate Constructors
> ### Aliases: DiscreteVariate BinomialVariate NegBinomialVariate
> ###   PoissonVariate
> 
> ### ** Examples
> 
> BinomialVariate(rbinom(25, 10, 0.5), size = 10)
Object of class "BinomialVariate"
      Success Failure
 [1,]       4       6
 [2,]       4       6
 [3,]       5       5
 [4,]       7       3
 [5,]       4       6
 [6,]       7       3
 [7,]       7       3
 [8,]       6       4
 [9,]       6       4
[10,]       3       7
... with 15 more row s
> PoissonVariate(rpois(25, 10))
Object of class "PoissonVariate"
 [1]  9  9  8 10  7 12 11 12 12 10  3 11 13  7  8 11 14  9 11  9  5 13  8 13  9
Range: 0, Inf
> 
> 
> 
> 
> cleanEx()
> nameEx("EarthModel")
> ### * EarthModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: EarthModel
> ### Title: Multivariate Adaptive Regression Splines Model
> ### Aliases: EarthModel
> 
> ### ** Examples
> 
> model_fit <- fit(Species ~ ., data = iris, model = EarthModel)
Warning: glm.fit: algorithm did not converge
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
Warning: the glm algorithm did not converge for response "setosa"
> varimp(model_fit, metric = "nsubsets", scale = FALSE)
Object of class "VarImp"
             nsubsets
Petal.Length        8
Petal.Width         6
Sepal.Width         2
Sepal.Length        1
> 
> 
> 
> 
> cleanEx()
> nameEx("FDAModel")
> ### * FDAModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: FDAModel
> ### Title: Flexible and Penalized Discriminant Analysis Models
> ### Aliases: FDAModel PDAModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = FDAModel)
Call:
mda::fda(formula = formula, data = as.data.frame(data), weights = weights, 
    eps = 2.22044604925031e-16, method = ..2)

Dimension: 2 

Percent Between-Group Variance Explained:
    v1     v2 
 99.12 100.00 

Degrees of Freedom (per dimension): 5 

Training Misclassification Error: 0.02 ( N = 150 )
> 
> fit(Species ~ ., data = iris, model = PDAModel)
Call:
mda::fda(formula = formula, data = as.data.frame(data), weights = weights, 
    eps = 2.22044604925031e-16, method = ..2, lambda = 1)

Dimension: 2 

Percent Between-Group Variance Explained:
    v1     v2 
 99.11 100.00 

Degrees of Freedom (per dimension): 3.672761 

Training Misclassification Error: 0.02 ( N = 150 )
> 
> 
> 
> 
> cleanEx()
> nameEx("GAMBoostModel")
> ### * GAMBoostModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GAMBoostModel
> ### Title: Gradient Boosting with Additive Models
> ### Aliases: GAMBoostModel
> 
> ### ** Examples
> 
> library(MASS)
> 
> fit(type ~ ., data = Pima.tr, model = GAMBoostModel)

	 Model-based Boosting

Call:
mboost::gamboost(formula = formula, data = as.data.frame(data),     na.action = na.pass, weights = weights, family = family,     control = ..3, baselearner = "bbs", dfbase = 4)


	 Negative Binomial Likelihood (logit link) 

Loss function: { 
     f <- pmin(abs(f), 36) * sign(f) 
     p <- exp(f)/(exp(f) + exp(-f)) 
     y <- (y + 1)/2 
     -y * log(p) - (1 - y) * log(1 - p) 
 } 
 

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  -0.3316471 
Number of baselearners:  7 

> 
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("GBMModel")
> ### * GBMModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GBMModel
> ### Title: Generalized Boosted Regression Model
> ### Aliases: GBMModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = GBMModel)
gbm::gbm(formula = formula, distribution = distribution, data = as.data.frame(data), 
    weights = weights, n.trees = 100, interaction.depth = 1, 
    n.minobsinnode = 10, shrinkage = 0.1, bag.fraction = 0.5)
A gradient boosted model with multinomial loss function.
100 iterations were performed.
There were 4 predictors of which 4 had non-zero influence.
> 
> 
> 
> 
> cleanEx()
> nameEx("GLMBoostModel")
> ### * GLMBoostModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GLMBoostModel
> ### Title: Gradient Boosting with Linear Models
> ### Aliases: GLMBoostModel
> 
> ### ** Examples
> 
> library(MASS)
> 
> fit(type ~ ., data = Pima.tr, model = GLMBoostModel)

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = formula, data = as.data.frame(data),     weights = weights, family = family, na.action = na.pass,     control = ..1)


	 Negative Binomial Likelihood (logit link) 

Loss function: { 
     f <- pmin(abs(f), 36) * sign(f) 
     p <- exp(f)/(exp(f) + exp(-f)) 
     y <- (y + 1)/2 
     -y * log(p) - (1 - y) * log(1 - p) 
 } 
 

Number of boosting iterations: mstop = 100 
Step size:  0.1 
Offset:  -0.3316471 

Coefficients: 

NOTE: Coefficients from a Binomial model are half the size of coefficients
 from a model fitted via glm(... , family = 'binomial').
See Warning section in ?coef.mboost

(Intercept)       npreg         glu         bmi         ped         age 
-3.87891917  0.04058164  0.01418041  0.03159626  0.71053574  0.01722494 
attr(,"offset")
[1] -0.3316471

> 
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("GLMModel")
> ### * GLMModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GLMModel
> ### Title: Generalized Linear Model
> ### Aliases: GLMModel GLMStepAICModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = GLMModel)

Call:  stats::glm(formula = formula, family = family, data = data, weights = weights, 
    control = control)

Coefficients:
                  (Intercept)                      sale_year  
                   -1.742e+07                      2.513e+03  
                   sale_month                          built  
                   -7.918e+02                      3.029e+02  
                   styleCondo      construction1 Story Brick  
                   -1.774e+04                     -1.176e+04  
    construction1 Story Condo      construction1 Story Frame  
                   -6.658e+04                     -6.416e+04  
    construction2 Story Brick      construction2 Story Condo  
                    1.131e+05                      9.514e+03  
    construction2 Story Frame  constructionSplit Foyer Frame  
                    2.323e+04                     -5.714e+04  
constructionSplit Level Frame                      base_size  
                   -3.302e+04                      1.635e+02  
                     add_size                   garage1_size  
                    1.433e+02                      5.151e+01  
                 garage2_size                       lot_size  
                    6.838e+01                      1.232e+00  
                     bedrooms                    basementYes  
                   -1.557e+03                      2.387e+04  
                        acYes                       atticYes  
                   -2.600e+03                      4.763e+04  
                          lon                            lat  
                    5.430e+03                      2.949e+05  

Degrees of Freedom: 752 Total (i.e. Null);  729 Residual
Null Deviance:	    6.323e+12 
Residual Deviance: 1.76e+12 	AIC: 18430
> 
> 
> 
> 
> cleanEx()
> nameEx("GLMNetModel")
> ### * GLMNetModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GLMNetModel
> ### Title: GLM Lasso or Elasticnet Model
> ### Aliases: GLMNetModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = GLMNetModel(lambda = 0.01))

Call:  glmnet::glmnet(x = x, y = y, family = family, weights = weights,      offset = x.offset, alpha = 1, nlambda = nlambda, lambda = 0.01,      standardize = TRUE, thresh = 1e-07, penalty.factor = ..4,      maxit = 1e+05, type.gaussian = ..8, type.logistic = "Newton",      standardize.response = FALSE, type.multinomial = "ungrouped") 

  Df  %Dev Lambda
1 24 72.17   0.01
> 
> 
> 
> 
> cleanEx()
> nameEx("Grid")
> ### * Grid
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Grid
> ### Title: Tuning Grid Control
> ### Aliases: Grid
> 
> ### ** Examples
> 
> TunedModel(GBMModel, grid = Grid(10, random = 5))
Object of class "TunedModel"

Model name: TunedModel
Label: Grid Tuned Model
Package: 
Response types: factor, numeric, PoissonVariate, Surv
Tuning grid: FALSE
Variable importance: FALSE

Tuning parameters:

Object of class "MLModelFunction"

Model name: GBMModel
Label: Generalized Boosted Regression
Package: gbm
Response types: factor, numeric, PoissonVariate, Surv
Tuning grid: TRUE
Variable importance: TRUE

Arguments:
function (distribution = NULL, n.trees = 100, interaction.depth = 1, 
    n.minobsinnode = 10, shrinkage = 0.1, bag.fraction = 0.5) 
NULL

Object of class "Grid"

Length: 10
Random sample: 5

Object of class "MLControl"

Name: CVControl
Label: K-Fold Cross-Validation
Folds: 10
Repeats: 1
Seed: 1140350788 
> 
> 
> 
> 
> cleanEx()
> nameEx("KNNModel")
> ### * KNNModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: KNNModel
> ### Title: Weighted k-Nearest Neighbor Model
> ### Aliases: KNNModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = KNNModel)
$formula
Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
<environment: 0x7fa858ee9510>

$train
       Species Sepal.Length Sepal.Width Petal.Length Petal.Width (names)
1       setosa          5.1         3.5          1.4         0.2       1
2       setosa          4.9         3.0          1.4         0.2       2
3       setosa          4.7         3.2          1.3         0.2       3
4       setosa          4.6         3.1          1.5         0.2       4
5       setosa          5.0         3.6          1.4         0.2       5
6       setosa          5.4         3.9          1.7         0.4       6
7       setosa          4.6         3.4          1.4         0.3       7
8       setosa          5.0         3.4          1.5         0.2       8
9       setosa          4.4         2.9          1.4         0.2       9
10      setosa          4.9         3.1          1.5         0.1      10
11      setosa          5.4         3.7          1.5         0.2      11
12      setosa          4.8         3.4          1.6         0.2      12
13      setosa          4.8         3.0          1.4         0.1      13
14      setosa          4.3         3.0          1.1         0.1      14
15      setosa          5.8         4.0          1.2         0.2      15
16      setosa          5.7         4.4          1.5         0.4      16
17      setosa          5.4         3.9          1.3         0.4      17
18      setosa          5.1         3.5          1.4         0.3      18
19      setosa          5.7         3.8          1.7         0.3      19
20      setosa          5.1         3.8          1.5         0.3      20
21      setosa          5.4         3.4          1.7         0.2      21
22      setosa          5.1         3.7          1.5         0.4      22
23      setosa          4.6         3.6          1.0         0.2      23
24      setosa          5.1         3.3          1.7         0.5      24
25      setosa          4.8         3.4          1.9         0.2      25
26      setosa          5.0         3.0          1.6         0.2      26
27      setosa          5.0         3.4          1.6         0.4      27
28      setosa          5.2         3.5          1.5         0.2      28
29      setosa          5.2         3.4          1.4         0.2      29
30      setosa          4.7         3.2          1.6         0.2      30
31      setosa          4.8         3.1          1.6         0.2      31
32      setosa          5.4         3.4          1.5         0.4      32
33      setosa          5.2         4.1          1.5         0.1      33
34      setosa          5.5         4.2          1.4         0.2      34
35      setosa          4.9         3.1          1.5         0.2      35
36      setosa          5.0         3.2          1.2         0.2      36
37      setosa          5.5         3.5          1.3         0.2      37
38      setosa          4.9         3.6          1.4         0.1      38
39      setosa          4.4         3.0          1.3         0.2      39
40      setosa          5.1         3.4          1.5         0.2      40
41      setosa          5.0         3.5          1.3         0.3      41
42      setosa          4.5         2.3          1.3         0.3      42
43      setosa          4.4         3.2          1.3         0.2      43
44      setosa          5.0         3.5          1.6         0.6      44
45      setosa          5.1         3.8          1.9         0.4      45
46      setosa          4.8         3.0          1.4         0.3      46
47      setosa          5.1         3.8          1.6         0.2      47
48      setosa          4.6         3.2          1.4         0.2      48
49      setosa          5.3         3.7          1.5         0.2      49
50      setosa          5.0         3.3          1.4         0.2      50
51  versicolor          7.0         3.2          4.7         1.4      51
52  versicolor          6.4         3.2          4.5         1.5      52
53  versicolor          6.9         3.1          4.9         1.5      53
54  versicolor          5.5         2.3          4.0         1.3      54
55  versicolor          6.5         2.8          4.6         1.5      55
56  versicolor          5.7         2.8          4.5         1.3      56
57  versicolor          6.3         3.3          4.7         1.6      57
58  versicolor          4.9         2.4          3.3         1.0      58
59  versicolor          6.6         2.9          4.6         1.3      59
60  versicolor          5.2         2.7          3.9         1.4      60
61  versicolor          5.0         2.0          3.5         1.0      61
62  versicolor          5.9         3.0          4.2         1.5      62
63  versicolor          6.0         2.2          4.0         1.0      63
64  versicolor          6.1         2.9          4.7         1.4      64
65  versicolor          5.6         2.9          3.6         1.3      65
66  versicolor          6.7         3.1          4.4         1.4      66
67  versicolor          5.6         3.0          4.5         1.5      67
68  versicolor          5.8         2.7          4.1         1.0      68
69  versicolor          6.2         2.2          4.5         1.5      69
70  versicolor          5.6         2.5          3.9         1.1      70
71  versicolor          5.9         3.2          4.8         1.8      71
72  versicolor          6.1         2.8          4.0         1.3      72
73  versicolor          6.3         2.5          4.9         1.5      73
74  versicolor          6.1         2.8          4.7         1.2      74
75  versicolor          6.4         2.9          4.3         1.3      75
76  versicolor          6.6         3.0          4.4         1.4      76
77  versicolor          6.8         2.8          4.8         1.4      77
78  versicolor          6.7         3.0          5.0         1.7      78
79  versicolor          6.0         2.9          4.5         1.5      79
80  versicolor          5.7         2.6          3.5         1.0      80
81  versicolor          5.5         2.4          3.8         1.1      81
82  versicolor          5.5         2.4          3.7         1.0      82
83  versicolor          5.8         2.7          3.9         1.2      83
84  versicolor          6.0         2.7          5.1         1.6      84
85  versicolor          5.4         3.0          4.5         1.5      85
86  versicolor          6.0         3.4          4.5         1.6      86
87  versicolor          6.7         3.1          4.7         1.5      87
88  versicolor          6.3         2.3          4.4         1.3      88
89  versicolor          5.6         3.0          4.1         1.3      89
90  versicolor          5.5         2.5          4.0         1.3      90
91  versicolor          5.5         2.6          4.4         1.2      91
92  versicolor          6.1         3.0          4.6         1.4      92
93  versicolor          5.8         2.6          4.0         1.2      93
94  versicolor          5.0         2.3          3.3         1.0      94
95  versicolor          5.6         2.7          4.2         1.3      95
96  versicolor          5.7         3.0          4.2         1.2      96
97  versicolor          5.7         2.9          4.2         1.3      97
98  versicolor          6.2         2.9          4.3         1.3      98
99  versicolor          5.1         2.5          3.0         1.1      99
100 versicolor          5.7         2.8          4.1         1.3     100
101  virginica          6.3         3.3          6.0         2.5     101
102  virginica          5.8         2.7          5.1         1.9     102
103  virginica          7.1         3.0          5.9         2.1     103
104  virginica          6.3         2.9          5.6         1.8     104
105  virginica          6.5         3.0          5.8         2.2     105
106  virginica          7.6         3.0          6.6         2.1     106
107  virginica          4.9         2.5          4.5         1.7     107
108  virginica          7.3         2.9          6.3         1.8     108
109  virginica          6.7         2.5          5.8         1.8     109
110  virginica          7.2         3.6          6.1         2.5     110
111  virginica          6.5         3.2          5.1         2.0     111
112  virginica          6.4         2.7          5.3         1.9     112
113  virginica          6.8         3.0          5.5         2.1     113
114  virginica          5.7         2.5          5.0         2.0     114
115  virginica          5.8         2.8          5.1         2.4     115
116  virginica          6.4         3.2          5.3         2.3     116
117  virginica          6.5         3.0          5.5         1.8     117
118  virginica          7.7         3.8          6.7         2.2     118
119  virginica          7.7         2.6          6.9         2.3     119
120  virginica          6.0         2.2          5.0         1.5     120
121  virginica          6.9         3.2          5.7         2.3     121
122  virginica          5.6         2.8          4.9         2.0     122
123  virginica          7.7         2.8          6.7         2.0     123
124  virginica          6.3         2.7          4.9         1.8     124
125  virginica          6.7         3.3          5.7         2.1     125
126  virginica          7.2         3.2          6.0         1.8     126
127  virginica          6.2         2.8          4.8         1.8     127
128  virginica          6.1         3.0          4.9         1.8     128
129  virginica          6.4         2.8          5.6         2.1     129
130  virginica          7.2         3.0          5.8         1.6     130
131  virginica          7.4         2.8          6.1         1.9     131
132  virginica          7.9         3.8          6.4         2.0     132
133  virginica          6.4         2.8          5.6         2.2     133
134  virginica          6.3         2.8          5.1         1.5     134
135  virginica          6.1         2.6          5.6         1.4     135
136  virginica          7.7         3.0          6.1         2.3     136
137  virginica          6.3         3.4          5.6         2.4     137
138  virginica          6.4         3.1          5.5         1.8     138
139  virginica          6.0         3.0          4.8         1.8     139
140  virginica          6.9         3.1          5.4         2.1     140
141  virginica          6.7         3.1          5.6         2.4     141
142  virginica          6.9         3.1          5.1         2.3     142
143  virginica          5.8         2.7          5.1         1.9     143
144  virginica          6.8         3.2          5.9         2.3     144
145  virginica          6.7         3.3          5.7         2.5     145
146  virginica          6.7         3.0          5.2         2.3     146
147  virginica          6.3         2.5          5.0         1.9     147
148  virginica          6.5         3.0          5.2         2.0     148
149  virginica          6.2         3.4          5.4         2.3     149
150  virginica          5.9         3.0          5.1         1.8     150
      (strata) (weights)
1       setosa         1
2       setosa         1
3       setosa         1
4       setosa         1
5       setosa         1
6       setosa         1
7       setosa         1
8       setosa         1
9       setosa         1
10      setosa         1
11      setosa         1
12      setosa         1
13      setosa         1
14      setosa         1
15      setosa         1
16      setosa         1
17      setosa         1
18      setosa         1
19      setosa         1
20      setosa         1
21      setosa         1
22      setosa         1
23      setosa         1
24      setosa         1
25      setosa         1
26      setosa         1
27      setosa         1
28      setosa         1
29      setosa         1
30      setosa         1
31      setosa         1
32      setosa         1
33      setosa         1
34      setosa         1
35      setosa         1
36      setosa         1
37      setosa         1
38      setosa         1
39      setosa         1
40      setosa         1
41      setosa         1
42      setosa         1
43      setosa         1
44      setosa         1
45      setosa         1
46      setosa         1
47      setosa         1
48      setosa         1
49      setosa         1
50      setosa         1
51  versicolor         1
52  versicolor         1
53  versicolor         1
54  versicolor         1
55  versicolor         1
56  versicolor         1
57  versicolor         1
58  versicolor         1
59  versicolor         1
60  versicolor         1
61  versicolor         1
62  versicolor         1
63  versicolor         1
64  versicolor         1
65  versicolor         1
66  versicolor         1
67  versicolor         1
68  versicolor         1
69  versicolor         1
70  versicolor         1
71  versicolor         1
72  versicolor         1
73  versicolor         1
74  versicolor         1
75  versicolor         1
76  versicolor         1
77  versicolor         1
78  versicolor         1
79  versicolor         1
80  versicolor         1
81  versicolor         1
82  versicolor         1
83  versicolor         1
84  versicolor         1
85  versicolor         1
86  versicolor         1
87  versicolor         1
88  versicolor         1
89  versicolor         1
90  versicolor         1
91  versicolor         1
92  versicolor         1
93  versicolor         1
94  versicolor         1
95  versicolor         1
96  versicolor         1
97  versicolor         1
98  versicolor         1
99  versicolor         1
100 versicolor         1
101  virginica         1
102  virginica         1
103  virginica         1
104  virginica         1
105  virginica         1
106  virginica         1
107  virginica         1
108  virginica         1
109  virginica         1
110  virginica         1
111  virginica         1
112  virginica         1
113  virginica         1
114  virginica         1
115  virginica         1
116  virginica         1
117  virginica         1
118  virginica         1
119  virginica         1
120  virginica         1
121  virginica         1
122  virginica         1
123  virginica         1
124  virginica         1
125  virginica         1
126  virginica         1
127  virginica         1
128  virginica         1
129  virginica         1
130  virginica         1
131  virginica         1
132  virginica         1
133  virginica         1
134  virginica         1
135  virginica         1
136  virginica         1
137  virginica         1
138  virginica         1
139  virginica         1
140  virginica         1
141  virginica         1
142  virginica         1
143  virginica         1
144  virginica         1
145  virginica         1
146  virginica         1
147  virginica         1
148  virginica         1
149  virginica         1
150  virginica         1

$k
[1] 7

$distance
[1] 2

$scale
[1] TRUE

$kernel
[1] "optimal"

attr(,"class")
[1] "list"
> 
> 
> 
> 
> cleanEx()
> nameEx("LARSModel")
> ### * LARSModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: LARSModel
> ### Title: Least Angle Regression, Lasso and Infinitesimal Forward
> ###   Stagewise Models
> ### Aliases: LARSModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = LARSModel)

Call:
lars::lars(x = x, y = y, type = "lasso", trace = FALSE, normalize = TRUE, 
    intercept = TRUE, use.Gram = TRUE)
R-squared: 0.722 
Sequence of LASSO moves:
     lot_size bedrooms base_size garage1_size add_size
Var        18       19        14           16       15
Step        1        2         3            4        5
     construction2 Story Frame basementYes construction2 Story Brick
Var                         11          20                         9
Step                         6           7                         8
     construction1 Story Frame atticYes constructionSplit Foyer Frame
Var                          8       22                            12
Step                         9       10                            11
     garage2_size construction1 Story Condo styleCondo
Var            17                         7          5
Step           12                        13         14
     construction2 Story Condo lat construction1 Story Brick sale_year
Var                         10  24                         6         1
Step                        15  16                        17        18
     sale_month built constructionSplit Level Frame construction1 Story Brick
Var           2     3                            13                        -6
Step         19    20                            21                        22
     bedrooms acYes construction1 Story Brick bedrooms lon styleHome
Var       -19    21                         6       19  23         4
Step       23    24                        25       26  27        28
> 
> 
> 
> 
> cleanEx()
> nameEx("LDAModel")
> ### * LDAModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: LDAModel
> ### Title: Linear Discriminant Analysis Model
> ### Aliases: LDAModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = LDAModel)
Call:
lda(formula, data = as.data.frame(data), tol = 1e-04, method = "moment", 
    nu = 5)

Prior probabilities of groups:
    setosa versicolor  virginica 
 0.3333333  0.3333333  0.3333333 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa            5.006       3.428        1.462       0.246
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026

Coefficients of linear discriminants:
                    LD1         LD2
Sepal.Length  0.8293776  0.02410215
Sepal.Width   1.5344731  2.16452123
Petal.Length -2.2012117 -0.93192121
Petal.Width  -2.8104603  2.83918785

Proportion of trace:
   LD1    LD2 
0.9912 0.0088 
> 
> 
> 
> 
> cleanEx()
> nameEx("LMModel")
> ### * LMModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: LMModel
> ### Title: Linear Models
> ### Aliases: LMModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = LMModel)

Call:
stats::lm(formula = formula, data = data, weights = weights)

Coefficients:
                  (Intercept)                      sale_year  
                   -1.742e+07                      2.513e+03  
                   sale_month                          built  
                   -7.918e+02                      3.029e+02  
                   styleCondo      construction1 Story Brick  
                   -1.774e+04                     -1.176e+04  
    construction1 Story Condo      construction1 Story Frame  
                   -6.658e+04                     -6.416e+04  
    construction2 Story Brick      construction2 Story Condo  
                    1.131e+05                      9.514e+03  
    construction2 Story Frame  constructionSplit Foyer Frame  
                    2.323e+04                     -5.714e+04  
constructionSplit Level Frame                      base_size  
                   -3.302e+04                      1.635e+02  
                     add_size                   garage1_size  
                    1.433e+02                      5.151e+01  
                 garage2_size                       lot_size  
                    6.838e+01                      1.232e+00  
                     bedrooms                    basementYes  
                   -1.557e+03                      2.387e+04  
                        acYes                       atticYes  
                   -2.600e+03                      4.763e+04  
                          lon                            lat  
                    5.430e+03                      2.949e+05  

> 
> 
> 
> 
> cleanEx()
> nameEx("MDAModel")
> ### * MDAModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: MDAModel
> ### Title: Mixture Discriminant Analysis Model
> ### Aliases: MDAModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = MDAModel)
Call:
mda::mda(formula = formula, data = as.data.frame(data), subclasses = 3, 
    dimension = 2, eps = 2.22044604925031e-16, iter = 5, method = ..5, 
    trace = FALSE)

Dimension: 2 

Percent Between-Group Variance Explained:
   v1    v2 
96.74 99.50 

Degrees of Freedom (per dimension): 5 

Training Misclassification Error: 0.01333 ( N = 150 )

Deviance: 18.014 
> 
> 
> 
> 
> cleanEx()
> nameEx("MLControl")
> ### * MLControl
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: MLControl
> ### Title: Resampling Controls
> ### Aliases: MLControl controls BootControl BootOptimismControl CVControl
> ###   CVOptimismControl OOBControl SplitControl TrainControl
> 
> ### ** Examples
> 
> ## Bootstrapping with 100 samples
> BootControl(samples = 100)
Object of class "MLControl"

Name: BootControl
Label: Bootstrap Resampling
Samples: 100 
Seed: 1140350788 
> 
> ## Optimism-corrected bootstrapping with 100 samples
> BootOptimismControl(samples = 100)
Object of class "MLControl"

Name: BootOptimismControl
Label: Optimism-Corrected Bootstrap Resampling
Samples: 100 
Seed: 312928385 
> 
> ## Cross-validation with 5 repeats of 10 folds
> CVControl(folds = 10, repeats = 5)
Object of class "MLControl"

Name: CVControl
Label: K-Fold Cross-Validation
Folds: 10
Repeats: 5
Seed: 866248189 
> 
> ## Optimism-corrected cross-validation with 5 repeats of 10 folds
> CVOptimismControl(folds = 10, repeats = 5)
Object of class "MLControl"

Name: CVOptimismControl
Label: Optimism-Corrected K-Fold Cross-Validation
Folds: 10
Repeats: 5
Seed: 1909893419 
> 
> ## Out-of-bootstrap validation with 100 samples
> OOBControl(samples = 100)
Object of class "MLControl"

Name: OOBControl
Label: Out-Of-Bootstrap Resampling
Samples: 100
Seed: 554504146 
> 
> ## Split sample validation with 2/3 training and 1/3 testing
> SplitControl(prop = 2/3)
Object of class "MLControl"

Name: SplitControl
Label: Split Training and Test Samples
Training proportion: 0.6666667
Seed: 884616499 
> 
> ## Training set evaluation
> TrainControl()
Object of class "MLControl"

Name: TrainControl
Label: Training Resubstitution
Seed: 803234389 
> 
> 
> 
> 
> cleanEx()
> nameEx("MLMetric")
> ### * MLMetric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: MLMetric
> ### Title: MLMetric Class Constructor
> ### Aliases: MLMetric MLMetric<-
> 
> ### ** Examples
> 
> f2_score <- function(observed, predicted, ...) {
+   f_score(observed, predicted, beta = 2, ...)
+ }
> 
> MLMetric(f2_score) <- list(name = "f2_score",
+                            label = "F Score (beta = 2)",
+                            maximize = TRUE)
> 
> 
> 
> 
> cleanEx()
> nameEx("MLModel")
> ### * MLModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: MLModel
> ### Title: MLModel Class Constructor
> ### Aliases: MLModel
> 
> ### ** Examples
> 
> ## Logistic regression model
> LogisticModel <- MLModel(
+   name = "LogisticModel",
+   response_types = "binary",
+   fit = function(formula, data, weights, ...) {
+     glm(formula, data = data, weights = weights, family = binomial, ...)
+   },
+   predict = function(object, newdata, ...) {
+     predict(object, newdata = newdata, type = "response")
+   },
+   varimp = function(object, ...) {
+     pchisq(coef(object)^2 / diag(vcov(object)), 1)
+   }
+ )
> 
> library(MASS)
> res <- resample(type ~ ., data = Pima.tr, model = LogisticModel)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following object is masked from ‘package:MASS’:

    select

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> summary(res)
             Statistic
Metric             Mean    Median         SD        Min       Max NA
  Brier       0.1639794 0.1509733 0.04906847 0.10252110 0.2638165  0
  Accuracy    0.7497995 0.7947368 0.08178706 0.60000000 0.8095238  0
  Kappa       0.4196863 0.5115413 0.17915083 0.05882353 0.5604396  0
  ROC AUC     0.8321821 0.8516484 0.07325054 0.71428571 0.9340659  0
  Sensitivity 0.5476190 0.5714286 0.15307382 0.28571429 0.8333333  0
  Specificity 0.8549451 0.8846154 0.09985297 0.69230769 1.0000000  0
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’,
  ‘package:MASS’

> nameEx("ModelFrame-methods")
> ### * ModelFrame-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ModelFrame
> ### Title: ModelFrame Class
> ### Aliases: ModelFrame ModelFrame.formula ModelFrame.matrix
> 
> ### ** Examples
> 
> mf <- ModelFrame(ncases / (ncases + ncontrols) ~ agegp + tobgp + alcgp,
+                  data = esoph, weights = with(esoph, ncases + ncontrols))
> gbm_fit <- fit(mf, model = GBMModel)
> varimp(gbm_fit)
Object of class "VarImp"
        Overall
alcgp 100.00000
agegp  72.46204
tobgp   0.00000
> 
> 
> 
> 
> cleanEx()
> nameEx("ModeledInput-methods")
> ### * ModeledInput-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ModeledInput
> ### Title: ModeledInput Classes
> ### Aliases: ModeledInput ModeledFrame ModeledRecipe ModeledInput.formula
> ###   ModeledInput.matrix ModeledInput.ModelFrame ModeledInput.recipe
> ###   ModeledInput.MLModel ModeledInput.MLModelFunction
> 
> ### ** Examples
> 
> ## Modeled model frame
> mod_mf <- ModeledInput(sale_amount ~ ., data = ICHomes, model = GLMModel)
> fit(mod_mf)

Call:  stats::glm(formula = formula, family = family, data = data, weights = weights, 
    control = control)

Coefficients:
                  (Intercept)                      sale_year  
                   -1.742e+07                      2.513e+03  
                   sale_month                          built  
                   -7.918e+02                      3.029e+02  
                   styleCondo      construction1 Story Brick  
                   -1.774e+04                     -1.176e+04  
    construction1 Story Condo      construction1 Story Frame  
                   -6.658e+04                     -6.416e+04  
    construction2 Story Brick      construction2 Story Condo  
                    1.131e+05                      9.514e+03  
    construction2 Story Frame  constructionSplit Foyer Frame  
                    2.323e+04                     -5.714e+04  
constructionSplit Level Frame                      base_size  
                   -3.302e+04                      1.635e+02  
                     add_size                   garage1_size  
                    1.433e+02                      5.151e+01  
                 garage2_size                       lot_size  
                    6.838e+01                      1.232e+00  
                     bedrooms                    basementYes  
                   -1.557e+03                      2.387e+04  
                        acYes                       atticYes  
                   -2.600e+03                      4.763e+04  
                          lon                            lat  
                    5.430e+03                      2.949e+05  

Degrees of Freedom: 752 Total (i.e. Null);  729 Residual
Null Deviance:	    6.323e+12 
Residual Deviance: 1.76e+12 	AIC: 18430
> 
> ## Modeled recipe
> library(recipes)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> 
> rec <- recipe(sale_amount ~ ., data = ICHomes)
> mod_rec <- ModeledInput(rec, model = GLMModel)
> fit(mod_rec)

Call:  stats::glm(formula = formula, family = family, data = data, weights = weights, 
    control = control)

Coefficients:
                  (Intercept)                      sale_year  
                   -1.742e+07                      2.513e+03  
                   sale_month                          built  
                   -7.918e+02                      3.029e+02  
                   styleCondo      construction1 Story Brick  
                   -1.774e+04                     -1.176e+04  
    construction1 Story Condo      construction1 Story Frame  
                   -6.658e+04                     -6.416e+04  
    construction2 Story Brick      construction2 Story Condo  
                    1.131e+05                      9.514e+03  
    construction2 Story Frame  constructionSplit Foyer Frame  
                    2.323e+04                     -5.714e+04  
constructionSplit Level Frame                      base_size  
                   -3.302e+04                      1.635e+02  
                     add_size                   garage1_size  
                    1.433e+02                      5.151e+01  
                 garage2_size                       lot_size  
                    6.838e+01                      1.232e+00  
                     bedrooms                    basementYes  
                   -1.557e+03                      2.387e+04  
                        acYes                       atticYes  
                   -2.600e+03                      4.763e+04  
                          lon                            lat  
                    5.430e+03                      2.949e+05  

Degrees of Freedom: 752 Total (i.e. Null);  729 Residual
Null Deviance:	    6.323e+12 
Residual Deviance: 1.76e+12 	AIC: 18430
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’

> nameEx("NNetModel")
> ### * NNetModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: NNetModel
> ### Title: Neural Network Model
> ### Aliases: NNetModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = NNetModel)
a 23-1-1 network with 26 weights
inputs: sale_year sale_month built styleCondo construction1 Story Brick construction1 Story Condo construction1 Story Frame construction2 Story Brick construction2 Story Condo construction2 Story Frame constructionSplit Foyer Frame constructionSplit Level Frame base_size add_size garage1_size garage2_size lot_size bedrooms basementYes acYes atticYes lon lat 
output(s): sale_amount 
options were -
> 
> 
> 
> 
> cleanEx()
> nameEx("NaiveBayesModel")
> ### * NaiveBayesModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: NaiveBayesModel
> ### Title: Naive Bayes Classifier Model
> ### Aliases: NaiveBayesModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = NaiveBayesModel)

Naive Bayes Classifier for Discrete Predictors

Call:
naiveBayes.default(x = X, y = Y, laplace = laplace)

A-priori probabilities:
Y
    setosa versicolor  virginica 
 0.3333333  0.3333333  0.3333333 

Conditional probabilities:
            Sepal.Length
Y             [,1]      [,2]
  setosa     5.006 0.3524897
  versicolor 5.936 0.5161711
  virginica  6.588 0.6358796

            Sepal.Width
Y             [,1]      [,2]
  setosa     3.428 0.3790644
  versicolor 2.770 0.3137983
  virginica  2.974 0.3224966

            Petal.Length
Y             [,1]      [,2]
  setosa     1.462 0.1736640
  versicolor 4.260 0.4699110
  virginica  5.552 0.5518947

            Petal.Width
Y             [,1]      [,2]
  setosa     0.246 0.1053856
  versicolor 1.326 0.1977527
  virginica  2.026 0.2746501

> 
> 
> 
> 
> cleanEx()
> nameEx("PLSModel")
> ### * PLSModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: PLSModel
> ### Title: Partial Least Squares Model
> ### Aliases: PLSModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = PLSModel)
Partial least squares regression , fitted with the kernel algorithm.
Call:
plsr(formula = formula, ncomp = 1, data = data, scale = FALSE)
> 
> 
> 
> 
> cleanEx()
> nameEx("POLRModel")
> ### * POLRModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: POLRModel
> ### Title: Ordered Logistic or Probit Regression Model
> ### Aliases: POLRModel
> 
> ### ** Examples
> 
> library(MASS)
> 
> df <- within(Boston,
+              medv <- cut(medv,
+                          breaks = c(0, 10, 15, 20, 25, 50),
+                          ordered = TRUE))
> fit(medv ~ ., data = df, model = POLRModel)
Call:
MASS::polr(formula = formula, data = as.data.frame(data), weights = weights, 
    Hess = TRUE, method = "logistic")

Coefficients:
        crim           zn        indus         chas          nox           rm 
-0.080542658  0.018147523  0.041468252  1.391044577 -7.451763426  0.813790690 
         age          dis          rad          tax      ptratio        black 
-0.013212452 -0.588957301  0.158362385 -0.008421052 -0.382420894  0.006844861 
       lstat 
-0.308490350 

Intercepts:
 (0,10]|(10,15] (10,15]|(15,20] (15,20]|(20,25] (20,25]|(25,50] 
     -19.841984      -15.919569      -12.653795       -8.903613 

Residual Deviance: 759.2677 
AIC: 793.2677 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’

> nameEx("ParameterGrid")
> ### * ParameterGrid
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ParameterGrid
> ### Title: Tuning Parameters Grid
> ### Aliases: ParameterGrid ParameterGrid.param ParameterGrid.list
> ###   ParameterGrid.parameters
> 
> ### ** Examples
> 
> ## GBMModel tuning parameters
> library(dials)
Loading required package: scales
> 
> grid <- ParameterGrid(
+   n.trees = trees(),
+   interaction.depth = tree_depth(),
+   random = 5
+ )
> TunedModel(GBMModel, grid = grid)
Object of class "TunedModel"

Model name: TunedModel
Label: Grid Tuned Model
Package: 
Response types: factor, numeric, PoissonVariate, Surv
Tuning grid: FALSE
Variable importance: FALSE

Tuning parameters:

Object of class "MLModelFunction"

Model name: GBMModel
Label: Generalized Boosted Regression
Package: gbm
Response types: factor, numeric, PoissonVariate, Surv
Tuning grid: TRUE
Variable importance: TRUE

Arguments:
function (distribution = NULL, n.trees = 100, interaction.depth = 1, 
    n.minobsinnode = 10, shrinkage = 0.1, bag.fraction = 0.5) 
NULL

Object of class "ParameterGrid"
Collection of 2 parameters for tuning

                id parameter type object class
           n.trees          trees    nparam[+]
 interaction.depth     tree_depth    nparam[+]

Random sample: 5 

Object of class "MLControl"

Name: CVControl
Label: K-Fold Cross-Validation
Folds: 10
Repeats: 1
Seed: 1140350788 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:dials’, ‘package:scales’

> nameEx("QDAModel")
> ### * QDAModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: QDAModel
> ### Title: Quadratic Discriminant Analysis Model
> ### Aliases: QDAModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = QDAModel)
Call:
qda(formula, data = as.data.frame(data), method = "moment", nu = 5)

Prior probabilities of groups:
    setosa versicolor  virginica 
 0.3333333  0.3333333  0.3333333 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa            5.006       3.428        1.462       0.246
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026
> 
> 
> 
> 
> cleanEx()
> nameEx("RPartModel")
> ### * RPartModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: RPartModel
> ### Title: Recursive Partitioning and Regression Tree Models
> ### Aliases: RPartModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = RPartModel)
n= 150 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  
  2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
  3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  
    6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
    7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
> 
> 
> 
> 
> cleanEx()
> nameEx("RandomForestModel")
> ### * RandomForestModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: RandomForestModel
> ### Title: Random Forest Model
> ### Aliases: RandomForestModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = RandomForestModel)

Call:
 randomForest(formula = formula, data = as.data.frame(data), ntree = 500,      mtry = ..2, replace = TRUE, nodesize = ..4) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 5

          Mean of squared residuals: 2372118364
                    % Var explained: 71.75
> 
> 
> 
> 
> cleanEx()
> nameEx("RangerModel")
> ### * RangerModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: RangerModel
> ### Title: Fast Random Forest Model
> ### Aliases: RangerModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = RangerModel)
Ranger result

Call:
 ranger::ranger(formula, data = as.data.frame(data), case.weights = weights,      probability = is(response(data), "factor"), ...) 

Type:                             Probability estimation 
Number of trees:                  500 
Sample size:                      150 
Number of independent variables:  4 
Mtry:                             2 
Target node size:                 10 
Variable importance mode:         impurity 
Splitrule:                        gini 
OOB prediction error (Brier s.):  0.03386357 
> 
> 
> 
> 
> cleanEx()
> nameEx("SVMModel")
> ### * SVMModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SVMModel
> ### Title: Support Vector Machine Models
> ### Aliases: SVMModel SVMANOVAModel SVMBesselModel SVMLaplaceModel
> ###   SVMLinearModel SVMPolyModel SVMRadialModel SVMSplineModel
> ###   SVMTanhModel
> 
> ### ** Examples
> 
> fit(sale_amount ~ ., data = ICHomes, model = SVMRadialModel)
Support Vector Machine object of class "ksvm" 

SV type: eps-svr  (regression) 
 parameter : epsilon = 0.1  cost C = 1 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  0.065392361053293 

Number of Support Vectors : 506 

Objective Function Value : -146.2279 
Training error : 0.271507 
Laplace distr. width : 152905.4 
> 
> 
> 
> 
> cleanEx()
> nameEx("SelectedInput")
> ### * SelectedInput
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SelectedInput
> ### Title: Selected Model Inputs
> ### Aliases: SelectedInput SelectedModelFrame SelectedModelRecipe
> ###   SelectedInput.formula SelectedInput.matrix SelectedInput.ModelFrame
> ###   SelectedInput.recipe SelectedInput.list
> 
> ### ** Examples
> 
> ## Selected model frame
> sel_mf <- SelectedInput(
+   sale_amount ~ sale_year + built + style + construction,
+   sale_amount ~ sale_year + base_size + bedrooms + basement,
+   data = ICHomes
+ )
> 
> fit(sel_mf, model = GLMModel)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step


Call:  stats::glm(formula = formula, family = family, data = data, weights = weights, 
    control = control)

Coefficients:
(Intercept)    sale_year    base_size     bedrooms  basementYes  
 -1721787.2        844.4         93.1      28592.2      38419.6  

Degrees of Freedom: 752 Total (i.e. Null);  748 Residual
Null Deviance:	    6.323e+12 
Residual Deviance: 3.939e+12 	AIC: 19000
> 
> ## Selected recipe
> library(recipes)
> library(MASS)

Attaching package: ‘MASS’

The following object is masked from ‘package:dplyr’:

    select

> 
> rec1 <- recipe(medv ~ crim + zn + indus + chas + nox + rm, data = Boston)
> rec2 <- recipe(medv ~ chas + nox + rm + age + dis + rad + tax, data = Boston)
> sel_rec <- SelectedInput(rec1, rec2)
> 
> fit(sel_rec, model = GLMModel)

Call:  stats::glm(formula = formula, family = family, data = data, weights = weights, 
    control = control)

Coefficients:
(Intercept)         chas          nox           rm          age          dis  
   -3.46367      4.42243    -12.66216      7.47093     -0.06916     -1.10377  
        rad          tax  
    0.04480     -0.01405  

Degrees of Freedom: 505 Total (i.e. Null);  498 Residual
Null Deviance:	    42720 
Residual Deviance: 16770 	AIC: 3225
> 
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’, ‘package:recipes’, ‘package:dplyr’,
  ‘package:survival’

> nameEx("SelectedModel")
> ### * SelectedModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SelectedModel
> ### Title: Selected Model
> ### Aliases: SelectedModel
> 
> ### ** Examples
> 
> model_fit <- fit(sale_amount ~ ., data = ICHomes,
+                  model = SelectedModel(GBMModel, GLMNetModel, SVMRadialModel))
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> (selected_model <- as.MLModel(model_fit))
Object of class "MLModel"

Model name: GLMNetModel
Label: Trained Lasso and Elastic-Net
Package: glmnet
Response types: BinomialVariate, factor, matrix, numeric, PoissonVariate, Surv
Tuning grid: TRUE
Variable importance: TRUE

Parameters:
List of 10
 $ alpha               : num 1
 $ lambda              : num 0
 $ standardize         : logi TRUE
 $ penalty.factor      : language rep(1, nvars)
 $ standardize.response: logi FALSE
 $ thresh              : num 1e-07
 $ maxit               : num 1e+05
 $ type.gaussian       : language ifelse(nvars < 500, "covariance", "naive")
 $ type.logistic       : chr "Newton"
 $ type.multinomial    : chr "ungrouped"

TrainStep1 :
Object of class "TrainBit"

Grid (selected = 2):
[90m# A tibble: 3 x 1[39m
  Model
  [3m[90m<fct>[39m[23m
[90m1[39m 1    
[90m2[39m 2    
[90m3[39m 3    

Object of class "Performance"

Metrics: RMSE, R2, MAE 
Models: GBMModel, GLMNetModel, SVMRadialModel 

Selected model: GLMNetModel 
RMSE value: 52174.29 

> summary(selected_model)
$TrainStep1
, , Metric = RMSE

                Statistic
Model                Mean   Median        SD      Min      Max NA
  GBMModel       54306.70 55624.72  9930.404 37678.21 70226.38  0
  GLMNetModel    52174.29 49681.91  9297.166 41425.69 73069.69  0
  SVMRadialModel 56980.89 57756.54 15337.766 36522.91 78434.67  0

, , Metric = R2

                Statistic
Model                 Mean    Median        SD       Min       Max NA
  GBMModel       0.6236452 0.6424882 0.1143535 0.4059194 0.8018221  0
  GLMNetModel    0.6520089 0.6945077 0.1131726 0.3764064 0.7464400  0
  SVMRadialModel 0.5999655 0.5990770 0.1174970 0.4217759 0.8137890  0

, , Metric = MAE

                Statistic
Model                Mean   Median       SD      Min      Max NA
  GBMModel       33414.03 34161.85 3482.785 27450.03 37721.29  0
  GLMNetModel    32917.57 33160.13 2844.599 29383.73 37582.69  0
  SVMRadialModel 28807.26 29765.96 4321.728 21233.54 36440.75  0


> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("StackedModel")
> ### * StackedModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: StackedModel
> ### Title: Stacked Regression Model
> ### Aliases: StackedModel
> 
> ### ** Examples
> 
> model <- StackedModel(GBMModel, SVMRadialModel, GLMNetModel(lambda = 0.01))
> model_fit <- fit(sale_amount ~ ., data = ICHomes, model = model)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> predict(model_fit, newdata = ICHomes)
  [1] 100449 190329 226684 140213 260121 269356 166601 235766 206405 117339
 [11] 176168 169449 512377 598169 147613 210994 137914 411971 276748 115120
 [21] 221781  92463 148505 169247 150491 117145 178672 137186 165389 356866
 [31] 187164 113539 111737 171995 169043 176373  92606 348008 172025 136742
 [41] 182443 123901  98160 319366 163641 312642 198619 202290 438903 279920
 [51] 340510 131925 211469 273352  93641 164875 168145 153495 148912 180889
 [61] 147894 142724 123863 192918  95768 138643 290559 134333 273496 169969
 [71] 130141 165083  70688 248024 163482 193847 161187 183438  91143 220660
 [81] 206267 158983 207191 118575 200662 181916 165808 203926 263877 146898
 [91] 162938 137807 124592 186812 416080 198763 125750 218104 161781 143504
[101] 190983 221674 170301  96426 158045 186749 317016 143965 148432 100139
[111] 172720 144927 260253 379994 150004 397573 172567 129127 219612 158177
[121] 123866 118649  81490 150672 191648 132317 127031 155987 238956 179838
[131] 177375  69732 273984 195000 160583 155901 125108 181237 148237 147208
[141] 173635 115395 193528 123390 114811 165344  92916 290587 143824 232562
[151] 136167 432174 170681 214800 213044 155664 167606 116646 154432 160483
[161] 262004 152546 129576 250227 148518 200850 204344 149817  82178 184072
[171] 344317 189667 132814 102707 198280 381879 167364 244491 133421 164390
[181] 128544 174264 205715 171781 146504 304725 145261  98522 199177 130629
[191] 142507 149884 125423 161318 326069 118149 132475  84614 151489 194610
[201]  93292 250114 149497 178053  98006  82285 264233 144987 149010 168991
[211] 147505 145485 144726 109569 152874 251034 134120 155273 168062 113537
[221] 180036 303754 233123 224832  73704 170055 187400 104356 111257 189903
[231] 177190 192339 239369 185368 254968 269238 138584  94386 194150 146761
[241] 163369 110012 197972 118230 159320 132930 168788 203184 151081 149993
[251] 144588 174802 143554 147025 151566 120643 276297  94307 143590 200724
[261] 141959 488896 238864 167082 195593 125016  44835 144946 244304 160190
[271] 202975 173489 104716 249343 113030 137560 183835 222737 190580 386967
[281]  88854 189393  83478 111857 139079 228264 256340  93985 166025 135955
[291] 238312 108804 160098 155636 179510 135686 186101 115159 196514 240324
[301] 453859 131465 136520 115989 226465  81069 150734 230795 185384 201050
[311] 148720 208531 135243 113102 158372 126557 331498 364544 261527 156880
[321]  40932 229519 183099 138195 185973 161825 177275 147592 168052 112851
[331] 148176 364817 289647 147289 211546 144202 252524 111633 379519 109827
[341] 171055  81006 145793 184213 180420  93900 174401 216893 141228 182126
[351] 140442 152933 207091 140909 126072 230975 203256 168349 272959 101992
[361] 165026  89350 163610 160951 239319 140961 129642 154699 237479 300991
[371] 141958 190196 307449 104164 185709  95204 191506 148094 190355 150332
[381] 226895 186074 144627 216119 190629 137598 126410 484026 166804 204964
[391] 175087 141107 123996 184589 260330 121073 140252 180313 203022 224070
[401] 222704 236005 153678 368411 275858 123964 281550 130948 147542 147523
[411] 279077 134057 164710 257187 170029 180853 191831 131140 127351  97976
[421] 120073 348822 110566 217040 134311 151029 151207 202847 303928 165235
[431] 145558 132257 332387 209836 146039 248539  94461 270437 211694 340602
[441] 132894 189711 176364  85164 154211 160710 174619 254280 272715 256984
[451] 184004 231401 180510 144373 116621 153182 162977 164701 202546 547213
[461] 191349 141516 142974 192139 115189 135322 191435 156432 251715 160443
[471] 145903 296561 123222 143640 200663 194384 181928  68924 146246 150734
[481] 259742  52335 123851 207827  93404 215950 129707 152568 173925 376262
[491] 168667 137112 157841 260064 149207 211109 144754 306565  89362 129192
[501] 161654 125257 185122 187506 161098 111060  97428 141511 147732 154524
[511] 182402 273186 131933 142607 108785 162841 126379 348225 162365 148923
[521] 229904 256450 180175 175370 111807 138597 152964 154505 379493 181356
[531] 195800 230602 149848 104060 133056 166016 116841 193300 111732 170484
[541] 196758 151074 202341 128726 144940 191606 137686  73181  89223 320623
[551] 159138 183944 137236 152201 141830 166709 235435 295425 273248 134829
[561] 100952 198364 127709 223862 137079 208203 173748 362244 517748 241899
[571] 182790 230941 230965 125468  73519 144094 112278 172744  73946 359856
[581] 229438 177744 134410 138615 161554 158660 151967 168624 312888 249245
[591] 339480 142399 187482 116758 224148 294619 159693 175011 277244 255108
[601] 116928 243664 132616 256005 150616 119162 270486 244953 271042 134324
[611] 165147 187942 177339 100395 285181 152239 173845 184946 127604 173033
[621] 193703 224982 187297 123927 177563 147689 104993 177570 147675 239610
[631] 127826 158264 200505 167192 182964 158909 135176 221154  87315 185325
[641] 146977 192944 153761 115557 147171 234370 112843 156950 148158 117704
[651] 104881 126408 107148 132317  95242 135907 178297 140254 221081  92155
[661] 180282 357501 137867 219187 195653 172302 125739  97490 143093 237189
[671] 150669 288257 184690  97015 178989 262214 147167 152050 167840  95340
[681]  72826 134457 160916  40537 100919 164646 264667 116977 258193 144915
[691] 173234 104276 121481  83125 139368 132067 221036 152879 280776 122050
[701] 131116  94498 148726 207978 166909 143144 129784 193141 267840 207846
[711] 354776 218897 146991 200406 154195 360233 168085 145010 176933 141832
[721] 128215 201842 291473 367872 163031 295208 151985 210730 104433 201651
[731] 208128 159308 154486 198894 141974 238763 219962 139005 237769 128874
[741] 129629 186420 196295  87846 155821 172396 186441 141477  90476 148497
[751]  70233 152227 168132
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("SuperModel")
> ### * SuperModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SuperModel
> ### Title: Super Learner Model
> ### Aliases: SuperModel
> 
> ### ** Examples
> 
> model <- SuperModel(GBMModel, SVMRadialModel, GLMNetModel(lambda = 0.01))
> model_fit <- fit(sale_amount ~ ., data = ICHomes, model = model)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> predict(model_fit, newdata = ICHomes)
  [1] 102122 175209 210577 135352 256377 258359 165186 230394 188600 112257
 [11] 175209 153307 538723 538723 146876 216753 138192 483707 285838 120204
 [21] 208527 108890 137838 165186 139635 116246 175209 128076 148493 366384
 [31] 175209 114469 114469 171278 177438 177438  99142 338973 153307 138192
 [41] 159249 123376 102122 303708 148551 321832 203974 205591 525340 269185
 [51] 356818 133908 179055 287831 102122 150211 165186 137838 140677 175209
 [61] 135352 142121 120927 203974 102122 125628 311689 137420 285838 173764
 [71] 119503 161391  95321 261847 164453 159249 165065 175209 101230 216753
 [81] 196154 133771 176826 114469 188600 140486 159244 206298 285838 137838
 [91] 159249 129119 116246 205591 515333 175209 126585 184926 153307 131285
[101] 205591 190217 169267 104209 150821 175209 293450 142121 142121 102122
[111] 175209 135352 246225 494013 137838 477234 175209 134934 216753 162836
[121] 130651 128874  95321 146347 203974 138192 126585 152813 246225 177438
[131] 175209  95321 287831 179055 156763 146876 130651 154494 137838 135352
[141] 177438 121599 203974 116246 119150 156763 102122 293069 139635 216753
[151] 133137 504740 198031 187988 188623 161391 169267 114469 148833 156763
[161] 246225 142121 123030 261847 146876 179055 179055 146876  99142 176826
[171] 354252 205591 129842 102122 179055 438620 159249 261847 133908 154494
[181] 134934 159249 215136 198031 137838 313682 128114 128589 177438 119503
[191] 145433 142121 123376 159244 307791 128874 138192  95321 137838 175209
[201] 102122 261847 139635 154924 107813  95321 256377 140677 139635 175209
[211] 130562 145433 150327 131075 144268 246225 142176 141782 162836 105488
[221] 175209 327410 230394 216753  95321 165186 175209 108890 117561 176826
[231] 175209 205591 230394 175209 261847 285838 139635 102122 203974 135352
[241] 154494 120009 203974 119150 163348 126682 173764 176826 145433 146876
[251] 150327 159249 130562 135352 133944 120927 285405 102122 130562 225951
[261] 136394 538723 234033 162700 179055 134934  95321 142121 246225 142121
[271] 176826 175209 102122 259866 127285 135352 154494 199785 159249 400653
[281] 104538 175209  99142 117561 135352 216753 258359 102122 159249 142947
[291] 216753 103711 167822 153307 175209 136032 177438 127285 205591 246225
[301] 538723 124184 139635 117996 216753  95321 142121 232375 173764 205591
[311] 137838 205591 126670 127965 152813 129842 347580 391290 283856 128114
[321]  95321 190217 175209 125628 205591 164793 173764 146876 159249 127285
[331] 139635 449042 290512 146876 181435 140677 240743 123218 367562 110792
[341] 169267  95321 146876 154494 175209 104209 175209 216753 139635 175209
[351] 135352 162836 221213 125628 123376 232375 205591 172723 261847 110792
[361] 162836 102122 159249 163348 216753 137838 134589 142121 216753 278846
[371] 139635 177438 293450 109277 175209 102122 179055 146876 205591 132058
[381] 190217 177438 139635 230394 179055 133908 130651 538723 171495 188600
[391] 175209 135352 130651 159249 270215 126585 135352 172723 215136 187988
[401] 190217 216753 142121 369119 287831 134054 273638 129842 146876 139635
[411] 265023 133908 159244 246225 173764 175209 203974 138192 126633 104209
[421] 133157 351204 129771 187988 138192 141782 142121 205591 305068 153307
[431] 131285 124184 324409 215136 140302 259866 102122 224245 208527 348104
[441] 133908 175209 175209 102122 139795 146876 198031 271656 285838 261847
[451] 177438 261847 175209 146876 125271 150821 162700 156763 179055 538723
[461] 175209 131285 135352 175209 107576 138192 203974 147050 213972 130562
[471] 146876 282969 116246 128076 166803 179055 177438  95321 144390 139635
[481] 246225  97409 120927 205591 102122 216753 124184 139795 170884 371471
[491] 175209 138192 152813 261847 146876 205591 133771 295876 102122 123376
[501] 159249 130651 187006 176826 165065 103711 102122 144390 130562 142121
[511] 175209 287831 129842 144390 121240 159249 120927 338113 153307 146876
[521] 230394 261847 176826 175209 105488 138192 150821 142121 486825 146144
[531] 205591 216753 144390 104209 124184 167022 121981 205591 120204 165186
[541] 176826 146066 205591 133137 146876 150211 139635  95321 104209 292516
[551] 159244 175209 121989 137838 142947 162836 216753 287261 287831 133908
[561] 102122 185070 124184 211844 138192 205591 154924 429072 538723 230394
[571] 177438 216753 216753 130651  95321 146876 114469 159249  95321 346281
[581] 232375 177438 126633 142947 146876 150821 152813 165186 292907 232375
[591] 403221 140677 175209 112257 216753 279084 156763 165186 287831 258993
[601] 119150 190217 133908 261847 146876 128874 272208 216753 258359 138192
[611] 175209 175209 175209 104209 287831 142121 172723 173764 125238 177438
[621] 203974 216753 146144 126264 154494 142121 104209 175209 137838 221361
[631] 134934 162700 188623 175209 203974 159244 131285 216753 102122 187006
[641] 146876 154494 144078 107576 144390 216753 107576 153307 146876 124807
[651] 102122 130651 131568 128076 102122 138192 185070 142947 216753 102122
[661] 177438 454778 138192 216753 187006 175209 130306 102122 131285 216753
[671] 133771 287831 175209 102122 175209 256377 137838 144390 175209 102122
[681]  95321 136394 165065  95321 122898 169267 258359 115366 258359 144390
[691] 159244 102122 124606  99142 129119 123413 219232 141782 279216 120927
[701] 129842 102122 135352 208908 169267 129119 130651 177438 272208 188600
[711] 338128 230758 144390 179055 148833 389831 175209 128076 175209 139635
[721] 139690 203974 296067 460030 152008 309263 137838 216753 111516 186371
[731] 196232 148551 148833 186687 142121 216753 230758 142947 199785 124184
[741] 126633 175209 177438 102122 146876 159249 203974 145433 102122 152813
[751]  95321 137838 167822
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("SurvRegModel")
> ### * SurvRegModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SurvRegModel
> ### Title: Parametric Survival Model
> ### Aliases: SurvRegModel SurvRegStepAICModel
> 
> ### ** Examples
> 
> library(survival)
> 
> fit(Surv(time, status) ~ ., data = veteran, model = SurvRegModel)
Parametric Survival Model: Weibull Distribution
 
 rms::psm(formula = formula, data = as.data.frame(data), weights = weights, 
     dist = "weibull", control = ..2)
 
                        Model Likelihood    Discrimination    
                              Ratio Test           Indexes    
      Obs      137    LR chi2      65.08    R2       0.378    
      Events   128    d.f.             8    Dxy      0.473    
 Sum of Weights137    Pr(> chi2) <0.0001    g        0.899    
      sigma 0.9281                          gr       2.457    
 
                    Coef    S.E.   Wald Z Pr(>|Z|)
 (Intercept)         3.4905 0.6912  5.05  <0.0001 
 trt                -0.2285 0.1868 -1.22  0.2213  
 celltype=smallcell -0.8262 0.2463 -3.35  0.0008  
 celltype=adeno     -1.1327 0.2576 -4.40  <0.0001 
 celltype=large     -0.3977 0.2547 -1.56  0.1185  
 karno               0.0301 0.0048  6.23  <0.0001 
 diagtime           -0.0005 0.0084 -0.06  0.9553  
 age                 0.0061 0.0086  0.71  0.4758  
 prior              -0.0044 0.0212 -0.21  0.8362  
 Log(scale)         -0.0746 0.0663 -1.12  0.2606  
 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("TreeModel")
> ### * TreeModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: TreeModel
> ### Title: Classification and Regression Tree Models
> ### Aliases: TreeModel
> 
> ### ** Examples
> 
> fit(Species ~ ., data = iris, model = TreeModel)
node), split, n, deviance, yval, (yprob)
      * denotes terminal node

 1) root 150 329.600 setosa ( 0.33333 0.33333 0.33333 )  
   2) Petal.Length < 2.45 50   0.000 setosa ( 1.00000 0.00000 0.00000 ) *
   3) Petal.Length > 2.45 100 138.600 versicolor ( 0.00000 0.50000 0.50000 )  
     6) Petal.Width < 1.75 54  33.320 versicolor ( 0.00000 0.90741 0.09259 )  
      12) Petal.Length < 4.95 48   9.721 versicolor ( 0.00000 0.97917 0.02083 )  
        24) Sepal.Length < 5.15 5   5.004 versicolor ( 0.00000 0.80000 0.20000 ) *
        25) Sepal.Length > 5.15 43   0.000 versicolor ( 0.00000 1.00000 0.00000 ) *
      13) Petal.Length > 4.95 6   7.638 virginica ( 0.00000 0.33333 0.66667 ) *
     7) Petal.Width > 1.75 46   9.635 virginica ( 0.00000 0.02174 0.97826 )  
      14) Petal.Length < 4.95 6   5.407 virginica ( 0.00000 0.16667 0.83333 ) *
      15) Petal.Length > 4.95 40   0.000 virginica ( 0.00000 0.00000 1.00000 ) *
> 
> 
> 
> 
> cleanEx()
> nameEx("TunedInput")
> ### * TunedInput
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: TunedInput
> ### Title: Tuned Model Inputs
> ### Aliases: TunedInput TunedModelRecipe TunedInput.recipe
> 
> ### ** Examples
> 
> library(recipes)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> library(MASS)

Attaching package: ‘MASS’

The following object is masked from ‘package:dplyr’:

    select

> 
> rec <- recipe(medv ~ ., data = Boston) %>%
+   step_pca(all_numeric(), -all_outcomes(), id = "pca")
> 
> grid <- expand_steps(
+   pca = list(num_comp = 1:2)
+ )
> 
> fit(TunedInput(rec, grid = grid), model = GLMModel)

Call:  stats::glm(formula = formula, family = family, data = data, weights = weights, 
    control = control)

Coefficients:
(Intercept)          PC1          PC2  
   25.96039      0.00726     -0.02589  

Degrees of Freedom: 505 Total (i.e. Null);  503 Residual
Null Deviance:	    42720 
Residual Deviance: 32350 	AIC: 3548
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’, ‘package:MASS’, ‘package:recipes’,
  ‘package:dplyr’

> nameEx("TunedModel")
> ### * TunedModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: TunedModel
> ### Title: Tuned Model
> ### Aliases: TunedModel
> 
> ### ** Examples
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("XGBModel")
> ### * XGBModel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: XGBModel
> ### Title: Extreme Gradient Boosting Models
> ### Aliases: XGBModel XGBDARTModel XGBLinearModel XGBTreeModel
> 
> ### ** Examples
> 
> model_fit <- fit(Species ~ ., data = iris, model = XGBTreeModel)
[11:24:49] WARNING: amalgamation/../src/learner.cc:480: 
Parameters: { scale_pos_weight, update } might not be used.

  This may not be accurate due to some parameters are only used in language bindings but
  passed down to XGBoost core.  Or some parameters are not used but slip through this
  verification. Please open an issue if you find above cases.


> varimp(model_fit, metric = "Frequency", scale = FALSE)
Object of class "VarImp"
             Frequency
Petal.Length 0.5555556
Petal.Width  0.4444444
> 
> 
> 
> 
> cleanEx()
> nameEx("calibration")
> ### * calibration
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: calibration
> ### Title: Model Calibration
> ### Aliases: calibration
> 
> ### ** Examples
> 
> library(survival)
> 
> res <- resample(Surv(time, status) ~ ., data = veteran, model = GBMModel,
+                 control = CVControl(times = c(90, 180, 360)))
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> cal <- calibration(res)
> plot(cal)
$GBMModel

> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("confusion")
> ### * confusion
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confusion
> ### Title: Confusion Matrix
> ### Aliases: confusion ConfusionMatrix
> 
> ### ** Examples
> 
> res <- resample(Species ~ ., data = iris, model = GBMModel)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> (conf <- confusion(res))
Object of class "ConfusionList"

GBMModel :
Object of class "ConfusionMatrix"
            Observed
Predicted    setosa versicolor virginica
  setosa         50          0         0
  versicolor      0         46         4
  virginica       0          4        46

> plot(conf)
$GBMModel

> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("dependence")
> ### * dependence
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dependence
> ### Title: Partial Dependence
> ### Aliases: dependence
> 
> ### ** Examples
> 
> gbm_fit <- fit(Species ~ ., data = iris, model = GBMModel)
> (pd <- dependence(gbm_fit, select = c(Petal.Length, Petal.Width)))
   Statistic   Response        Value Predictors.Petal.Length
1       Mean     setosa 9.998897e-01                     1.0
2       Mean versicolor 8.461250e-05                     1.0
3       Mean  virginica 2.572645e-05                     1.0
4       Mean     setosa 9.998897e-01                     1.7
5       Mean versicolor 8.461250e-05                     1.7
6       Mean  virginica 2.572645e-05                     1.7
7       Mean     setosa 9.998443e-01                     1.9
8       Mean versicolor 1.194192e-04                     1.9
9       Mean  virginica 3.630981e-05                     1.9
10      Mean     setosa 1.196315e-05                     3.0
11      Mean versicolor 8.422882e-01                     3.0
12      Mean  virginica 1.576999e-01                     3.0
13      Mean     setosa 1.196315e-05                     3.6
14      Mean versicolor 8.422882e-01                     3.6
15      Mean  virginica 1.576999e-01                     3.6
16      Mean     setosa 1.196315e-05                     4.3
17      Mean versicolor 8.422882e-01                     4.3
18      Mean  virginica 1.576999e-01                     4.3
19      Mean     setosa 2.111405e-05                     4.9
20      Mean versicolor 6.780243e-01                     4.9
21      Mean  virginica 3.219545e-01                     4.9
22      Mean     setosa 3.329106e-05                     5.6
23      Mean versicolor 2.620089e-01                     5.6
24      Mean  virginica 7.379578e-01                     5.6
25      Mean     setosa 3.099966e-05                     6.3
26      Mean versicolor 1.753858e-01                     6.3
27      Mean  virginica 8.245832e-01                     6.3
28      Mean     setosa 3.099966e-05                     6.9
29      Mean versicolor 1.753858e-01                     6.9
30      Mean  virginica 8.245832e-01                     6.9
31      Mean     setosa 3.333313e-01                      NA
32      Mean versicolor 5.135732e-01                      NA
33      Mean  virginica 1.530955e-01                      NA
34      Mean     setosa 3.333313e-01                      NA
35      Mean versicolor 5.135732e-01                      NA
36      Mean  virginica 1.530955e-01                      NA
37      Mean     setosa 3.333313e-01                      NA
38      Mean versicolor 5.135732e-01                      NA
39      Mean  virginica 1.530955e-01                      NA
40      Mean     setosa 3.332960e-01                      NA
41      Mean versicolor 5.136052e-01                      NA
42      Mean  virginica 1.530987e-01                      NA
43      Mean     setosa 3.332960e-01                      NA
44      Mean versicolor 5.136052e-01                      NA
45      Mean  virginica 1.530987e-01                      NA
46      Mean     setosa 3.333052e-01                      NA
47      Mean versicolor 4.860892e-01                      NA
48      Mean  virginica 1.806056e-01                      NA
49      Mean     setosa 3.333249e-01                      NA
50      Mean versicolor 3.323524e-01                      NA
51      Mean  virginica 3.343227e-01                      NA
52      Mean     setosa 3.333197e-01                      NA
53      Mean versicolor 1.227964e-01                      NA
54      Mean  virginica 5.438838e-01                      NA
55      Mean     setosa 3.333197e-01                      NA
56      Mean versicolor 1.227964e-01                      NA
57      Mean  virginica 5.438838e-01                      NA
58      Mean     setosa 3.333197e-01                      NA
59      Mean versicolor 1.227964e-01                      NA
60      Mean  virginica 5.438838e-01                      NA
   Predictors.Petal.Width
1                      NA
2                      NA
3                      NA
4                      NA
5                      NA
6                      NA
7                      NA
8                      NA
9                      NA
10                     NA
11                     NA
12                     NA
13                     NA
14                     NA
15                     NA
16                     NA
17                     NA
18                     NA
19                     NA
20                     NA
21                     NA
22                     NA
23                     NA
24                     NA
25                     NA
26                     NA
27                     NA
28                     NA
29                     NA
30                     NA
31                    0.1
32                    0.1
33                    0.1
34                    0.4
35                    0.4
36                    0.4
37                    0.6
38                    0.6
39                    0.6
40                    1.0
41                    1.0
42                    1.0
43                    1.2
44                    1.2
45                    1.2
46                    1.4
47                    1.4
48                    1.4
49                    1.7
50                    1.7
51                    1.7
52                    2.0
53                    2.0
54                    2.0
55                    2.2
56                    2.2
57                    2.2
58                    2.5
59                    2.5
60                    2.5
> plot(pd)
$Petal.Length

$Petal.Width

> 
> 
> 
> 
> cleanEx()
> nameEx("diff-methods")
> ### * diff-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: diff
> ### Title: Model Performance Differences
> ### Aliases: diff diff.MLModel diff.Performance diff.Resamples
> 
> ### ** Examples
> 
> ## Survival response example
> library(survival)
> 
> fo <- Surv(time, status) ~ .
> control <- CVControl()
> 
> gbm_res1 <- resample(fo, data = veteran, GBMModel(n.trees = 25), control)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> gbm_res2 <- resample(fo, data = veteran, GBMModel(n.trees = 50), control)
> gbm_res3 <- resample(fo, data = veteran, GBMModel(n.trees = 100), control)
> 
> res <- c(GBM1 = gbm_res1, GBM2 = gbm_res2, GBM3 = gbm_res3)
> res_diff <- diff(res)
> summary(res_diff)
, , Metric = C-Index

             Statistic
Model                Mean      Median         SD         Min        Max NA
  GBM1 - GBM2 0.005189589 0.009199134 0.03473199 -0.03977273 0.06410256  0
  GBM1 - GBM3 0.007467704 0.021749084 0.04696282 -0.06250000 0.07894737  0
  GBM2 - GBM3 0.002278114 0.011642074 0.02963421 -0.04444444 0.03947368  0

> plot(res_diff)
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("dot-")
> ### * dot-
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: .
> ### Title: Quote Operator
> ### Aliases: .
> 
> ### ** Examples
> 
> ## Stepwise variable selection with BIC
> glm_fit <- fit(sale_amount ~ ., ICHomes, GLMStepAICModel(k = .(log(nobs))))
> varimp(glm_fit)
Object of class "VarImp"
                Pr(>Chi)
base_size    100.0000000
construction  62.5123347
add_size      27.8455146
garage1_size   5.5077615
attic          1.7682145
garage2_size   0.7852837
lot_size       0.1026169
basement       0.0000000
> 
> 
> 
> 
> cleanEx()
> nameEx("expand_model")
> ### * expand_model
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expand_model
> ### Title: Model Expansion Over Tuning Parameters
> ### Aliases: expand_model
> 
> ### ** Examples
> 
> library(MASS)
> 
> models <- expand_model(GBMModel, n.trees = c(50, 100),
+                                  interaction.depth = 1:2)
> 
> fit(medv ~ ., data = Boston, model = SelectedModel(models))
Loading required package: dplyr

Attaching package: ‘dplyr’

The following object is masked from ‘package:MASS’:

    select

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

gbm::gbm(formula = formula, distribution = distribution, data = as.data.frame(data), 
    weights = weights, n.trees = 100, interaction.depth = 2L, 
    n.minobsinnode = 10, shrinkage = 0.1, bag.fraction = 0.5)
A gradient boosted model with gaussian loss function.
100 iterations were performed.
There were 13 predictors of which 13 had non-zero influence.
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’,
  ‘package:MASS’

> nameEx("expand_params")
> ### * expand_params
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expand_params
> ### Title: Model Parameters Expansion
> ### Aliases: expand_params
> 
> ### ** Examples
> 
> library(MASS)
> 
> grid <- expand_params(
+   n.trees = c(50, 100),
+   interaction.depth = 1:2
+ )
> 
> fit(medv ~ ., data = Boston, model = TunedModel(GBMModel, grid = grid))
Loading required package: dplyr

Attaching package: ‘dplyr’

The following object is masked from ‘package:MASS’:

    select

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

gbm::gbm(formula = formula, distribution = distribution, data = as.data.frame(data), 
    weights = weights, n.trees = 100, interaction.depth = 2L, 
    n.minobsinnode = 10, shrinkage = 0.1, bag.fraction = 0.5)
A gradient boosted model with gaussian loss function.
100 iterations were performed.
There were 13 predictors of which 13 had non-zero influence.
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’,
  ‘package:MASS’

> nameEx("expand_steps")
> ### * expand_steps
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: expand_steps
> ### Title: Recipe Step Parameters Expansion
> ### Aliases: expand_steps
> 
> ### ** Examples
> 
> library(recipes)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> library(MASS)

Attaching package: ‘MASS’

The following object is masked from ‘package:dplyr’:

    select

> 
> rec <- recipe(medv ~ ., data = Boston) %>%
+   step_corr(all_numeric(), -all_outcomes(), id = "corr") %>%
+   step_pca(all_numeric(), -all_outcomes(), id = "pca")
> 
> expand_steps(
+   corr = list(threshold = c(0.8, 0.9),
+               method = c("pearson", "spearman")),
+   pca = list(num_comp = 1:3)
+ )
Object of class "RecipeGrid"
[90m# A tibble: 12 x 2[39m
   corr$threshold $method  pca$num_comp
 [90m*[39m          [3m[90m<dbl>[39m[23m [3m[90m<chr>[39m[23m           [3m[90m<int>[39m[23m
[90m 1[39m            0.8 pearson             1
[90m 2[39m            0.9 pearson             1
[90m 3[39m            0.8 spearman            1
[90m 4[39m            0.9 spearman            1
[90m 5[39m            0.8 pearson             2
[90m 6[39m            0.9 pearson             2
[90m 7[39m            0.8 spearman            2
[90m 8[39m            0.9 spearman            2
[90m 9[39m            0.8 pearson             3
[90m10[39m            0.9 pearson             3
[90m# … with 2 more rows[39m
> 
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’, ‘package:recipes’, ‘package:dplyr’

> nameEx("fit-methods")
> ### * fit-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fit
> ### Title: Model Fitting
> ### Aliases: fit fit.formula fit.matrix fit.ModelFrame fit.recipe
> ###   fit.MLModel fit.MLModelFunction
> 
> ### ** Examples
> 
> ## Survival response example
> library(survival)
> 
> gbm_fit <- fit(Surv(time, status) ~ ., data = veteran, model = GBMModel)
> varimp(gbm_fit)
Object of class "VarImp"
           Overall
age      100.00000
karno     93.83406
celltype  69.41357
diagtime  31.39921
trt       13.16085
prior      0.00000
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("lift")
> ### * lift
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lift
> ### Title: Model Lift Curves
> ### Aliases: lift
> 
> ### ** Examples
> 
> library(MASS)
> 
> res <- resample(type ~ ., data = Pima.tr, model = GBMModel)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following object is masked from ‘package:MASS’:

    select

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> lf <- lift(res)
> plot(lf)
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’,
  ‘package:MASS’

> nameEx("metricinfo")
> ### * metricinfo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: metricinfo
> ### Title: Display Performance Metric Information
> ### Aliases: metricinfo
> 
> ### ** Examples
> 
> ## All metrics
> metricinfo()
$accuracy
$accuracy$label
[1] "Accuracy"

$accuracy$maximize
[1] TRUE

$accuracy$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$accuracy$response_types
         observed  predicted
1 ConfusionMatrix       NULL
2       Resamples       NULL
3            Surv SurvEvents
4            Surv  SurvProbs
5          factor     factor
6          factor     matrix
7          factor    numeric


$auc
$auc$label
[1] "Area Under Performance Curve"

$auc$maximize
[1] TRUE

$auc$arguments
function (observed, predicted = NULL, metrics = c(MachineShop::tpr, 
    MachineShop::fpr), stat = MachineShop::settings("stat.Curve"), 
    ...) 
NULL

$auc$response_types
          observed predicted
1 PerformanceCurve      NULL
2        Resamples      NULL
3             Surv SurvProbs
4           factor   numeric


$brier
$brier$label
[1] "Brier Score"

$brier$maximize
[1] FALSE

$brier$arguments
function (observed, predicted = NULL, ...) 
NULL

$brier$response_types
   observed predicted
1 Resamples      NULL
2      Surv SurvProbs
3    factor    matrix
4    factor   numeric


$cindex
$cindex$label
[1] "Concordance Index"

$cindex$maximize
[1] TRUE

$cindex$arguments
function (observed, predicted = NULL, ...) 
NULL

$cindex$response_types
   observed predicted
1 Resamples      NULL
2      Surv   numeric
3    factor   numeric


$cross_entropy
$cross_entropy$label
[1] "Cross Entropy"

$cross_entropy$maximize
[1] FALSE

$cross_entropy$arguments
function (observed, predicted = NULL, ...) 
NULL

$cross_entropy$response_types
   observed predicted
1 Resamples      NULL
2    factor    matrix
3    factor   numeric


$f_score
$f_score$label
[1] "F Score"

$f_score$maximize
[1] TRUE

$f_score$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    beta = 1, ...) 
NULL

$f_score$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$fnr
$fnr$label
[1] "False Negative Rate"

$fnr$maximize
[1] FALSE

$fnr$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$fnr$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$fpr
$fpr$label
[1] "False Positive Rate"

$fpr$maximize
[1] FALSE

$fpr$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$fpr$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$gini
$gini$label
[1] "Gini Coefficient"

$gini$maximize
[1] FALSE

$gini$arguments
function (observed, predicted = NULL, ...) 
NULL

$gini$response_types
         observed predicted
1 BinomialVariate   numeric
2       Resamples      NULL
3            Surv   numeric
4          matrix    matrix
5         numeric   numeric


$kappa2
$kappa2$label
[1] "Cohen's Kappa"

$kappa2$maximize
[1] TRUE

$kappa2$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$kappa2$response_types
         observed  predicted
1 ConfusionMatrix       NULL
2       Resamples       NULL
3            Surv SurvEvents
4            Surv  SurvProbs
5          factor     factor
6          factor     matrix
7          factor    numeric


$mae
$mae$label
[1] "Mean Absolute Error"

$mae$maximize
[1] FALSE

$mae$arguments
function (observed, predicted = NULL, ...) 
NULL

$mae$response_types
         observed predicted
1 BinomialVariate   numeric
2       Resamples      NULL
3            Surv   numeric
4          matrix    matrix
5         numeric   numeric


$mse
$mse$label
[1] "Mean Squared Error"

$mse$maximize
[1] FALSE

$mse$arguments
function (observed, predicted = NULL, ...) 
NULL

$mse$response_types
         observed predicted
1 BinomialVariate   numeric
2       Resamples      NULL
3            Surv   numeric
4          matrix    matrix
5         numeric   numeric


$msle
$msle$label
[1] "Mean Squared Log Error"

$msle$maximize
[1] FALSE

$msle$arguments
function (observed, predicted = NULL, ...) 
NULL

$msle$response_types
         observed predicted
1 BinomialVariate   numeric
2       Resamples      NULL
3            Surv   numeric
4          matrix    matrix
5         numeric   numeric


$npv
$npv$label
[1] "Negative Predictive Value"

$npv$maximize
[1] TRUE

$npv$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$npv$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$ppv
$ppv$label
[1] "Positive Predictive Value"

$ppv$maximize
[1] TRUE

$ppv$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$ppv$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$pr_auc
$pr_auc$label
[1] "Area Under Precision-Recall Curve"

$pr_auc$maximize
[1] TRUE

$pr_auc$arguments
function (observed, predicted = NULL, ...) 
NULL

$pr_auc$response_types
   observed predicted
1 Resamples      NULL
2      Surv SurvProbs
3    factor   numeric


$precision
$precision$label
[1] "Precision"

$precision$maximize
[1] TRUE

$precision$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$precision$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$r2
$r2$label
[1] "Coefficient of Determination"

$r2$maximize
[1] TRUE

$r2$arguments
function (observed, predicted = NULL, dist = NULL, ...) 
NULL

$r2$response_types
         observed predicted
1 BinomialVariate   numeric
2       Resamples      NULL
3            Surv   numeric
4          matrix    matrix
5         numeric   numeric


$recall
$recall$label
[1] "Recall"

$recall$maximize
[1] TRUE

$recall$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$recall$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$rmse
$rmse$label
[1] "Root Mean Squared Error"

$rmse$maximize
[1] FALSE

$rmse$arguments
function (observed, predicted = NULL, ...) 
NULL

$rmse$response_types
         observed predicted
1 BinomialVariate   numeric
2       Resamples      NULL
3            Surv   numeric
4          matrix    matrix
5         numeric   numeric


$rmsle
$rmsle$label
[1] "Root Mean Squared Log Error"

$rmsle$maximize
[1] FALSE

$rmsle$arguments
function (observed, predicted = NULL, ...) 
NULL

$rmsle$response_types
         observed predicted
1 BinomialVariate   numeric
2       Resamples      NULL
3            Surv   numeric
4          matrix    matrix
5         numeric   numeric


$roc_auc
$roc_auc$label
[1] "Area Under ROC Curve"

$roc_auc$maximize
[1] TRUE

$roc_auc$arguments
function (observed, predicted = NULL, ...) 
NULL

$roc_auc$response_types
   observed predicted
1 Resamples      NULL
2      Surv SurvProbs
3    factor   numeric


$roc_index
$roc_index$label
[1] "ROC Index"

$roc_index$maximize
[1] TRUE

$roc_index$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    f = function(sensitivity, specificity) (sensitivity + specificity)/2, 
    ...) 
NULL

$roc_index$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$rpp
$rpp$label
[1] "Rate of Positive Prediction"

$rpp$maximize
[1] FALSE

$rpp$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$rpp$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$sensitivity
$sensitivity$label
[1] "Sensitivity"

$sensitivity$maximize
[1] TRUE

$sensitivity$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$sensitivity$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$specificity
$specificity$label
[1] "Specificity"

$specificity$maximize
[1] TRUE

$specificity$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$specificity$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$tnr
$tnr$label
[1] "True Negative Rate"

$tnr$maximize
[1] TRUE

$tnr$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$tnr$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$tpr
$tpr$label
[1] "True Positive Rate"

$tpr$maximize
[1] TRUE

$tpr$arguments
function (observed, predicted = NULL, cutoff = MachineShop::settings("cutoff"), 
    ...) 
NULL

$tpr$response_types
               observed  predicted
1 BinaryConfusionMatrix       NULL
2             Resamples       NULL
3                  Surv SurvEvents
4                  Surv  SurvProbs
5                factor     factor
6                factor    numeric


$weighted_kappa2
$weighted_kappa2$label
[1] "Weighted Cohen's Kappa"

$weighted_kappa2$maximize
[1] TRUE

$weighted_kappa2$arguments
function (observed, predicted = NULL, power = 1, ...) 
NULL

$weighted_kappa2$response_types
                observed predicted
1 OrderedConfusionMatrix      NULL
2              Resamples      NULL
3                ordered    matrix
4                ordered   ordered


> 
> ## Metrics by observed and predicted response types
> names(metricinfo(factor(0)))
 [1] "accuracy"      "auc"           "brier"         "cindex"       
 [5] "cross_entropy" "f_score"       "fnr"           "fpr"          
 [9] "kappa2"        "npv"           "ppv"           "pr_auc"       
[13] "precision"     "recall"        "roc_auc"       "roc_index"    
[17] "rpp"           "sensitivity"   "specificity"   "tnr"          
[21] "tpr"          
> names(metricinfo(factor(0), factor(0)))
[1] "accuracy" "kappa2"  
> names(metricinfo(factor(0), matrix(0)))
[1] "accuracy"      "brier"         "cross_entropy" "kappa2"       
> names(metricinfo(factor(0), numeric(0)))
 [1] "accuracy"      "auc"           "brier"         "cindex"       
 [5] "cross_entropy" "f_score"       "fnr"           "fpr"          
 [9] "kappa2"        "npv"           "ppv"           "pr_auc"       
[13] "precision"     "recall"        "roc_auc"       "roc_index"    
[17] "rpp"           "sensitivity"   "specificity"   "tnr"          
[21] "tpr"          
> 
> ## Metric-specific information
> metricinfo(auc)
$auc
$auc$label
[1] "Area Under Performance Curve"

$auc$maximize
[1] TRUE

$auc$arguments
function (observed, predicted = NULL, metrics = c(MachineShop::tpr, 
    MachineShop::fpr), stat = MachineShop::settings("stat.Curve"), 
    ...) 
NULL

$auc$response_types
          observed predicted
1 PerformanceCurve      NULL
2        Resamples      NULL
3             Surv SurvProbs
4           factor   numeric


> 
> 
> 
> 
> cleanEx()
> nameEx("modelinfo")
> ### * modelinfo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: modelinfo
> ### Title: Display Model Information
> ### Aliases: modelinfo
> 
> ### ** Examples
> 
> ## All models
> modelinfo()
$AdaBagModel
$AdaBagModel$label
[1] "Bagging with Classification Trees"

$AdaBagModel$packages
[1] "adabag"

$AdaBagModel$response_types
[1] "factor"

$AdaBagModel$arguments
function (mfinal = 100, minsplit = 20, minbucket = round(minsplit/3), 
    cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, 
    xval = 10, surrogatestyle = 0, maxdepth = 30) 
NULL

$AdaBagModel$grid
[1] TRUE

$AdaBagModel$varimp
[1] TRUE


$AdaBoostModel
$AdaBoostModel$label
[1] "Boosting with Classification Trees"

$AdaBoostModel$packages
[1] "adabag"

$AdaBoostModel$response_types
[1] "factor"

$AdaBoostModel$arguments
function (boos = TRUE, mfinal = 100, coeflearn = c("Breiman", 
    "Freund", "Zhu"), minsplit = 20, minbucket = round(minsplit/3), 
    cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, 
    xval = 10, surrogatestyle = 0, maxdepth = 30) 
NULL

$AdaBoostModel$grid
[1] TRUE

$AdaBoostModel$varimp
[1] TRUE


$BARTModel
$BARTModel$label
[1] "Bayesian Additive Regression Trees"

$BARTModel$packages
[1] "BART"

$BARTModel$response_types
[1] "factor"  "numeric" "Surv"   

$BARTModel$arguments
function (K = NULL, sparse = FALSE, theta = 0, omega = 1, a = 0.5, 
    b = 1, rho = NULL, augment = FALSE, xinfo = NULL, usequants = FALSE, 
    sigest = NA, sigdf = 3, sigquant = 0.9, lambda = NA, k = 2, 
    power = 2, base = 0.95, tau.num = NULL, offset = NULL, ntree = NULL, 
    numcut = 100, ndpost = 1000, nskip = NULL, keepevery = NULL, 
    printevery = 1000) 
NULL

$BARTModel$grid
[1] FALSE

$BARTModel$varimp
[1] FALSE


$BARTMachineModel
$BARTMachineModel$label
[1] "Bayesian Additive Regression Trees"

$BARTMachineModel$packages
[1] "bartMachine"

$BARTMachineModel$response_types
[1] "binary"  "numeric"

$BARTMachineModel$arguments
function (num_trees = 50, num_burn = 250, num_iter = 1000, alpha = 0.95, 
    beta = 2, k = 2, q = 0.9, nu = 3, mh_prob_steps = c(2.5, 
        2.5, 4)/9, verbose = FALSE, ...) 
NULL

$BARTMachineModel$grid
[1] TRUE

$BARTMachineModel$varimp
[1] TRUE


$BlackBoostModel
$BlackBoostModel$label
[1] "Gradient Boosting with Regression Trees"

$BlackBoostModel$packages
[1] "mboost"   "partykit"

$BlackBoostModel$response_types
[1] "binary"             "BinomialVariate"    "NegBinomialVariate"
[4] "numeric"            "PoissonVariate"     "Surv"              

$BlackBoostModel$arguments
function (family = NULL, mstop = 100, nu = 0.1, risk = c("inbag", 
    "oobag", "none"), stopintern = FALSE, trace = FALSE, teststat = c("quadratic", 
    "maximum"), testtype = c("Teststatistic", "Univariate", "Bonferroni", 
    "MonteCarlo"), mincriterion = 0, minsplit = 10, minbucket = 4, 
    maxdepth = 2, saveinfo = FALSE, ...) 
NULL

$BlackBoostModel$grid
[1] TRUE

$BlackBoostModel$varimp
[1] TRUE


$C50Model
$C50Model$label
[1] "C5.0 Classification"

$C50Model$packages
[1] "C50"

$C50Model$response_types
[1] "factor"

$C50Model$arguments
function (trials = 1, rules = FALSE, subset = TRUE, bands = 0, 
    winnow = FALSE, noGlobalPruning = FALSE, CF = 0.25, minCases = 2, 
    fuzzyThreshold = FALSE, sample = 0, earlyStopping = TRUE) 
NULL

$C50Model$grid
[1] TRUE

$C50Model$varimp
[1] TRUE


$CForestModel
$CForestModel$label
[1] "Conditional Random Forests"

$CForestModel$packages
[1] "party"

$CForestModel$response_types
[1] "factor"  "numeric" "Surv"   

$CForestModel$arguments
function (teststat = c("quad", "max"), testtype = c("Univariate", 
    "Teststatistic", "Bonferroni", "MonteCarlo"), mincriterion = 0, 
    ntree = 500, mtry = 5, replace = TRUE, fraction = 0.632) 
NULL

$CForestModel$grid
[1] TRUE

$CForestModel$varimp
[1] TRUE


$CoxModel
$CoxModel$label
[1] "Cox Regression"

$CoxModel$packages
[1] "survival"

$CoxModel$response_types
[1] "Surv"

$CoxModel$arguments
function (ties = c("efron", "breslow", "exact"), ...) 
NULL

$CoxModel$grid
[1] FALSE

$CoxModel$varimp
[1] TRUE


$CoxStepAICModel
$CoxStepAICModel$label
[1] "Cox Regression (Stepwise)"

$CoxStepAICModel$packages
[1] "survival" "MASS"    

$CoxStepAICModel$response_types
[1] "Surv"

$CoxStepAICModel$arguments
function (ties = c("efron", "breslow", "exact"), ..., direction = c("both", 
    "backward", "forward"), scope = NULL, k = 2, trace = FALSE, 
    steps = 1000) 
NULL

$CoxStepAICModel$grid
[1] FALSE

$CoxStepAICModel$varimp
[1] TRUE


$EarthModel
$EarthModel$label
[1] "Multivariate Adaptive Regression Splines"

$EarthModel$packages
[1] "earth"

$EarthModel$response_types
[1] "factor"  "numeric"

$EarthModel$arguments
function (pmethod = c("backward", "none", "exhaustive", "forward", 
    "seqrep", "cv"), trace = 0, degree = 1, nprune = NULL, nfold = 0, 
    ncross = 1, stratify = TRUE) 
NULL

$EarthModel$grid
[1] TRUE

$EarthModel$varimp
[1] TRUE


$FDAModel
$FDAModel$label
[1] "Flexible Discriminant Analysis"

$FDAModel$packages
[1] "mda"

$FDAModel$response_types
[1] "factor"

$FDAModel$arguments
function (theta = NULL, dimension = NULL, eps = .Machine$double.eps, 
    method = .(mda::polyreg), ...) 
NULL

$FDAModel$grid
[1] TRUE

$FDAModel$varimp
[1] FALSE


$GAMBoostModel
$GAMBoostModel$label
[1] "Gradient Boosting with Additive Models"

$GAMBoostModel$packages
[1] "mboost"

$GAMBoostModel$response_types
[1] "binary"             "BinomialVariate"    "NegBinomialVariate"
[4] "numeric"            "PoissonVariate"     "Surv"              

$GAMBoostModel$arguments
function (family = NULL, baselearner = c("bbs", "bols", "btree", 
    "bss", "bns"), dfbase = 4, mstop = 100, nu = 0.1, risk = c("inbag", 
    "oobag", "none"), stopintern = FALSE, trace = FALSE) 
NULL

$GAMBoostModel$grid
[1] TRUE

$GAMBoostModel$varimp
[1] TRUE


$GBMModel
$GBMModel$label
[1] "Generalized Boosted Regression"

$GBMModel$packages
[1] "gbm"

$GBMModel$response_types
[1] "factor"         "numeric"        "PoissonVariate" "Surv"          

$GBMModel$arguments
function (distribution = NULL, n.trees = 100, interaction.depth = 1, 
    n.minobsinnode = 10, shrinkage = 0.1, bag.fraction = 0.5) 
NULL

$GBMModel$grid
[1] TRUE

$GBMModel$varimp
[1] TRUE


$GLMBoostModel
$GLMBoostModel$label
[1] "Gradient Boosting with Linear Models"

$GLMBoostModel$packages
[1] "mboost"

$GLMBoostModel$response_types
[1] "binary"             "BinomialVariate"    "NegBinomialVariate"
[4] "numeric"            "PoissonVariate"     "Surv"              

$GLMBoostModel$arguments
function (family = NULL, mstop = 100, nu = 0.1, risk = c("inbag", 
    "oobag", "none"), stopintern = FALSE, trace = FALSE) 
NULL

$GLMBoostModel$grid
[1] TRUE

$GLMBoostModel$varimp
[1] TRUE


$GLMModel
$GLMModel$label
[1] "Generalized Linear Models"

$GLMModel$packages
[1] "MASS"  "nnet"  "stats"

$GLMModel$response_types
[1] "BinomialVariate"    "factor"             "matrix"            
[4] "NegBinomialVariate" "numeric"            "PoissonVariate"    

$GLMModel$arguments
function (family = NULL, quasi = FALSE, ...) 
NULL

$GLMModel$grid
[1] FALSE

$GLMModel$varimp
[1] TRUE


$GLMStepAICModel
$GLMStepAICModel$label
[1] "Generalized Linear Models (Stepwise)"

$GLMStepAICModel$packages
[1] "MASS"  "nnet"  "stats"

$GLMStepAICModel$response_types
[1] "binary"             "BinomialVariate"    "NegBinomialVariate"
[4] "numeric"            "PoissonVariate"    

$GLMStepAICModel$arguments
function (family = NULL, quasi = FALSE, ..., direction = c("both", 
    "backward", "forward"), scope = NULL, k = 2, trace = FALSE, 
    steps = 1000) 
NULL

$GLMStepAICModel$grid
[1] FALSE

$GLMStepAICModel$varimp
[1] TRUE


$GLMNetModel
$GLMNetModel$label
[1] "Lasso and Elastic-Net"

$GLMNetModel$packages
[1] "glmnet"

$GLMNetModel$response_types
[1] "BinomialVariate" "factor"          "matrix"          "numeric"        
[5] "PoissonVariate"  "Surv"           

$GLMNetModel$arguments
function (family = NULL, alpha = 1, lambda = 0, standardize = TRUE, 
    intercept = NULL, penalty.factor = .(rep(1, nvars)), standardize.response = FALSE, 
    thresh = 1e-07, maxit = 1e+05, type.gaussian = .(ifelse(nvars < 
        500, "covariance", "naive")), type.logistic = c("Newton", 
        "modified.Newton"), type.multinomial = c("ungrouped", 
        "grouped")) 
NULL

$GLMNetModel$grid
[1] TRUE

$GLMNetModel$varimp
[1] TRUE


$KNNModel
$KNNModel$label
[1] "K-Nearest Neighbors Model"

$KNNModel$packages
[1] "kknn"

$KNNModel$response_types
[1] "factor"  "numeric" "ordered"

$KNNModel$arguments
function (k = 7, distance = 2, scale = TRUE, kernel = c("optimal", 
    "biweight", "cos", "epanechnikov", "gaussian", "inv", "rank", 
    "rectangular", "triangular", "triweight")) 
NULL

$KNNModel$grid
[1] TRUE

$KNNModel$varimp
[1] FALSE


$LARSModel
$LARSModel$label
[1] "Least Angle Regression"

$LARSModel$packages
[1] "lars"

$LARSModel$response_types
[1] "numeric"

$LARSModel$arguments
function (type = c("lasso", "lar", "forward.stagewise", "stepwise"), 
    trace = FALSE, normalize = TRUE, intercept = TRUE, step = NULL, 
    use.Gram = TRUE) 
NULL

$LARSModel$grid
[1] TRUE

$LARSModel$varimp
[1] FALSE


$LDAModel
$LDAModel$label
[1] "Linear Discriminant Analysis"

$LDAModel$packages
[1] "MASS"

$LDAModel$response_types
[1] "factor"

$LDAModel$arguments
function (prior = NULL, tol = 1e-04, method = c("moment", "mle", 
    "mve", "t"), nu = 5, dimen = NULL, use = c("plug-in", "debiased", 
    "predictive")) 
NULL

$LDAModel$grid
[1] TRUE

$LDAModel$varimp
[1] FALSE


$LMModel
$LMModel$label
[1] "Linear Model"

$LMModel$packages
[1] "stats"

$LMModel$response_types
[1] "factor"  "matrix"  "numeric"

$LMModel$arguments
function () 
NULL

$LMModel$grid
[1] FALSE

$LMModel$varimp
[1] TRUE


$MDAModel
$MDAModel$label
[1] "Mixture Discriminant Analysis"

$MDAModel$packages
[1] "mda"

$MDAModel$response_types
[1] "factor"

$MDAModel$arguments
function (subclasses = 3, sub.df = NULL, tot.df = NULL, dimension = sum(subclasses) - 
    1, eps = .Machine$double.eps, iter = 5, method = .(mda::polyreg), 
    trace = FALSE, ...) 
NULL

$MDAModel$grid
[1] TRUE

$MDAModel$varimp
[1] FALSE


$NaiveBayesModel
$NaiveBayesModel$label
[1] "Naive Bayes Classifier"

$NaiveBayesModel$packages
[1] "e1071"

$NaiveBayesModel$response_types
[1] "factor"

$NaiveBayesModel$arguments
function (laplace = 0) 
NULL

$NaiveBayesModel$grid
[1] FALSE

$NaiveBayesModel$varimp
[1] FALSE


$NNetModel
$NNetModel$label
[1] "Feed-Forward Neural Networks"

$NNetModel$packages
[1] "nnet"

$NNetModel$response_types
[1] "factor"  "numeric"

$NNetModel$arguments
function (size = 1, linout = FALSE, entropy = NULL, softmax = NULL, 
    censored = FALSE, skip = FALSE, rang = 0.7, decay = 0, maxit = 100, 
    trace = FALSE, MaxNWts = 1000, abstol = 1e-04, reltol = 1e-08) 
NULL

$NNetModel$grid
[1] TRUE

$NNetModel$varimp
[1] TRUE


$PDAModel
$PDAModel$label
[1] "Penalized Discriminant Analysis"

$PDAModel$packages
[1] "mda"

$PDAModel$response_types
[1] "factor"

$PDAModel$arguments
function (lambda = 1, df = NULL, ...) 
NULL

$PDAModel$grid
[1] TRUE

$PDAModel$varimp
[1] FALSE


$PLSModel
$PLSModel$label
[1] "Partial Least Squares"

$PLSModel$packages
[1] "pls"

$PLSModel$response_types
[1] "factor"  "numeric"

$PLSModel$arguments
function (ncomp = 1, scale = FALSE) 
NULL

$PLSModel$grid
[1] TRUE

$PLSModel$varimp
[1] TRUE


$POLRModel
$POLRModel$label
[1] "Ordered Logistic Regression"

$POLRModel$packages
[1] "MASS"

$POLRModel$response_types
[1] "ordered"

$POLRModel$arguments
function (method = c("logistic", "probit", "loglog", "cloglog", 
    "cauchit")) 
NULL

$POLRModel$grid
[1] FALSE

$POLRModel$varimp
[1] TRUE


$QDAModel
$QDAModel$label
[1] "Quadratic Discriminant Analysis"

$QDAModel$packages
[1] "MASS"

$QDAModel$response_types
[1] "factor"

$QDAModel$arguments
function (prior = NULL, method = c("moment", "mle", "mve", "t"), 
    nu = 5, use = c("plug-in", "predictive", "debiased", "looCV")) 
NULL

$QDAModel$grid
[1] FALSE

$QDAModel$varimp
[1] FALSE


$RandomForestModel
$RandomForestModel$label
[1] "Random Forests"

$RandomForestModel$packages
[1] "randomForest"

$RandomForestModel$response_types
[1] "factor"  "numeric"

$RandomForestModel$arguments
function (ntree = 500, mtry = .(if (is.factor(y)) floor(sqrt(nvars)) else max(floor(nvars/3), 
    1)), replace = TRUE, nodesize = .(if (is.factor(y)) 1 else 5), 
    maxnodes = NULL) 
NULL

$RandomForestModel$grid
[1] TRUE

$RandomForestModel$varimp
[1] TRUE


$RangerModel
$RangerModel$label
[1] "Fast Random Forests"

$RangerModel$packages
[1] "ranger"

$RangerModel$response_types
[1] "factor"  "numeric" "Surv"   

$RangerModel$arguments
function (num.trees = 500, mtry = NULL, importance = c("impurity", 
    "impurity_corrected", "permutation"), min.node.size = NULL, 
    replace = TRUE, sample.fraction = ifelse(replace, 1, 0.632), 
    splitrule = NULL, num.random.splits = 1, alpha = 0.5, minprop = 0.1, 
    split.select.weights = NULL, always.split.variables = NULL, 
    respect.unordered.factors = NULL, scale.permutation.importance = FALSE, 
    verbose = FALSE) 
NULL

$RangerModel$grid
[1] TRUE

$RangerModel$varimp
[1] TRUE


$RPartModel
$RPartModel$label
[1] "Recursive Partitioning and Regression Trees"

$RPartModel$packages
[1] "rpart"    "partykit"

$RPartModel$response_types
[1] "factor"  "numeric" "Surv"   

$RPartModel$arguments
function (minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, 
    maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, 
    surrogatestyle = 0, maxdepth = 30) 
NULL

$RPartModel$grid
[1] TRUE

$RPartModel$varimp
[1] TRUE


$SelectedModel
$SelectedModel$label
[1] "Selected Model"

$SelectedModel$packages
character(0)

$SelectedModel$response_types
 [1] "binary"             "BinomialVariate"    "DiscreteVariate"   
 [4] "factor"             "matrix"             "NegBinomialVariate"
 [7] "numeric"            "ordered"            "PoissonVariate"    
[10] "Surv"              

$SelectedModel$arguments
function (..., control = MachineShop::settings("control"), metrics = NULL, 
    stat = MachineShop::settings("stat.train"), cutoff = MachineShop::settings("cutoff")) 
NULL

$SelectedModel$grid
[1] FALSE

$SelectedModel$varimp
[1] FALSE


$StackedModel
$StackedModel$label
[1] "Stacked Regression"

$StackedModel$packages
character(0)

$StackedModel$response_types
 [1] "binary"             "BinomialVariate"    "DiscreteVariate"   
 [4] "factor"             "matrix"             "NegBinomialVariate"
 [7] "numeric"            "ordered"            "PoissonVariate"    
[10] "Surv"              

$StackedModel$arguments
function (..., control = MachineShop::settings("control"), weights = NULL) 
NULL

$StackedModel$grid
[1] FALSE

$StackedModel$varimp
[1] FALSE


$SuperModel
$SuperModel$label
[1] "Super Learner"

$SuperModel$packages
character(0)

$SuperModel$response_types
[1] "factor"         "numeric"        "PoissonVariate" "Surv"          

$SuperModel$arguments
function (..., model = GBMModel, control = MachineShop::settings("control"), 
    all_vars = FALSE) 
NULL

$SuperModel$grid
[1] FALSE

$SuperModel$varimp
[1] FALSE


$SurvRegModel
$SurvRegModel$label
[1] "Parametric Survival"

$SurvRegModel$packages
[1] "rms"   "Hmisc"

$SurvRegModel$response_types
[1] "Surv"

$SurvRegModel$arguments
function (dist = c("weibull", "exponential", "gaussian", "logistic", 
    "lognormal", "logloglogistic"), scale = NULL, parms = NULL, 
    ...) 
NULL

$SurvRegModel$grid
[1] FALSE

$SurvRegModel$varimp
[1] TRUE


$SurvRegStepAICModel
$SurvRegStepAICModel$label
[1] "Parametric Survival (Stepwise)"

$SurvRegStepAICModel$packages
[1] "rms"   "Hmisc" "MASS" 

$SurvRegStepAICModel$response_types
[1] "Surv"

$SurvRegStepAICModel$arguments
function (dist = c("weibull", "exponential", "gaussian", "logistic", 
    "lognormal", "logloglogistic"), scale = NULL, parms = NULL, 
    ..., direction = c("both", "backward", "forward"), scope = NULL, 
    k = 2, trace = FALSE, steps = 1000) 
NULL

$SurvRegStepAICModel$grid
[1] FALSE

$SurvRegStepAICModel$varimp
[1] TRUE


$SVMModel
$SVMModel$label
[1] "Support Vector Machines"

$SVMModel$packages
[1] "kernlab"

$SVMModel$response_types
[1] "factor"  "numeric"

$SVMModel$arguments
function (scaled = TRUE, type = NULL, kernel = c("rbfdot", "polydot", 
    "vanilladot", "tanhdot", "laplacedot", "besseldot", "anovadot", 
    "splinedot"), kpar = "automatic", C = 1, nu = 0.2, epsilon = 0.1, 
    cache = 40, tol = 0.001, shrinking = TRUE) 
NULL

$SVMModel$grid
[1] FALSE

$SVMModel$varimp
[1] FALSE


$SVMANOVAModel
$SVMANOVAModel$label
[1] "Support Vector Machines (ANOVA)"

$SVMANOVAModel$packages
[1] "kernlab"

$SVMANOVAModel$response_types
[1] "factor"  "numeric"

$SVMANOVAModel$arguments
function (sigma = 1, degree = 1, ...) 
NULL

$SVMANOVAModel$grid
[1] TRUE

$SVMANOVAModel$varimp
[1] FALSE


$SVMBesselModel
$SVMBesselModel$label
[1] "Support Vector Machines (Bessel)"

$SVMBesselModel$packages
[1] "kernlab"

$SVMBesselModel$response_types
[1] "factor"  "numeric"

$SVMBesselModel$arguments
function (sigma = 1, order = 1, degree = 1, ...) 
NULL

$SVMBesselModel$grid
[1] TRUE

$SVMBesselModel$varimp
[1] FALSE


$SVMLaplaceModel
$SVMLaplaceModel$label
[1] "Support Vector Machines (Laplace)"

$SVMLaplaceModel$packages
[1] "kernlab"

$SVMLaplaceModel$response_types
[1] "factor"  "numeric"

$SVMLaplaceModel$arguments
function (sigma = NULL, ...) 
NULL

$SVMLaplaceModel$grid
[1] TRUE

$SVMLaplaceModel$varimp
[1] FALSE


$SVMLinearModel
$SVMLinearModel$label
[1] "Support Vector Machines (Linear)"

$SVMLinearModel$packages
[1] "kernlab"

$SVMLinearModel$response_types
[1] "factor"  "numeric"

$SVMLinearModel$arguments
function (...) 
NULL

$SVMLinearModel$grid
[1] TRUE

$SVMLinearModel$varimp
[1] FALSE


$SVMPolyModel
$SVMPolyModel$label
[1] "Support Vector Machines (Poly)"

$SVMPolyModel$packages
[1] "kernlab"

$SVMPolyModel$response_types
[1] "factor"  "numeric"

$SVMPolyModel$arguments
function (degree = 1, scale = 1, offset = 1, ...) 
NULL

$SVMPolyModel$grid
[1] TRUE

$SVMPolyModel$varimp
[1] FALSE


$SVMRadialModel
$SVMRadialModel$label
[1] "Support Vector Machines (Radial)"

$SVMRadialModel$packages
[1] "kernlab"

$SVMRadialModel$response_types
[1] "factor"  "numeric"

$SVMRadialModel$arguments
function (sigma = NULL, ...) 
NULL

$SVMRadialModel$grid
[1] TRUE

$SVMRadialModel$varimp
[1] FALSE


$SVMSplineModel
$SVMSplineModel$label
[1] "Support Vector Machines (Spline)"

$SVMSplineModel$packages
[1] "kernlab"

$SVMSplineModel$response_types
[1] "factor"  "numeric"

$SVMSplineModel$arguments
function (...) 
NULL

$SVMSplineModel$grid
[1] FALSE

$SVMSplineModel$varimp
[1] FALSE


$SVMTanhModel
$SVMTanhModel$label
[1] "Support Vector Machines (Tanh)"

$SVMTanhModel$packages
[1] "kernlab"

$SVMTanhModel$response_types
[1] "factor"  "numeric"

$SVMTanhModel$arguments
function (scale = 1, offset = 1, ...) 
NULL

$SVMTanhModel$grid
[1] FALSE

$SVMTanhModel$varimp
[1] FALSE


$TreeModel
$TreeModel$label
[1] "Regression and Classification Trees"

$TreeModel$packages
[1] "tree"

$TreeModel$response_types
[1] "factor"  "numeric"

$TreeModel$arguments
function (mincut = 5, minsize = 10, mindev = 0.01, split = c("deviance", 
    "gini"), k = NULL, best = NULL, method = c("deviance", "misclass")) 
NULL

$TreeModel$grid
[1] FALSE

$TreeModel$varimp
[1] FALSE


$TunedModel
$TunedModel$label
[1] "Grid Tuned Model"

$TunedModel$packages
character(0)

$TunedModel$response_types
 [1] "binary"             "BinomialVariate"    "DiscreteVariate"   
 [4] "factor"             "matrix"             "NegBinomialVariate"
 [7] "numeric"            "ordered"            "PoissonVariate"    
[10] "Surv"              

$TunedModel$arguments
function (model, grid = MachineShop::settings("grid"), fixed = NULL, 
    control = MachineShop::settings("control"), metrics = NULL, 
    stat = MachineShop::settings("stat.train"), cutoff = MachineShop::settings("cutoff")) 
NULL

$TunedModel$grid
[1] FALSE

$TunedModel$varimp
[1] FALSE


$XGBModel
$XGBModel$label
[1] "Extreme Gradient Boosting"

$XGBModel$packages
[1] "xgboost"

$XGBModel$response_types
[1] "factor"         "numeric"        "PoissonVariate"

$XGBModel$arguments
function (params = list(), nrounds = 1, verbose = 0, print_every_n = 1) 
NULL

$XGBModel$grid
[1] FALSE

$XGBModel$varimp
[1] TRUE


$XGBDARTModel
$XGBDARTModel$label
[1] "Extreme Gradient Boosting (DART)"

$XGBDARTModel$packages
[1] "xgboost"

$XGBDARTModel$response_types
[1] "factor"         "numeric"        "PoissonVariate"

$XGBDARTModel$arguments
function (objective = NULL, base_score = 0.5, eta = 0.3, gamma = 0, 
    max_depth = 6, min_child_weight = 1, max_delta_step = 0, 
    subsample = 1, colsample_bytree = 1, colsample_bylevel = 1, 
    lambda = 1, alpha = 0, tree_method = "auto", sketch_eps = 0.03, 
    scale_pos_weight = 1, update = "grow_colmaker,prune", refresh_leaf = 1, 
    process_type = "default", grow_policy = "depthwise", max_leaves = 0, 
    max_bin = 256, sample_type = "uniform", normalize_type = "tree", 
    rate_drop = 0, one_drop = 0, skip_drop = 0, ...) 
NULL

$XGBDARTModel$grid
[1] TRUE

$XGBDARTModel$varimp
[1] TRUE


$XGBLinearModel
$XGBLinearModel$label
[1] "Extreme Gradient Boosting (Linear)"

$XGBLinearModel$packages
[1] "xgboost"

$XGBLinearModel$response_types
[1] "factor"         "numeric"        "PoissonVariate"

$XGBLinearModel$arguments
function (objective = NULL, base_score = 0.5, lambda = 0, alpha = 0, 
    updater = "shotgun", feature_selector = "cyclic", top_k = 0, 
    ...) 
NULL

$XGBLinearModel$grid
[1] TRUE

$XGBLinearModel$varimp
[1] TRUE


$XGBTreeModel
$XGBTreeModel$label
[1] "Extreme Gradient Boosting (Tree)"

$XGBTreeModel$packages
[1] "xgboost"

$XGBTreeModel$response_types
[1] "factor"         "numeric"        "PoissonVariate"

$XGBTreeModel$arguments
function (objective = NULL, base_score = 0.5, eta = 0.3, gamma = 0, 
    max_depth = 6, min_child_weight = 1, max_delta_step = 0, 
    subsample = 1, colsample_bytree = 1, colsample_bylevel = 1, 
    lambda = 1, alpha = 0, tree_method = "auto", sketch_eps = 0.03, 
    scale_pos_weight = 1, update = "grow_colmaker,prune", refresh_leaf = 1, 
    process_type = "default", grow_policy = "depthwise", max_leaves = 0, 
    max_bin = 256, ...) 
NULL

$XGBTreeModel$grid
[1] TRUE

$XGBTreeModel$varimp
[1] TRUE


> 
> ## Models by response types
> names(modelinfo(factor(0)))
 [1] "AdaBagModel"       "AdaBoostModel"     "BARTModel"        
 [4] "C50Model"          "CForestModel"      "EarthModel"       
 [7] "FDAModel"          "GBMModel"          "GLMModel"         
[10] "GLMNetModel"       "KNNModel"          "LDAModel"         
[13] "LMModel"           "MDAModel"          "NaiveBayesModel"  
[16] "NNetModel"         "PDAModel"          "PLSModel"         
[19] "QDAModel"          "RandomForestModel" "RangerModel"      
[22] "RPartModel"        "SelectedModel"     "StackedModel"     
[25] "SuperModel"        "SVMModel"          "SVMANOVAModel"    
[28] "SVMBesselModel"    "SVMLaplaceModel"   "SVMLinearModel"   
[31] "SVMPolyModel"      "SVMRadialModel"    "SVMSplineModel"   
[34] "SVMTanhModel"      "TreeModel"         "TunedModel"       
[37] "XGBModel"          "XGBDARTModel"      "XGBLinearModel"   
[40] "XGBTreeModel"     
> names(modelinfo(factor(0), numeric(0)))
 [1] "BARTModel"         "CForestModel"      "EarthModel"       
 [4] "GBMModel"          "GLMModel"          "GLMNetModel"      
 [7] "KNNModel"          "LMModel"           "NNetModel"        
[10] "PLSModel"          "RandomForestModel" "RangerModel"      
[13] "RPartModel"        "SelectedModel"     "StackedModel"     
[16] "SuperModel"        "SVMModel"          "SVMANOVAModel"    
[19] "SVMBesselModel"    "SVMLaplaceModel"   "SVMLinearModel"   
[22] "SVMPolyModel"      "SVMRadialModel"    "SVMSplineModel"   
[25] "SVMTanhModel"      "TreeModel"         "TunedModel"       
[28] "XGBModel"          "XGBDARTModel"      "XGBLinearModel"   
[31] "XGBTreeModel"     
> 
> ## Model-specific information
> modelinfo(GBMModel)
$GBMModel
$GBMModel$label
[1] "Generalized Boosted Regression"

$GBMModel$packages
[1] "gbm"

$GBMModel$response_types
[1] "factor"         "numeric"        "PoissonVariate" "Surv"          

$GBMModel$arguments
function (distribution = NULL, n.trees = 100, interaction.depth = 1, 
    n.minobsinnode = 10, shrinkage = 0.1, bag.fraction = 0.5) 
NULL

$GBMModel$grid
[1] TRUE

$GBMModel$varimp
[1] TRUE


> 
> 
> 
> 
> cleanEx()
> nameEx("performance")
> ### * performance
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: performance
> ### Title: Model Performance Metrics
> ### Aliases: performance performance.BinomialVariate performance.factor
> ###   performance.matrix performance.numeric performance.Surv
> ###   performance.ConfusionList performance.ConfusionMatrix
> ###   performance.Resamples
> 
> ### ** Examples
> 
> res <- resample(Species ~ ., data = iris, model = GBMModel)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> (perf <- performance(res))
Object of class "Performance"

Metrics: Brier, Accuracy, Kappa 
> summary(perf)
          Statistic
Metric          Mean    Median         SD          Min       Max NA
  Brier    0.1013612 0.1028044 0.07537201 2.775549e-05 0.2599035  0
  Accuracy 0.9466667 0.9333333 0.04216370 8.666667e-01 1.0000000  0
  Kappa    0.9200000 0.9000000 0.06324555 8.000000e-01 1.0000000  0
> plot(perf)
> 
> ## Survival response example
> library(survival)
> 
> gbm_fit <- fit(Surv(time, status) ~ ., data = veteran, model = GBMModel)
> 
> obs <- response(gbm_fit, newdata = veteran)
> pred <- predict(gbm_fit, newdata = veteran, type = "prob")
> performance(obs, pred)
  C-Index 
0.7397774 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("performance_curve")
> ### * performance_curve
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: performance_curve
> ### Title: Model Performance Curves
> ### Aliases: performance_curve curves performance_curve.default
> ###   performance_curve.Resamples
> 
> ### ** Examples
> 
> library(MASS)
> 
> res <- resample(type ~ ., data = Pima.tr, model = GBMModel)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following object is masked from ‘package:MASS’:

    select

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> 
> ## ROC curve
> roc <- performance_curve(res)
> plot(roc)
> auc(roc)
Model: GBMModel
[1] 0.7747572
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’,
  ‘package:MASS’

> nameEx("plot-methods")
> ### * plot-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot
> ### Title: Model Performance Plots
> ### Aliases: plot plot.Calibration plot.ConfusionList plot.ConfusionMatrix
> ###   plot.LiftCurve plot.MLModel plot.PartialDependence plot.Performance
> ###   plot.PerformanceCurve plot.Resamples plot.VarImp
> 
> ### ** Examples
> 
> ## Factor response example
> 
> fo <- Species ~ .
> control <- CVControl()
> 
> gbm_fit <- fit(fo, data = iris, model = GBMModel, control = control)
> plot(varimp(gbm_fit))
> 
> gbm_res1 <- resample(fo, iris, GBMModel(n.trees = 25), control)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> gbm_res2 <- resample(fo, iris, GBMModel(n.trees = 50), control)
> gbm_res3 <- resample(fo, iris, GBMModel(n.trees = 100), control)
> plot(gbm_res3)
> 
> res <- c(GBM1 = gbm_res1, GBM2 = gbm_res2, GBM3 = gbm_res3)
> plot(res)
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("predict")
> ### * predict
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict
> ### Title: Model Prediction
> ### Aliases: predict predict.MLModelFit
> 
> ### ** Examples
> 
> ## Survival response example
> library(survival)
> 
> gbm_fit <- fit(Surv(time, status) ~ ., data = veteran, model = GBMModel)
> predict(gbm_fit, newdata = veteran, times = c(90, 180, 360), type = "prob")
Object of class "SurvProbs"
         Time 1     Time 2      Time 3
 [1,] 0.8365916 0.56145986 0.273655904
 [2,] 0.8531376 0.59818523 0.315487494
 [3,] 0.8285085 0.54409864 0.255023237
 [4,] 0.8254649 0.53765859 0.248296394
 [5,] 0.8532835 0.59851631 0.315879640
 [6,] 0.4621335 0.08231311 0.003674177
 [7,] 0.6347068 0.22976955 0.036818119
 [8,] 0.9112564 0.74033926 0.509171310
 [9,] 0.7807315 0.44897872 0.165662818
[10,] 0.8851947 0.67400333 0.412416048
... with 127 more row s
Times:
[1]  90 180 360
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("recipe_roles")
> ### * recipe_roles
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: recipe_roles
> ### Title: Set Recipe Roles
> ### Aliases: recipe_roles role_binom role_case role_pred role_surv
> 
> ### ** Examples
> 
> library(survival)
> library(recipes)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> 
> rec <- recipe(time + status ~ ., data = veteran) %>%
+   role_surv(time = time, event = status) %>%
+   role_case(stratum = status)
> 
> (res <- resample(rec, model = CoxModel))
Object of class "Resamples"

Models: CoxModel 
Stratification variable: status 

Object of class "MLControl"

Name: CVControl
Label: K-Fold Cross-Validation
Folds: 10
Repeats: 1
Seed: 1140350788 
> summary(res)
         Statistic
Metric         Mean    Median         SD       Min       Max NA
  C-Index 0.7011006 0.6949694 0.06982871 0.6111111 0.8076923  0
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("resample-methods")
> ### * resample-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: resample
> ### Title: Resample Estimation of Model Performance
> ### Aliases: resample resample.formula resample.matrix resample.ModelFrame
> ###   resample.recipe resample.MLModel resample.MLModelFunction
> 
> ### ** Examples
> 
> ## Factor response example
> 
> fo <- Species ~ .
> control <- CVControl()
> 
> gbm_res1 <- resample(fo, iris, GBMModel(n.trees = 25), control)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> gbm_res2 <- resample(fo, iris, GBMModel(n.trees = 50), control)
> gbm_res3 <- resample(fo, iris, GBMModel(n.trees = 100), control)
> 
> summary(gbm_res1)
          Statistic
Metric           Mean     Median         SD         Min       Max NA
  Brier    0.07054845 0.07188329 0.05302823 0.002284792 0.1397875  0
  Accuracy 0.96000000 0.93333333 0.03442652 0.933333333 1.0000000  0
  Kappa    0.94000000 0.90000000 0.05163978 0.900000000 1.0000000  0
> plot(gbm_res1)
> 
> res <- c(GBM1 = gbm_res1, GBM2 = gbm_res2, GBM3 = gbm_res3)
> summary(res)
, , Metric = Brier

      Statistic
Model        Mean     Median         SD          Min       Max NA
  GBM1 0.07054845 0.07188329 0.05302823 2.284792e-03 0.1397875  0
  GBM2 0.08602665 0.07214286 0.06988881 4.750789e-04 0.2240059  0
  GBM3 0.10136124 0.10280439 0.07537201 2.775549e-05 0.2599035  0

, , Metric = Accuracy

      Statistic
Model       Mean    Median         SD       Min Max NA
  GBM1 0.9600000 0.9333333 0.03442652 0.9333333   1  0
  GBM2 0.9466667 0.9333333 0.04216370 0.8666667   1  0
  GBM3 0.9466667 0.9333333 0.04216370 0.8666667   1  0

, , Metric = Kappa

      Statistic
Model  Mean Median         SD Min Max NA
  GBM1 0.94    0.9 0.05163978 0.9   1  0
  GBM2 0.92    0.9 0.06324555 0.8   1  0
  GBM3 0.92    0.9 0.06324555 0.8   1  0

> plot(res)
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("response-methods")
> ### * response-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: response
> ### Title: Extract Response Variable
> ### Aliases: response response.MLModelFit response.ModelFrame
> ###   response.recipe
> 
> ### ** Examples
> 
> ## Survival response example
> library(survival)
> 
> mf <- ModelFrame(Surv(time, status) ~ ., data = veteran)
> response(mf)
   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 
  72  411  228  126  118   10   82  110  314 100+   42    8  144  25+   11   30 
  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 
 384    4   54   13 123+  97+  153   59  117   16  151   22   56   21   18  139 
  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 
  20   31   52  287   18   51  122   27   54    7   63  392   10    8   92   35 
  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 
 117  132   12  162    3   95  177  162  216  553  278   12  260  200  156 182+ 
  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 
 143  105  103  250  100  999  112  87+ 231+  242  991  111    1  587  389   33 
  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 
  25  357  467  201    1   30   44  283   15   25 103+   21   13   87    2   20 
  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 
   7   24   99    8   99   61   25   95   80   51   29   24   18  83+   31   51 
 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 
  90   52   73    8   36   48    7  140  186   84   19   45   80   52  164   19 
 129  130  131  132  133  134  135  136  137 
  53   15   43  340  133  111  231  378   49 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("settings")
> ### * settings
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: settings
> ### Title: MachineShop Settings
> ### Aliases: settings
> 
> ### ** Examples
> 
> ## View all current settings
> settings()
$control
[1] "CVControl"

$cutoff
[1] 0.5

$dist.Surv
[1] "weibull"

$dist.SurvProbs
[1] "empirical"

$grid
Object of class "Grid"

Length: 3
Random sample: FALSE

$max.print
[1] 10

$method.EmpiricalSurv
[1] "efron"

$metrics.ConfusionMatrix
         Accuracy             Kappa    Weighted Kappa       Sensitivity 
       "accuracy"          "kappa2" "weighted_kappa2"     "sensitivity" 
      Specificity 
    "specificity" 

$metrics.factor
            Brier          Accuracy             Kappa    Weighted Kappa 
          "brier"        "accuracy"          "kappa2" "weighted_kappa2" 
          ROC AUC       Sensitivity       Specificity 
        "roc_auc"     "sensitivity"     "specificity" 

$metrics.matrix
  RMSE     R2    MAE 
"rmse"   "r2"  "mae" 

$metrics.numeric
  RMSE     R2    MAE 
"rmse"   "r2"  "mae" 

$metrics.Surv
   C-Index      Brier    ROC AUC   Accuracy 
  "cindex"    "brier"  "roc_auc" "accuracy" 

$progress.resample
[1] TRUE

$require
[1] "MachineShop" "survival"    "recipes"    

$reset
character(0)

$stat.Curve
[1] "base::mean"

$stat.Resamples
[1] "base::mean"

$stat.train
[1] "base::mean"

$stats.PartialDependence
        Mean 
"base::mean" 

$stats.Resamples
           Mean          Median              SD             Min             Max 
   "base::mean" "stats::median"     "stats::sd"     "base::min"     "base::max" 

$verbose.resample
[1] FALSE

> 
> ## Change settings
> presets <- settings(control = "BootControl", grid = 10)
> 
> ## View one setting
> settings("control")
[1] "BootControl"
> 
> ## View multiple settings
> settings("control", "grid")
$control
[1] "BootControl"

$grid
Object of class "Grid"

Length: 10
Random sample: FALSE

> 
> ## Restore the previous settings
> settings(presets)
> 
> 
> 
> 
> cleanEx()
> nameEx("step_kmeans")
> ### * step_kmeans
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: step_kmeans
> ### Title: K-Means Clustering Variable Reduction
> ### Aliases: step_kmeans tidy.step_kmeans tunable.step_kmeans
> 
> ### ** Examples
> 
> library(recipes)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> 
> rec <- recipe(rating ~ ., data = attitude)
> kmeans_rec <- rec %>%
+   step_kmeans(all_predictors(), k = 3)
> kmeans_prep <- prep(kmeans_rec, training = attitude)
> kmeans_data <- bake(kmeans_prep, attitude)
> 
> pairs(kmeans_data, lower.panel = NULL)
> 
> tidy(kmeans_rec, number = 1)
[90m# A tibble: 1 x 5[39m
  terms            cluster sqdist names id          
  [3m[90m<chr>[39m[23m              [3m[90m<int>[39m[23m  [3m[90m<dbl>[39m[23m [3m[90m<chr>[39m[23m [3m[90m<chr>[39m[23m       
[90m1[39m all_predictors()      [31mNA[39m     [31mNA[39m [31mNA[39m    kmeans_4dMaH
> tidy(kmeans_prep, number = 1)
[90m# A tibble: 6 x 5[39m
  terms      cluster sqdist names   id          
  [3m[90m<chr>[39m[23m        [3m[90m<int>[39m[23m  [3m[90m<dbl>[39m[23m [3m[90m<chr>[39m[23m   [3m[90m<chr>[39m[23m       
[90m1[39m complaints       3   6.40 KMeans3 kmeans_4dMaH
[90m2[39m privileges       3   6.40 KMeans3 kmeans_4dMaH
[90m3[39m learning         2   7.93 KMeans2 kmeans_4dMaH
[90m4[39m raises           2   7.11 KMeans2 kmeans_4dMaH
[90m5[39m critical         1   0    KMeans1 kmeans_4dMaH
[90m6[39m advance          2   9.21 KMeans2 kmeans_4dMaH
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’

> nameEx("step_kmedoids")
> ### * step_kmedoids
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: step_kmedoids
> ### Title: K-Medoids Clustering Variable Selection
> ### Aliases: step_kmedoids tidy.step_kmedoids tunable.step_kmedoids
> 
> ### ** Examples
> 
> library(recipes)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> 
> rec <- recipe(rating ~ ., data = attitude)
> kmedoids_rec <- rec %>%
+   step_kmedoids(all_predictors(), k = 3)
> kmedoids_prep <- prep(kmedoids_rec, training = attitude)
> kmedoids_data <- bake(kmedoids_prep, attitude)
> 
> pairs(kmedoids_data, lower.panel = NULL)
> 
> tidy(kmedoids_rec, number = 1)
[90m# A tibble: 1 x 6[39m
  terms            cluster medoid silhouette names id            
  [3m[90m<chr>[39m[23m              [3m[90m<int>[39m[23m [3m[90m<lgl>[39m[23m       [3m[90m<dbl>[39m[23m [3m[90m<chr>[39m[23m [3m[90m<chr>[39m[23m         
[90m1[39m all_predictors()      [31mNA[39m [31mNA[39m             [31mNA[39m [31mNA[39m    kmedoids_4dMaH
> tidy(kmedoids_prep, number = 1)
[90m# A tibble: 6 x 6[39m
  terms      cluster medoid silhouette names      id            
  [3m[90m<chr>[39m[23m        [3m[90m<int>[39m[23m [3m[90m<lgl>[39m[23m       [3m[90m<dbl>[39m[23m [3m[90m<chr>[39m[23m      [3m[90m<chr>[39m[23m         
[90m1[39m complaints       1 FALSE      0.071[4m2[24m raises     kmedoids_4dMaH
[90m2[39m privileges       2 TRUE       0      privileges kmedoids_4dMaH
[90m3[39m learning         1 FALSE      0.203  raises     kmedoids_4dMaH
[90m4[39m raises           1 TRUE       0.271  raises     kmedoids_4dMaH
[90m5[39m critical         3 TRUE       0      critical   kmedoids_4dMaH
[90m6[39m advance          1 FALSE      0.184  raises     kmedoids_4dMaH
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’

> nameEx("step_spca")
> ### * step_spca
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: step_spca
> ### Title: Sparse Principal Components Analysis Variable Reduction
> ### Aliases: step_spca tidy.step_spca tunable.step_spca
> 
> ### ** Examples
> 
> library(recipes)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> 
> rec <- recipe(rating ~ ., data = attitude)
> spca_rec <- rec %>%
+   step_spca(all_predictors(), num_comp = 5, sparsity = 1)
> spca_prep <- prep(spca_rec, training = attitude)
> spca_data <- bake(spca_prep, attitude)
> 
> pairs(spca_data, lower.panel = NULL)
> 
> tidy(spca_rec, number = 1)
[90m# A tibble: 1 x 4[39m
  terms            value component id        
  [3m[90m<chr>[39m[23m            [3m[90m<dbl>[39m[23m [3m[90m<chr>[39m[23m     [3m[90m<chr>[39m[23m     
[90m1[39m all_predictors()    [31mNA[39m [31mNA[39m        spca_4dMaH
> tidy(spca_prep, number = 1)
[90m# A tibble: 30 x 4[39m
   terms      value component id        
   [3m[90m<chr>[39m[23m      [3m[90m<dbl>[39m[23m [3m[90m<chr>[39m[23m     [3m[90m<chr>[39m[23m     
[90m 1[39m complaints 0.589 SPCA1     spca_4dMaH
[90m 2[39m privileges 0     SPCA1     spca_4dMaH
[90m 3[39m learning   0     SPCA1     spca_4dMaH
[90m 4[39m raises     0.754 SPCA1     spca_4dMaH
[90m 5[39m critical   0     SPCA1     spca_4dMaH
[90m 6[39m advance    0.291 SPCA1     spca_4dMaH
[90m 7[39m complaints 0     SPCA2     spca_4dMaH
[90m 8[39m privileges 0     SPCA2     spca_4dMaH
[90m 9[39m learning   0     SPCA2     spca_4dMaH
[90m10[39m raises     0     SPCA2     spca_4dMaH
[90m# … with 20 more rows[39m
> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’

> nameEx("summary-methods")
> ### * summary-methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: summary
> ### Title: Model Performance Summaries
> ### Aliases: summary summary.ConfusionList summary.ConfusionMatrix
> ###   summary.MLModel summary.Performance summary.PerformanceCurve
> ###   summary.Resamples
> 
> ### ** Examples
> 
> ## Factor response example
> 
> fo <- Species ~ .
> control <- CVControl()
> 
> gbm_res1 <- resample(fo, iris, GBMModel(n.trees = 25), control)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> gbm_res2 <- resample(fo, iris, GBMModel(n.trees = 50), control)
> gbm_res3 <- resample(fo, iris, GBMModel(n.trees = 100), control)
> summary(gbm_res3)
          Statistic
Metric          Mean    Median         SD          Min       Max NA
  Brier    0.1013612 0.1028044 0.07537201 2.775549e-05 0.2599035  0
  Accuracy 0.9466667 0.9333333 0.04216370 8.666667e-01 1.0000000  0
  Kappa    0.9200000 0.9000000 0.06324555 8.000000e-01 1.0000000  0
> 
> res <- c(GBM1 = gbm_res1, GBM2 = gbm_res2, GBM3 = gbm_res3)
> summary(res)
, , Metric = Brier

      Statistic
Model        Mean     Median         SD          Min       Max NA
  GBM1 0.07054845 0.07188329 0.05302823 2.284792e-03 0.1397875  0
  GBM2 0.08602665 0.07214286 0.06988881 4.750789e-04 0.2240059  0
  GBM3 0.10136124 0.10280439 0.07537201 2.775549e-05 0.2599035  0

, , Metric = Accuracy

      Statistic
Model       Mean    Median         SD       Min Max NA
  GBM1 0.9600000 0.9333333 0.03442652 0.9333333   1  0
  GBM2 0.9466667 0.9333333 0.04216370 0.8666667   1  0
  GBM3 0.9466667 0.9333333 0.04216370 0.8666667   1  0

, , Metric = Kappa

      Statistic
Model  Mean Median         SD Min Max NA
  GBM1 0.94    0.9 0.05163978 0.9   1  0
  GBM2 0.92    0.9 0.06324555 0.8   1  0
  GBM3 0.92    0.9 0.06324555 0.8   1  0

> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("t.test")
> ### * t.test
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: t.test
> ### Title: Paired t-Tests for Model Comparisons
> ### Aliases: t.test t.test.PerformanceDiff
> 
> ### ** Examples
> 
> ## Numeric response example
> fo <- sale_amount ~ .
> control <- CVControl()
> 
> gbm_res1 <- resample(fo, ICHomes, GBMModel(n.trees = 25), control)
Loading required package: dplyr

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union


Attaching package: ‘recipes’

The following object is masked from ‘package:stats’:

    step

> gbm_res2 <- resample(fo, ICHomes, GBMModel(n.trees = 50), control)
> gbm_res3 <- resample(fo, ICHomes, GBMModel(n.trees = 100), control)
> 
> res <- c(GBM1 = gbm_res1, GBM2 = gbm_res2, GBM3 = gbm_res3)
> res_diff <- diff(res)
> t.test(res_diff)
Object of class "PerformanceDiffTest"

Upper diagonal: mean differences (Model1 - Model2)
Lower diagonal: p-values
P-value adjustment method: holm

, , Metric = RMSE

      Model2
Model1         GBM1         GBM2     GBM3
  GBM1           NA 5.742016e+03 9741.323
  GBM2 5.074139e-05           NA 3999.308
  GBM3 5.235539e-05 6.638721e-05       NA

, , Metric = R2

      Model2
Model1         GBM1          GBM2        GBM3
  GBM1           NA -8.295219e-02 -0.13594820
  GBM2 5.584157e-06            NA -0.05299601
  GBM3 5.584157e-06  7.660176e-06          NA

, , Metric = MAE

      Model2
Model1         GBM1         GBM2     GBM3
  GBM1           NA 4.978540e+03 8273.419
  GBM2 7.704588e-06           NA 3294.879
  GBM3 1.062528e-06 1.062528e-06       NA

> 
> 
> 
> 
> cleanEx()

detaching ‘package:recipes’, ‘package:dplyr’, ‘package:survival’

> nameEx("varimp")
> ### * varimp
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: varimp
> ### Title: Variable Importance
> ### Aliases: varimp
> 
> ### ** Examples
> 
> ## Survival response example
> library(survival)
> 
> gbm_fit <- fit(Surv(time, status) ~ ., data = veteran, model = GBMModel)
> (vi <- varimp(gbm_fit))
Object of class "VarImp"
           Overall
age      100.00000
karno     93.83406
celltype  69.41357
diagtime  31.39921
trt       13.16085
prior      0.00000
> plot(vi)
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> cleanEx()

detaching ‘package:survival’

> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  41.304 1.927 43.832 0.002 0.007 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
